2017-06-19 15:07:19,820 INFO TC190_191_remove_create_osd_on_single_node.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. remove the osd one by one on the first node
4. check the status
5. create osd on the node

2017-06-19 15:07:20,784 INFO monitors.py [line:126]    "quorum_leader_name": "server113",
stdin: is not a tty

2017-06-19 15:07:20,784 INFO monitors.py [line:129]    "quorum_leader_name": "server113",
2017-06-19 15:07:22,787 INFO TC190_191_remove_create_osd_on_single_node.py [line:29] start to check cluster status before case running
2017-06-19 15:07:24,792 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-19 15:07:25,294 INFO cluster.py [line:213]    cluster e95425c5-dda1-4a27-be7b-9baccd00f2ec
     health HEALTH_WARN
            mon.server113 low disk space
            mon.server114 low disk space
     monmap e3: 3 mons at {server113=192.168.1.113:6789/0,server114=192.168.1.114:6789/0,server115=192.168.1.115:6789/0}
            election epoch 62, quorum 0,1,2 server113,server114,server115
     osdmap e4046: 15 osds: 15 up, 15 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v217477: 2192 pgs, 13 pools, 1832 GB data, 470 kobjects
            2614 GB used, 7873 GB / 10488 GB avail
                2192 active+clean
  client io 17735 B/s rd, 25 op/s rd, 0 op/s wr
stdin: is not a tty

2017-06-19 15:07:25,294 INFO cluster.py [line:238] PG number is 2192
2017-06-19 15:07:25,294 INFO cluster.py [line:239] usefull PG number is 2192
2017-06-19 15:07:25,295 INFO TC190_191_remove_create_osd_on_single_node.py [line:32] health status is OK
2017-06-19 15:07:25,295 INFO TC190_191_remove_create_osd_on_single_node.py [line:37] 
Step1: start IO from clients
2017-06-19 15:07:25,764 INFO client.py [line:172] ['oot      54937      1  0 Jun16 ?        00:00:00 sudo -i fio -direct=1 -name=/dev/nbd0 -iodepth_batch_complete=1 -ioengine=libaio -bs=8k -rw=randwrite -verify_fatal=1 -verify=md5 -verify_interval=512 -verify_dump=1 -do_verify=1 -overwrite=1 -filename=/dev/nbd0 -rate=400 -size=100G', 'root      54938  54937  0 Jun16 ?        00:09:04 fio -direct=1 -name=/dev/nbd0 -iodepth_batch_complete=1 -ioengine=libaio -bs=8k -rw=randwrite -verify_fatal=1 -verify=md5 -verify_interval=512 -verify_dump=1 -do_verify=1 -overwrite=1 -filename=/dev/nbd0 -rate=400 -size=100G', 'root      54949  54938  0 Jun16 ?        00:00:01 fio -direct=1 -name=/dev/nbd0 -iodepth_batch_complete=1 -ioengine=libaio -bs=8k -rw=randwrite -verify_fatal=1 -verify=md5 -verify_interval=512 -verify_dump=1 -do_verify=1 -overwrite=1 -filename=/dev/nbd0 -rate=400 -size=100G', 'root      55038      1  0 Jun16 ?        00:00:00 sudo -i fio -direct=1 -name=/dev/nbd1 -iodepth_batch_complete=1 -ioengine=libaio -bs=8k -rw=randwrite -verify_fatal=1 -verify=md5 -verify_interval=512 -verify_dump=1 -do_verify=1 -overwrite=1 -filename=/dev/nbd1 -rate=400 -size=200G', 'root      55039  55038  0 Jun16 ?        00:09:03 fio -direct=1 -name=/dev/nbd1 -iodepth_batch_complete=1 -ioengine=libaio -bs=8k -rw=randwrite -verify_fatal=1 -verify=md5 -verify_interval=512 -verify_dump=1 -do_verify=1 -overwrite=1 -filename=/dev/nbd1 -rate=400 -size=200G', 'root      55120  55039  0 Jun16 ?        00:00:01 fio -direct=1 -name=/dev/nbd1 -iodepth_batch_complete=1 -ioengine=libaio -bs=8k -rw=randwrite -verify_fatal=1 -verify=md5 -verify_interval=512 -verify_dump=1 -do_verify=1 -overwrite=1 -filename=/dev/nbd1 -rate=400 -size=200G', 'denali   122154 122153  0 07:07 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   122156 122154  0 07:07 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-19 15:07:25,764 INFO client.py [line:174] IO is running
2017-06-19 15:07:25,970 INFO node.py [line:176] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-19 15:07:25,971 INFO node.py [line:178] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-19 15:07:27,166 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-19 15:07:30,350 INFO osd.py [line:89] node is  server113
2017-06-19 15:07:30,350 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=0 & sleep 30
2017-06-19 15:08:00,665 ERROR osd.py [line:96] Error when start osdosd.0
2017-06-19 15:08:00,665 ERROR osd.py [line:97] sudo -i start ceph-osd id=0 & sleep 30
2017-06-19 15:08:00,665 ERROR osd.py [line:98] tdin: is not a tty
start: Job is already running: ceph-osd (ceph/0)

2017-06-19 15:08:00,665 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-19 15:08:03,873 INFO osd.py [line:89] node is  server113
2017-06-19 15:08:03,874 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=1 & sleep 30
2017-06-19 15:08:34,067 ERROR osd.py [line:96] Error when start osdosd.1
2017-06-19 15:08:34,067 ERROR osd.py [line:97] sudo -i start ceph-osd id=1 & sleep 30
2017-06-19 15:08:34,067 ERROR osd.py [line:98] eph-osd (ceph/1) start/running, process 31371
stdin: is not a tty

2017-06-19 15:08:34,067 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-19 15:08:37,284 INFO osd.py [line:89] node is  server113
2017-06-19 15:08:37,284 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=2 & sleep 30
2017-06-19 15:09:07,479 ERROR osd.py [line:96] Error when start osdosd.2
2017-06-19 15:09:07,479 ERROR osd.py [line:97] sudo -i start ceph-osd id=2 & sleep 30
2017-06-19 15:09:07,479 ERROR osd.py [line:98] eph-osd (ceph/2) start/running, process 42670
stdin: is not a tty

2017-06-19 15:09:07,479 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-19 15:09:10,663 INFO osd.py [line:89] node is  server113
2017-06-19 15:09:10,663 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=3 & sleep 30
2017-06-19 15:09:40,856 ERROR osd.py [line:96] Error when start osdosd.3
2017-06-19 15:09:40,856 ERROR osd.py [line:97] sudo -i start ceph-osd id=3 & sleep 30
2017-06-19 15:09:40,856 ERROR osd.py [line:98] eph-osd (ceph/3) start/running, process 53475
stdin: is not a tty

2017-06-19 15:09:40,856 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-19 15:09:44,082 INFO osd.py [line:89] node is  server113
2017-06-19 15:09:44,082 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=4 & sleep 30
2017-06-19 15:10:14,274 ERROR osd.py [line:96] Error when start osdosd.4
2017-06-19 15:10:14,274 ERROR osd.py [line:97] sudo -i start ceph-osd id=4 & sleep 30
2017-06-19 15:10:14,275 ERROR osd.py [line:98] eph-osd (ceph/4) start/running, process 8568
stdin: is not a tty

2017-06-19 15:11:14,282 INFO TC190_191_remove_create_osd_on_single_node.py [line:51] 
Step2: remove osd and create them 10 times
2017-06-19 15:11:14,494 INFO node.py [line:200] otal 0
2017-06-19 15:11:14,495 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 19 10:22 ceph-0-block -> ../../nvme2n1p4
2017-06-19 15:11:14,495 INFO node.py [line:203] 0
2017-06-19 15:11:14,495 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 19 10:22 ceph-0-block -> ../../nvme2n1p4
2017-06-19 15:11:14,495 INFO node.py [line:212] nvme2n1
2017-06-19 15:11:14,495 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 19 10:22 ceph-0-db -> ../../nvme2n1p3
2017-06-19 15:11:14,495 INFO node.py [line:203] 0
2017-06-19 15:11:14,496 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 19 10:22 ceph-0-db -> ../../nvme2n1p3
2017-06-19 15:11:14,496 INFO node.py [line:212] nvme2n1
2017-06-19 15:11:14,496 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 19 10:22 ceph-0-wal -> ../../nvme2n1p2
2017-06-19 15:11:14,496 INFO node.py [line:203] 0
2017-06-19 15:11:14,496 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 19 10:22 ceph-0-wal -> ../../nvme2n1p2
2017-06-19 15:11:14,496 INFO node.py [line:212] nvme2n1
2017-06-19 15:11:14,496 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-1-block -> ../../nvme1n1p4
2017-06-19 15:11:14,496 INFO node.py [line:203] 1
2017-06-19 15:11:14,496 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-1-block -> ../../nvme1n1p4
2017-06-19 15:11:14,497 INFO node.py [line:212] nvme1n1
2017-06-19 15:11:14,497 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-1-db -> ../../nvme1n1p3
2017-06-19 15:11:14,497 INFO node.py [line:203] 1
2017-06-19 15:11:14,497 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-1-db -> ../../nvme1n1p3
2017-06-19 15:11:14,497 INFO node.py [line:212] nvme1n1
2017-06-19 15:11:14,497 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-1-wal -> ../../nvme1n1p2
2017-06-19 15:11:14,497 INFO node.py [line:203] 1
2017-06-19 15:11:14,497 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-1-wal -> ../../nvme1n1p2
2017-06-19 15:11:14,497 INFO node.py [line:212] nvme1n1
2017-06-19 15:11:14,498 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-2-block -> ../../nvme4n1p4
2017-06-19 15:11:14,498 INFO node.py [line:203] 2
2017-06-19 15:11:14,498 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-2-block -> ../../nvme4n1p4
2017-06-19 15:11:14,498 INFO node.py [line:212] nvme4n1
2017-06-19 15:11:14,498 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-2-db -> ../../nvme4n1p3
2017-06-19 15:11:14,498 INFO node.py [line:203] 2
2017-06-19 15:11:14,498 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-2-db -> ../../nvme4n1p3
2017-06-19 15:11:14,498 INFO node.py [line:212] nvme4n1
2017-06-19 15:11:14,498 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-2-wal -> ../../nvme4n1p2
2017-06-19 15:11:14,498 INFO node.py [line:203] 2
2017-06-19 15:11:14,499 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-2-wal -> ../../nvme4n1p2
2017-06-19 15:11:14,499 INFO node.py [line:212] nvme4n1
2017-06-19 15:11:14,499 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-3-block -> ../../nvme0n1p4
2017-06-19 15:11:14,499 INFO node.py [line:203] 3
2017-06-19 15:11:14,499 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-3-block -> ../../nvme0n1p4
2017-06-19 15:11:14,499 INFO node.py [line:212] nvme0n1
2017-06-19 15:11:14,499 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-3-db -> ../../nvme0n1p3
2017-06-19 15:11:14,499 INFO node.py [line:203] 3
2017-06-19 15:11:14,499 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-3-db -> ../../nvme0n1p3
2017-06-19 15:11:14,500 INFO node.py [line:212] nvme0n1
2017-06-19 15:11:14,500 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-3-wal -> ../../nvme0n1p2
2017-06-19 15:11:14,500 INFO node.py [line:203] 3
2017-06-19 15:11:14,500 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-3-wal -> ../../nvme0n1p2
2017-06-19 15:11:14,500 INFO node.py [line:212] nvme0n1
2017-06-19 15:11:14,500 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-4-block -> ../../nvme3n1p4
2017-06-19 15:11:14,500 INFO node.py [line:203] 4
2017-06-19 15:11:14,500 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-4-block -> ../../nvme3n1p4
2017-06-19 15:11:14,500 INFO node.py [line:212] nvme3n1
2017-06-19 15:11:14,501 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-4-db -> ../../nvme3n1p3
2017-06-19 15:11:14,501 INFO node.py [line:203] 4
2017-06-19 15:11:14,501 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-4-db -> ../../nvme3n1p3
2017-06-19 15:11:14,501 INFO node.py [line:212] nvme3n1
2017-06-19 15:11:14,501 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-4-wal -> ../../nvme3n1p2
2017-06-19 15:11:14,501 INFO node.py [line:203] 4
2017-06-19 15:11:14,501 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 16 17:39 ceph-4-wal -> ../../nvme3n1p2
2017-06-19 15:11:14,501 INFO node.py [line:212] nvme3n1
2017-06-19 15:11:14,501 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 19 10:22 head-reverse-part -> ../../nvme2n1p1
2017-06-19 15:11:14,502 INFO node.py [line:200] 
2017-06-19 15:11:14,898 INFO node.py [line:220] osd.0  ---> disk 
2017-06-19 15:11:14,898 INFO node.py [line:220] osd.1  ---> disk 
2017-06-19 15:11:14,899 INFO node.py [line:220] osd.2  ---> disk 
2017-06-19 15:11:14,899 INFO node.py [line:220] osd.3  ---> disk 
2017-06-19 15:11:14,899 INFO node.py [line:220] osd.4  ---> disk 
2017-06-19 15:11:14,899 INFO TC190_191_remove_create_osd_on_single_node.py [line:55] start to delete osd on node server113 
2017-06-19 15:11:15,122 INFO osd.py [line:156] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/delete_osds_local.sh -n -d /dev/
2017-06-19 15:11:15,410 ERROR osd.py [line:160] Error when delete osd.0
2017-06-19 15:11:15,410 ERROR osd.py [line:161] sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/delete_osds_local.sh -n -d /dev/
2017-06-19 15:11:15,410 ERROR osd.py [line:162] ARNING: '/dev/' is not block device
'/dev/' is not block device, 
stdin: is not a tty

