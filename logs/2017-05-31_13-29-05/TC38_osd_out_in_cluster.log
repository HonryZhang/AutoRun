2017-05-31 13:29:06,003 INFO TC38_osd_out_in_cluster.py [line:25] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. out all osds in sequence
4. stop all osds in sequence
5. start all osds in sequence
6. add in all osds in sequence
7. check the cluster status
8. repeat step 2-7 on the other node

2017-05-31 13:29:07,364 INFO monitors.py [line:126]    "quorum_leader_name": "osdnode2",
stdin: is not a tty

2017-05-31 13:29:07,364 INFO monitors.py [line:129]    "quorum_leader_name": "osdnode2",
2017-05-31 13:29:07,364 INFO TC38_osd_out_in_cluster.py [line:30] start to check cluster status before case running
2017-05-31 13:29:07,427 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:29:08,203 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e90: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2174: 768 pgs, 2 pools, 1396 GB data, 375 kobjects
            187 GB used, 6804 GB / 6991 GB avail
                 768 active+clean
  client io 25338 kB/s wr, 0 op/s rd, 3167 op/s wr

2017-05-31 13:29:08,207 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:29:08,210 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:29:08,213 INFO TC38_osd_out_in_cluster.py [line:33] health status is OK
2017-05-31 13:29:08,216 INFO TC38_osd_out_in_cluster.py [line:38] 
Step1: Check IO from clients
2017-05-31 13:29:09,727 INFO client.py [line:169] ['oot      2463     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg0', 'root      2464  2463 39 04:50 ?        00:15:20 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg0', 'root      2500     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg1', 'root      2502  2500 39 04:50 ?        00:15:19 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg1', 'root      2540     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg2', 'root      2541  2540 39 04:50 ?        00:15:18 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg2', 'root      2573     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg3', 'root      2574  2573 39 04:50 ?        00:15:17 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg3', 'root      2606     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg4', 'root      2607  2606 39 04:50 ?        00:15:16 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg4', 'denali    3032  3031  0 05:29 ?        00:00:00 bash -c sudo -i ps -ef | grep fio', 'denali    3034  3032  0 05:29 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-05-31 13:29:09,743 INFO client.py [line:171] IO is running
2017-05-31 13:30:09,746 INFO TC38_osd_out_in_cluster.py [line:46] 
Step 2: Out the osd and check IO
2017-05-31 13:30:09,746 INFO TC38_osd_out_in_cluster.py [line:50] 
Now operate osdnode2
2017-05-31 13:30:09,746 INFO TC38_osd_out_in_cluster.py [line:51] 5
2017-05-31 13:30:09,746 INFO TC38_osd_out_in_cluster.py [line:52] 
Now operate osd.0
2017-05-31 13:30:09,746 INFO TC38_osd_out_in_cluster.py [line:53] out osd.0
2017-05-31 13:30:09,746 INFO osd.py [line:62] execute command is sudo -i ceph osd out osd.0 & sleep 3
2017-05-31 13:30:13,342 INFO osd.py [line:67] osd.0 is already out cluster
2017-05-31 13:30:13,342 INFO TC38_osd_out_in_cluster.py [line:55] check if IO error
2017-05-31 13:30:13,874 INFO osd.py [line:77] execute command is sudo -i ceph osd in osd.0 & sleep 3
2017-05-31 13:30:17,259 INFO osd.py [line:80] osd.0 is already in cluster
2017-05-31 13:30:47,802 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:30:48,503 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e96: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2265: 768 pgs, 2 pools, 1401 GB data, 375 kobjects
            191 GB used, 6799 GB / 6991 GB avail
                 768 active+clean
  client io 29637 kB/s wr, 0 op/s rd, 3704 op/s wr

2017-05-31 13:30:48,503 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:30:48,503 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:30:48,503 INFO TC38_osd_out_in_cluster.py [line:85] stop osd.0 in cluster successfully
2017-05-31 13:30:48,503 INFO TC38_osd_out_in_cluster.py [line:50] 
Now operate osdnode2
2017-05-31 13:30:48,503 INFO TC38_osd_out_in_cluster.py [line:51] 5
2017-05-31 13:30:48,503 INFO TC38_osd_out_in_cluster.py [line:52] 
Now operate osd.1
2017-05-31 13:30:48,503 INFO TC38_osd_out_in_cluster.py [line:53] out osd.1
2017-05-31 13:30:48,503 INFO osd.py [line:62] execute command is sudo -i ceph osd out osd.1 & sleep 3
2017-05-31 13:30:52,088 INFO osd.py [line:67] osd.1 is already out cluster
2017-05-31 13:30:52,088 INFO TC38_osd_out_in_cluster.py [line:55] check if IO error
2017-05-31 13:30:52,630 INFO osd.py [line:77] execute command is sudo -i ceph osd in osd.1 & sleep 3
2017-05-31 13:30:56,094 INFO osd.py [line:80] osd.1 is already in cluster
2017-05-31 13:31:26,729 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:31:27,566 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e102: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2302: 768 pgs, 2 pools, 1402 GB data, 375 kobjects
            193 GB used, 6798 GB / 6991 GB avail
                 768 active+clean
  client io 23066 kB/s wr, 0 op/s rd, 2883 op/s wr

2017-05-31 13:31:27,566 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:31:27,566 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:31:27,566 INFO TC38_osd_out_in_cluster.py [line:85] stop osd.1 in cluster successfully
2017-05-31 13:31:27,566 INFO TC38_osd_out_in_cluster.py [line:50] 
Now operate osdnode2
2017-05-31 13:31:27,566 INFO TC38_osd_out_in_cluster.py [line:51] 5
2017-05-31 13:31:27,566 INFO TC38_osd_out_in_cluster.py [line:52] 
Now operate osd.2
2017-05-31 13:31:27,566 INFO TC38_osd_out_in_cluster.py [line:53] out osd.2
2017-05-31 13:31:27,566 INFO osd.py [line:62] execute command is sudo -i ceph osd out osd.2 & sleep 3
2017-05-31 13:31:31,092 INFO osd.py [line:67] osd.2 is already out cluster
2017-05-31 13:31:31,092 INFO TC38_osd_out_in_cluster.py [line:55] check if IO error
2017-05-31 13:31:31,648 INFO osd.py [line:77] execute command is sudo -i ceph osd in osd.2 & sleep 3
2017-05-31 13:31:35,193 INFO osd.py [line:80] osd.2 is already in cluster
2017-05-31 13:32:05,819 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:32:06,467 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e108: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2339: 768 pgs, 2 pools, 1404 GB data, 375 kobjects
            194 GB used, 6796 GB / 6991 GB avail
                 768 active+clean
  client io 23710 kB/s wr, 0 op/s rd, 2963 op/s wr

2017-05-31 13:32:06,467 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:32:06,467 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:32:06,467 INFO TC38_osd_out_in_cluster.py [line:85] stop osd.2 in cluster successfully
2017-05-31 13:32:06,467 INFO TC38_osd_out_in_cluster.py [line:50] 
Now operate osdnode2
2017-05-31 13:32:06,467 INFO TC38_osd_out_in_cluster.py [line:51] 5
2017-05-31 13:32:06,467 INFO TC38_osd_out_in_cluster.py [line:52] 
Now operate osd.3
2017-05-31 13:32:06,467 INFO TC38_osd_out_in_cluster.py [line:53] out osd.3
2017-05-31 13:32:06,467 INFO osd.py [line:62] execute command is sudo -i ceph osd out osd.3 & sleep 3
2017-05-31 13:32:10,157 INFO osd.py [line:67] osd.3 is already out cluster
2017-05-31 13:32:10,157 INFO TC38_osd_out_in_cluster.py [line:55] check if IO error
2017-05-31 13:32:10,887 INFO osd.py [line:77] execute command is sudo -i ceph osd in osd.3 & sleep 3
2017-05-31 13:32:14,374 INFO osd.py [line:80] osd.3 is already in cluster
2017-05-31 13:32:45,035 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:32:45,700 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e114: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2376: 768 pgs, 2 pools, 1405 GB data, 375 kobjects
            196 GB used, 6794 GB / 6991 GB avail
                 768 active+clean
  client io 22310 kB/s wr, 0 op/s rd, 2788 op/s wr

2017-05-31 13:32:45,700 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:32:45,700 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:32:45,700 INFO TC38_osd_out_in_cluster.py [line:85] stop osd.3 in cluster successfully
2017-05-31 13:32:45,700 INFO TC38_osd_out_in_cluster.py [line:50] 
Now operate osdnode2
2017-05-31 13:32:45,700 INFO TC38_osd_out_in_cluster.py [line:51] 5
2017-05-31 13:32:45,700 INFO TC38_osd_out_in_cluster.py [line:52] 
Now operate osd.4
2017-05-31 13:32:45,700 INFO TC38_osd_out_in_cluster.py [line:53] out osd.4
2017-05-31 13:32:45,700 INFO osd.py [line:62] execute command is sudo -i ceph osd out osd.4 & sleep 3
2017-05-31 13:32:49,151 INFO osd.py [line:67] osd.4 is already out cluster
2017-05-31 13:32:49,151 INFO TC38_osd_out_in_cluster.py [line:55] check if IO error
2017-05-31 13:32:49,709 INFO osd.py [line:77] execute command is sudo -i ceph osd in osd.4 & sleep 3
2017-05-31 13:32:53,342 INFO osd.py [line:80] osd.4 is already in cluster
2017-05-31 13:33:23,943 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:33:24,678 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e120: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2413: 768 pgs, 2 pools, 1407 GB data, 375 kobjects
            198 GB used, 6793 GB / 6991 GB avail
                 768 active+clean
  client io 32051 kB/s wr, 0 op/s rd, 4006 op/s wr

2017-05-31 13:33:24,678 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:33:24,678 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:33:24,678 INFO TC38_osd_out_in_cluster.py [line:85] stop osd.4 in cluster successfully
2017-05-31 13:33:25,233 INFO TC38_osd_out_in_cluster.py [line:103] TC38_osd_out_in_cluster runs complete
