2017-05-31 13:57:52,693 INFO TC41_shutdown_two_osds_on_single_node.py [line:25] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. random pick two osds and stop them
4. start the stopped osds
5. check the cluster status
6. repeat step 2-5 on the other node

2017-05-31 13:57:53,882 INFO monitors.py [line:126]    "quorum_leader_name": "osdnode2",
stdin: is not a tty

2017-05-31 13:57:53,882 INFO monitors.py [line:129]    "quorum_leader_name": "osdnode2",
2017-05-31 13:57:53,882 INFO node.py [line:97] init osd on node osdnode2
2017-05-31 13:57:54,354 INFO node.py [line:112] osd.0  ---> processId 
2017-05-31 13:57:54,354 INFO node.py [line:112] osd.1  ---> processId 
2017-05-31 13:57:54,357 INFO node.py [line:112] osd.2  ---> processId 
2017-05-31 13:57:54,357 INFO node.py [line:112] osd.3  ---> processId 
2017-05-31 13:57:54,357 INFO node.py [line:112] osd.4  ---> processId 
2017-05-31 13:57:54,357 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:57:54,358 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=0 & sleep 3
2017-05-31 13:57:57,858 ERROR osd.py [line:34] Error when shutdown osdosd.0
2017-05-31 13:57:57,858 ERROR osd.py [line:35] sudo -i stop ceph-osd id=0 & sleep 3
2017-05-31 13:57:57,858 ERROR osd.py [line:36] tdin: is not a tty
stop: Unknown instance: ceph/0

2017-05-31 13:58:02,859 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:58:02,859 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-31 13:58:33,345 INFO osd.py [line:107] osd osd.0 is start successfully
2017-05-31 13:58:33,345 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:58:33,345 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=1 & sleep 3
2017-05-31 13:58:36,806 ERROR osd.py [line:34] Error when shutdown osdosd.1
2017-05-31 13:58:36,806 ERROR osd.py [line:35] sudo -i stop ceph-osd id=1 & sleep 3
2017-05-31 13:58:36,809 ERROR osd.py [line:36] tdin: is not a tty
stop: Unknown instance: ceph/1

2017-05-31 13:58:41,811 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:58:41,811 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-31 13:59:12,428 INFO osd.py [line:107] osd osd.1 is start successfully
2017-05-31 13:59:12,428 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:59:12,428 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=2 & sleep 3
2017-05-31 13:59:15,898 ERROR osd.py [line:34] Error when shutdown osdosd.2
2017-05-31 13:59:15,898 ERROR osd.py [line:35] sudo -i stop ceph-osd id=2 & sleep 3
2017-05-31 13:59:15,900 ERROR osd.py [line:36] tdin: is not a tty
stop: Unknown instance: ceph/2

2017-05-31 13:59:20,903 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:59:20,903 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-31 13:59:51,371 INFO osd.py [line:107] osd osd.2 is start successfully
2017-05-31 13:59:51,371 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:59:51,371 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=3 & sleep 3
2017-05-31 13:59:54,872 ERROR osd.py [line:34] Error when shutdown osdosd.3
2017-05-31 13:59:54,872 ERROR osd.py [line:35] sudo -i stop ceph-osd id=3 & sleep 3
2017-05-31 13:59:54,872 ERROR osd.py [line:36] tdin: is not a tty
stop: Unknown instance: ceph/3

2017-05-31 13:59:59,875 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:59:59,875 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-05-31 14:00:30,328 INFO osd.py [line:107] osd osd.3 is start successfully
2017-05-31 14:00:30,328 INFO osd.py [line:28] node is  osdnode2
2017-05-31 14:00:30,328 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=4 & sleep 3
2017-05-31 14:00:34,035 ERROR osd.py [line:34] Error when shutdown osdosd.4
2017-05-31 14:00:34,038 ERROR osd.py [line:35] sudo -i stop ceph-osd id=4 & sleep 3
2017-05-31 14:00:34,039 ERROR osd.py [line:36] tdin: is not a tty
stop: Unknown instance: ceph/4

2017-05-31 14:00:39,039 INFO osd.py [line:102] node is  osdnode2
2017-05-31 14:00:39,039 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-05-31 14:01:09,459 INFO osd.py [line:107] osd osd.4 is start successfully
2017-05-31 14:01:09,977 INFO node.py [line:133] osd.0  ---> processId 32488
2017-05-31 14:01:09,977 INFO node.py [line:133] osd.1  ---> processId 32679
2017-05-31 14:01:09,977 INFO node.py [line:133] osd.2  ---> processId 32867
2017-05-31 14:01:09,977 INFO node.py [line:133] osd.3  ---> processId 33058
2017-05-31 14:01:09,977 INFO node.py [line:133] osd.4  ---> processId 33247
2017-05-31 14:01:09,993 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 14:01:10,648 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e195: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3720: 768 pgs, 2 pools, 1444 GB data, 375 kobjects
            267 GB used, 6724 GB / 6991 GB avail
                 768 active+clean
  client io 20432 kB/s wr, 0 op/s rd, 2554 op/s wr

2017-05-31 14:01:10,648 INFO cluster.py [line:238] PG number is 768
2017-05-31 14:01:10,648 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 14:01:10,648 INFO cluster.py [line:302] osd on node osdnode2 were init successfully
2017-05-31 14:01:10,648 INFO node.py [line:97] init osd on node Denali@123
2017-05-31 14:01:11,424 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 14:01:12,141 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e195: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3721: 768 pgs, 2 pools, 1444 GB data, 375 kobjects
            267 GB used, 6724 GB / 6991 GB avail
                 768 active+clean
  client io 25880 kB/s wr, 0 op/s rd, 3235 op/s wr

2017-05-31 14:01:12,141 INFO cluster.py [line:238] PG number is 768
2017-05-31 14:01:12,141 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 14:01:12,141 INFO cluster.py [line:302] osd on node Denali@123 were init successfully
2017-05-31 14:01:12,141 INFO TC41_shutdown_two_osds_on_single_node.py [line:31] start to check cluster status before case running
2017-05-31 14:01:12,141 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 14:01:12,819 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e195: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3721: 768 pgs, 2 pools, 1444 GB data, 375 kobjects
            267 GB used, 6724 GB / 6991 GB avail
                 768 active+clean
  client io 25880 kB/s wr, 0 op/s rd, 3235 op/s wr

2017-05-31 14:01:12,819 INFO cluster.py [line:238] PG number is 768
2017-05-31 14:01:12,819 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 14:01:12,819 INFO TC41_shutdown_two_osds_on_single_node.py [line:34] health status is OK
2017-05-31 14:01:12,819 INFO TC41_shutdown_two_osds_on_single_node.py [line:39] 
Step1: Check IO from clients
2017-05-31 14:01:14,040 INFO client.py [line:169] ['oot      2463     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg0', 'root      2464  2463 39 04:50 ?        00:28:09 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg0', 'root      2500     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg1', 'root      2502  2500 40 04:50 ?        00:28:10 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg1', 'root      2540     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg2', 'root      2541  2540 39 04:50 ?        00:28:06 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg2', 'root      2573     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg3', 'root      2574  2573 39 04:50 ?        00:28:04 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg3', 'root      2606     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg4', 'root      2607  2606 39 04:50 ?        00:28:03 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg4', 'denali    3395  3394  0 06:01 ?        00:00:00 bash -c sudo -i ps -ef | grep fio', 'denali    3397  3395  0 06:01 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-05-31 14:01:14,040 INFO client.py [line:171] IO is running
2017-05-31 14:02:14,045 INFO TC41_shutdown_two_osds_on_single_node.py [line:47] 
Step2:shutdown osd
2017-05-31 14:02:14,045 INFO TC41_shutdown_two_osds_on_single_node.py [line:51] 
Now operate osdnode2
2017-05-31 14:02:14,551 INFO node.py [line:150] osd.0  ---> processId 32488
2017-05-31 14:02:14,551 INFO node.py [line:150] osd.1  ---> processId 32679
2017-05-31 14:02:14,552 INFO node.py [line:150] osd.2  ---> processId 32867
2017-05-31 14:02:14,552 INFO node.py [line:150] osd.3  ---> processId 33058
2017-05-31 14:02:14,552 INFO node.py [line:150] osd.4  ---> processId 33247
2017-05-31 14:02:14,552 INFO TC41_shutdown_two_osds_on_single_node.py [line:54] shutdown two osds on node osdnode2
2017-05-31 14:02:14,552 INFO osd.py [line:53] execute command is sudo -i kill 32867 & sleep 3
2017-05-31 14:02:18,032 INFO osd.py [line:53] execute command is sudo -i kill 32488 & sleep 3
2017-05-31 14:02:22,007 INFO TC41_shutdown_two_osds_on_single_node.py [line:64] start osd on node osdnode2
2017-05-31 14:02:22,007 INFO osd.py [line:102] node is  osdnode2
2017-05-31 14:02:22,007 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-31 14:02:52,516 INFO osd.py [line:107] osd osd.2 is start successfully
2017-05-31 14:02:52,516 INFO osd.py [line:102] node is  osdnode2
2017-05-31 14:02:52,516 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-31 14:03:23,292 INFO osd.py [line:107] osd osd.0 is start successfully
2017-05-31 14:03:23,292 INFO osd.py [line:115] node is  osdnode2
2017-05-31 14:03:23,292 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 2'
2017-05-31 14:03:23,780 INFO osd.py [line:118] oot     33752     1 99 14:02 ?        00:01:31 ceph-osd -i 2
root     33955 33944  0 14:03 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 2'
root     33957 33955  0 14:03 ?        00:00:00 grep ceph-osd -i 2

2017-05-31 14:03:23,780 INFO osd.py [line:127] osd.2has already started
2017-05-31 14:03:23,780 INFO osd.py [line:115] node is  osdnode2
2017-05-31 14:03:23,780 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 0'
2017-05-31 14:03:24,236 INFO osd.py [line:118] oot     33855     1 99 14:02 ?        00:00:58 ceph-osd -i 0
root     33969 33958  0 14:03 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 0'
root     33971 33969  0 14:03 ?        00:00:00 grep ceph-osd -i 0

2017-05-31 14:03:24,236 INFO osd.py [line:127] osd.0has already started
2017-05-31 14:03:54,871 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 14:03:55,579 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_WARN
            62 pgs degraded
            1 pgs recovering
            61 pgs recovery_wait
            recovery 10211/768032 objects degraded (1.330%)
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e205: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3851: 768 pgs, 2 pools, 1446 GB data, 375 kobjects
            274 GB used, 6717 GB / 6991 GB avail
            10211/768032 objects degraded (1.330%)
                 706 active+clean
                  61 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 530 MB/s, 256 objects/s
  client io 14775 kB/s wr, 0 op/s rd, 1846 op/s wr

2017-05-31 14:03:55,579 INFO cluster.py [line:238] PG number is 768
2017-05-31 14:03:55,579 INFO cluster.py [line:239] usefull PG number is 706
2017-05-31 14:04:55,594 INFO cluster.py [line:247] cost 61 seconds, left 5939 seconds when check the ceph status
2017-05-31 14:04:55,594 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 14:04:56,283 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e205: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3899: 768 pgs, 2 pools, 1447 GB data, 375 kobjects
            277 GB used, 6714 GB / 6991 GB avail
                 768 active+clean
  client io 25599 kB/s wr, 0 op/s rd, 3199 op/s wr

2017-05-31 14:04:56,283 INFO cluster.py [line:238] PG number is 768
2017-05-31 14:04:56,283 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 14:04:56,283 INFO TC41_shutdown_two_osds_on_single_node.py [line:89] stop two osds in cluster successfully
2017-05-31 14:04:56,283 INFO TC41_shutdown_two_osds_on_single_node.py [line:51] 
Now operate Denali@123
2017-05-31 14:04:56,653 INFO TC41_shutdown_two_osds_on_single_node.py [line:54] shutdown two osds on node Denali@123
