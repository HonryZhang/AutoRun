2017-05-31 13:33:25,250 INFO TC39_shutdown_osd_on_single_node.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. stop all osds in sequence
4. start all osds in sequence
5. check the cluster status
6. repeat step 2-5 on the other node

2017-05-31 13:33:25,250 INFO TC39_shutdown_osd_on_single_node.py [line:25] the timeout is 6000
2017-05-31 13:33:26,469 INFO monitors.py [line:126]    "quorum_leader_name": "osdnode2",
stdin: is not a tty

2017-05-31 13:33:26,469 INFO monitors.py [line:129]    "quorum_leader_name": "osdnode2",
2017-05-31 13:33:26,469 INFO node.py [line:97] init osd on node osdnode2
2017-05-31 13:33:27,118 INFO node.py [line:112] osd.0  ---> processId 26138
2017-05-31 13:33:27,118 INFO node.py [line:112] osd.1  ---> processId 26475
2017-05-31 13:33:27,118 INFO node.py [line:112] osd.2  ---> processId 26811
2017-05-31 13:33:27,118 INFO node.py [line:112] osd.3  ---> processId 27151
2017-05-31 13:33:27,118 INFO node.py [line:112] osd.4  ---> processId 27497
2017-05-31 13:33:27,118 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:33:27,121 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=0 & sleep 3
2017-05-31 13:33:32,736 INFO osd.py [line:32] osd osd.0 is shutdown successfully
2017-05-31 13:33:37,737 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:33:37,737 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-31 13:34:08,200 INFO osd.py [line:107] osd osd.0 is start successfully
2017-05-31 13:34:08,200 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:34:08,200 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=1 & sleep 3
2017-05-31 13:34:14,128 INFO osd.py [line:32] osd osd.1 is shutdown successfully
2017-05-31 13:34:19,131 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:34:19,131 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-31 13:34:49,743 INFO osd.py [line:107] osd osd.1 is start successfully
2017-05-31 13:34:49,743 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:34:49,743 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=2 & sleep 3
2017-05-31 13:34:55,374 INFO osd.py [line:32] osd osd.2 is shutdown successfully
2017-05-31 13:35:00,375 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:35:00,375 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-31 13:35:31,029 INFO osd.py [line:107] osd osd.2 is start successfully
2017-05-31 13:35:31,029 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:35:31,029 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=3 & sleep 3
2017-05-31 13:35:37,015 INFO osd.py [line:32] osd osd.3 is shutdown successfully
2017-05-31 13:35:42,016 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:35:42,016 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-05-31 13:36:12,578 INFO osd.py [line:107] osd osd.3 is start successfully
2017-05-31 13:36:12,578 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:36:12,578 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=4 & sleep 3
2017-05-31 13:36:18,223 INFO osd.py [line:32] osd osd.4 is shutdown successfully
2017-05-31 13:36:23,233 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:36:23,233 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-05-31 13:36:53,704 INFO osd.py [line:107] osd osd.4 is start successfully
2017-05-31 13:36:54,184 INFO node.py [line:133] osd.0  ---> processId 30403
2017-05-31 13:36:54,184 INFO node.py [line:133] osd.1  ---> processId 30534
2017-05-31 13:36:54,184 INFO node.py [line:133] osd.2  ---> processId 30665
2017-05-31 13:36:54,184 INFO node.py [line:133] osd.3  ---> processId 30788
2017-05-31 13:36:54,184 INFO node.py [line:133] osd.4  ---> processId 30919
2017-05-31 13:36:54,184 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:36:54,848 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_WARN
            51 pgs degraded
            1 pgs recovering
            50 pgs recovery_wait
            recovery 4689/768032 objects degraded (0.611%)
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e146: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2579: 768 pgs, 2 pools, 1415 GB data, 375 kobjects
            208 GB used, 6783 GB / 6991 GB avail
            4689/768032 objects degraded (0.611%)
                 717 active+clean
                  50 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 833 MB/s, 676 objects/s
  client io 34314 kB/s wr, 0 op/s rd, 4289 op/s wr

2017-05-31 13:36:54,848 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:36:54,848 INFO cluster.py [line:239] usefull PG number is 717
2017-05-31 13:37:54,849 INFO cluster.py [line:247] cost 60 seconds, left 5940 seconds when check the ceph status
2017-05-31 13:37:54,851 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:37:55,551 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e146: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2622: 768 pgs, 2 pools, 1417 GB data, 375 kobjects
            210 GB used, 6780 GB / 6991 GB avail
                 768 active+clean
  client io 22818 kB/s wr, 0 op/s rd, 2852 op/s wr

2017-05-31 13:37:55,551 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:37:55,551 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:37:55,551 INFO cluster.py [line:302] osd on node osdnode2 were init successfully
2017-05-31 13:37:55,566 INFO node.py [line:97] init osd on node Denali@123
2017-05-31 13:37:56,342 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:37:57,069 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e146: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2623: 768 pgs, 2 pools, 1417 GB data, 375 kobjects
            210 GB used, 6780 GB / 6991 GB avail
                 768 active+clean
  client io 12339 kB/s wr, 0 op/s rd, 1542 op/s wr

2017-05-31 13:37:57,069 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:37:57,069 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:37:57,069 INFO cluster.py [line:302] osd on node Denali@123 were init successfully
2017-05-31 13:37:57,084 INFO TC39_shutdown_osd_on_single_node.py [line:31] start to check cluster status before case running
2017-05-31 13:37:57,161 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:37:57,801 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e146: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2624: 768 pgs, 2 pools, 1417 GB data, 375 kobjects
            211 GB used, 6780 GB / 6991 GB avail
                 768 active+clean
  client io 28081 kB/s wr, 0 op/s rd, 3510 op/s wr

2017-05-31 13:37:57,803 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:37:57,803 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:37:57,805 INFO TC39_shutdown_osd_on_single_node.py [line:34] health status is OK
2017-05-31 13:37:57,809 INFO TC39_shutdown_osd_on_single_node.py [line:39] 
Step1: Check IO from clients
2017-05-31 13:37:58,963 INFO client.py [line:169] ['oot      2463     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg0', 'root      2464  2463 40 04:50 ?        00:18:53 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg0', 'root      2500     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg1', 'root      2502  2500 40 04:50 ?        00:18:51 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg1', 'root      2540     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg2', 'root      2541  2540 39 04:50 ?        00:18:49 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg2', 'root      2573     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg3', 'root      2574  2573 39 04:50 ?        00:18:47 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg3', 'root      2606     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg4', 'root      2607  2606 39 04:50 ?        00:18:47 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg4', 'denali    3141  3140  0 05:38 ?        00:00:00 bash -c sudo -i ps -ef | grep fio', 'denali    3143  3141  0 05:38 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-05-31 13:37:58,979 INFO client.py [line:171] IO is running
2017-05-31 13:38:59,006 INFO TC39_shutdown_osd_on_single_node.py [line:48] 
Step 2: stop osd and check IO
2017-05-31 13:38:59,006 INFO TC39_shutdown_osd_on_single_node.py [line:51] 
Now operate osd on osdnode2
2017-05-31 13:38:59,006 INFO TC39_shutdown_osd_on_single_node.py [line:54] 
Now operate osd.0
2017-05-31 13:38:59,006 INFO TC39_shutdown_osd_on_single_node.py [line:57] Set the osd.0 pid for kill
2017-05-31 13:38:59,535 INFO node.py [line:150] osd.0  ---> processId 30403
2017-05-31 13:38:59,535 INFO node.py [line:150] osd.1  ---> processId 30534
2017-05-31 13:38:59,535 INFO node.py [line:150] osd.2  ---> processId 30665
2017-05-31 13:38:59,535 INFO node.py [line:150] osd.3  ---> processId 30788
2017-05-31 13:38:59,535 INFO node.py [line:150] osd.4  ---> processId 30919
2017-05-31 13:38:59,535 INFO TC39_shutdown_osd_on_single_node.py [line:59] shutdown osd.0 by kill
2017-05-31 13:38:59,549 INFO osd.py [line:53] execute command is sudo -i kill 30403 & sleep 3
2017-05-31 13:39:08,614 INFO TC39_shutdown_osd_on_single_node.py [line:65] start osd.0
2017-05-31 13:39:08,614 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:39:08,614 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-31 13:39:39,145 INFO osd.py [line:107] osd osd.0 is start successfully
2017-05-31 13:39:39,145 INFO osd.py [line:115] node is  osdnode2
2017-05-31 13:39:39,145 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 0'
2017-05-31 13:39:39,657 INFO osd.py [line:118] oot     31179     1 99 13:39 ?        00:00:43 ceph-osd -i 0
root     31278 31266  0 13:39 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 0'
root     31280 31278  0 13:39 ?        00:00:00 grep ceph-osd -i 0

2017-05-31 13:39:39,657 INFO osd.py [line:127] osd.0has already started
2017-05-31 13:40:10,164 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:40:10,808 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e151: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2718: 768 pgs, 2 pools, 1421 GB data, 375 kobjects
            216 GB used, 6774 GB / 6991 GB avail
                 768 active+clean
  client io 26810 kB/s wr, 0 op/s rd, 3351 op/s wr

2017-05-31 13:40:10,808 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:40:10,808 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:40:10,808 INFO TC39_shutdown_osd_on_single_node.py [line:81] stop osd.0 in cluster successfully
2017-05-31 13:40:10,822 INFO TC39_shutdown_osd_on_single_node.py [line:54] 
Now operate osd.1
2017-05-31 13:40:10,822 INFO TC39_shutdown_osd_on_single_node.py [line:57] Set the osd.1 pid for kill
2017-05-31 13:40:16,313 INFO node.py [line:150] osd.0  ---> processId 31179
2017-05-31 13:40:16,315 INFO node.py [line:150] osd.1  ---> processId 30534
2017-05-31 13:40:16,318 INFO node.py [line:150] osd.2  ---> processId 30665
2017-05-31 13:40:16,321 INFO node.py [line:150] osd.3  ---> processId 30788
2017-05-31 13:40:16,322 INFO node.py [line:150] osd.4  ---> processId 30919
2017-05-31 13:40:16,325 INFO TC39_shutdown_osd_on_single_node.py [line:59] shutdown osd.1 by kill
2017-05-31 13:40:16,326 INFO osd.py [line:53] execute command is sudo -i kill 30534 & sleep 3
2017-05-31 13:40:25,315 INFO TC39_shutdown_osd_on_single_node.py [line:65] start osd.1
2017-05-31 13:40:25,318 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:40:25,319 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-31 13:40:55,750 INFO osd.py [line:107] osd osd.1 is start successfully
2017-05-31 13:40:55,750 INFO osd.py [line:115] node is  osdnode2
2017-05-31 13:40:55,750 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 1'
2017-05-31 13:40:56,289 INFO osd.py [line:118] oot     31366     1 99 13:40 ?        00:00:48 ceph-osd -i 1
root     31463 31452  0 13:40 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 1'
root     31465 31463  0 13:40 ?        00:00:00 grep ceph-osd -i 1

2017-05-31 13:40:56,289 INFO osd.py [line:127] osd.1has already started
2017-05-31 13:41:26,861 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:41:27,642 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_WARN
            7 pgs degraded
            1 pgs recovering
            6 pgs recovery_wait
            recovery 461/768032 objects degraded (0.060%)
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e156: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2779: 768 pgs, 2 pools, 1423 GB data, 375 kobjects
            220 GB used, 6771 GB / 6991 GB avail
            461/768032 objects degraded (0.060%)
                 761 active+clean
                   6 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 537 MB/s, 429 objects/s
  client io 18162 kB/s wr, 0 op/s rd, 2270 op/s wr

2017-05-31 13:41:27,642 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:41:27,657 INFO cluster.py [line:239] usefull PG number is 761
2017-05-31 13:42:27,660 INFO cluster.py [line:247] cost 61 seconds, left 5939 seconds when check the ceph status
2017-05-31 13:42:27,660 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:42:28,335 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e156: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2830: 768 pgs, 2 pools, 1424 GB data, 375 kobjects
            223 GB used, 6768 GB / 6991 GB avail
                 768 active+clean
  client io 35137 kB/s wr, 0 op/s rd, 4392 op/s wr

2017-05-31 13:42:28,338 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:42:28,338 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:42:28,341 INFO TC39_shutdown_osd_on_single_node.py [line:81] stop osd.1 in cluster successfully
2017-05-31 13:42:28,341 INFO TC39_shutdown_osd_on_single_node.py [line:54] 
Now operate osd.2
2017-05-31 13:42:28,341 INFO TC39_shutdown_osd_on_single_node.py [line:57] Set the osd.2 pid for kill
2017-05-31 13:42:28,839 INFO node.py [line:150] osd.0  ---> processId 31179
2017-05-31 13:42:28,839 INFO node.py [line:150] osd.1  ---> processId 31366
2017-05-31 13:42:28,839 INFO node.py [line:150] osd.2  ---> processId 30665
2017-05-31 13:42:28,839 INFO node.py [line:150] osd.3  ---> processId 30788
2017-05-31 13:42:28,839 INFO node.py [line:150] osd.4  ---> processId 30919
2017-05-31 13:42:28,839 INFO TC39_shutdown_osd_on_single_node.py [line:59] shutdown osd.2 by kill
2017-05-31 13:42:28,839 INFO osd.py [line:53] execute command is sudo -i kill 30665 & sleep 3
2017-05-31 13:42:37,884 INFO TC39_shutdown_osd_on_single_node.py [line:65] start osd.2
2017-05-31 13:42:37,885 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:42:37,885 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-31 13:43:08,348 INFO osd.py [line:107] osd osd.2 is start successfully
2017-05-31 13:43:08,348 INFO osd.py [line:115] node is  osdnode2
2017-05-31 13:43:08,348 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 2'
2017-05-31 13:43:08,930 INFO osd.py [line:118] oot     31588     1 99 13:42 ?        00:00:41 ceph-osd -i 2
root     31688 31677  0 13:43 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 2'
root     31690 31688  0 13:43 ?        00:00:00 grep ceph-osd -i 2

2017-05-31 13:43:08,930 INFO osd.py [line:127] osd.2has already started
2017-05-31 13:43:39,805 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:43:40,509 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e161: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2893: 768 pgs, 2 pools, 1426 GB data, 375 kobjects
            226 GB used, 6765 GB / 6991 GB avail
                 768 active+clean
recovery io 145 MB/s, 72 objects/s
  client io 21813 kB/s wr, 0 op/s rd, 2726 op/s wr

2017-05-31 13:43:40,509 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:43:40,509 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:43:40,509 INFO TC39_shutdown_osd_on_single_node.py [line:81] stop osd.2 in cluster successfully
2017-05-31 13:43:40,509 INFO TC39_shutdown_osd_on_single_node.py [line:54] 
Now operate osd.3
2017-05-31 13:43:40,509 INFO TC39_shutdown_osd_on_single_node.py [line:57] Set the osd.3 pid for kill
2017-05-31 13:43:41,017 INFO node.py [line:150] osd.0  ---> processId 31179
2017-05-31 13:43:41,017 INFO node.py [line:150] osd.1  ---> processId 31366
2017-05-31 13:43:41,017 INFO node.py [line:150] osd.2  ---> processId 31588
2017-05-31 13:43:41,017 INFO node.py [line:150] osd.3  ---> processId 30788
2017-05-31 13:43:41,017 INFO node.py [line:150] osd.4  ---> processId 30919
2017-05-31 13:43:41,017 INFO TC39_shutdown_osd_on_single_node.py [line:59] shutdown osd.3 by kill
2017-05-31 13:43:41,017 INFO osd.py [line:53] execute command is sudo -i kill 30788 & sleep 3
2017-05-31 13:43:50,069 INFO TC39_shutdown_osd_on_single_node.py [line:65] start osd.3
2017-05-31 13:43:50,069 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:43:50,069 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-05-31 13:44:20,522 INFO osd.py [line:107] osd osd.3 is start successfully
2017-05-31 13:44:20,522 INFO osd.py [line:115] node is  osdnode2
2017-05-31 13:44:20,522 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 3'
2017-05-31 13:44:21,030 INFO osd.py [line:118] oot     31771     1 99 13:43 ?        00:00:45 ceph-osd -i 3
root     31876 31865  0 13:44 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 3'
root     31878 31876  0 13:44 ?        00:00:00 grep ceph-osd -i 3

2017-05-31 13:44:21,030 INFO osd.py [line:127] osd.3has already started
2017-05-31 13:44:51,638 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:44:52,335 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_WARN
            7 pgs degraded
            2 pgs recovering
            5 pgs recovery_wait
            recovery 377/768032 objects degraded (0.049%)
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e166: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v2954: 768 pgs, 2 pools, 1428 GB data, 375 kobjects
            229 GB used, 6762 GB / 6991 GB avail
            377/768032 objects degraded (0.049%)
                 761 active+clean
                   5 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 822 MB/s, 607 objects/s
  client io 22769 kB/s wr, 0 op/s rd, 2846 op/s wr

2017-05-31 13:44:52,335 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:44:52,335 INFO cluster.py [line:239] usefull PG number is 761
2017-05-31 13:45:52,348 INFO cluster.py [line:247] cost 61 seconds, left 5939 seconds when check the ceph status
2017-05-31 13:45:52,348 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:45:53,121 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e166: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3006: 768 pgs, 2 pools, 1429 GB data, 375 kobjects
            232 GB used, 6759 GB / 6991 GB avail
                 768 active+clean
  client io 20470 kB/s wr, 0 op/s rd, 2558 op/s wr

2017-05-31 13:45:53,121 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:45:53,121 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:45:53,125 INFO TC39_shutdown_osd_on_single_node.py [line:81] stop osd.3 in cluster successfully
2017-05-31 13:45:53,125 INFO TC39_shutdown_osd_on_single_node.py [line:54] 
Now operate osd.4
2017-05-31 13:45:53,125 INFO TC39_shutdown_osd_on_single_node.py [line:57] Set the osd.4 pid for kill
2017-05-31 13:45:53,769 INFO node.py [line:150] osd.0  ---> processId 31179
2017-05-31 13:45:53,769 INFO node.py [line:150] osd.1  ---> processId 31366
2017-05-31 13:45:53,770 INFO node.py [line:150] osd.2  ---> processId 31588
2017-05-31 13:45:53,770 INFO node.py [line:150] osd.3  ---> processId 31771
2017-05-31 13:45:53,772 INFO node.py [line:150] osd.4  ---> processId 30919
2017-05-31 13:45:53,773 INFO TC39_shutdown_osd_on_single_node.py [line:59] shutdown osd.4 by kill
2017-05-31 13:45:53,773 INFO osd.py [line:53] execute command is sudo -i kill 30919 & sleep 3
2017-05-31 13:46:02,805 INFO TC39_shutdown_osd_on_single_node.py [line:65] start osd.4
2017-05-31 13:46:02,805 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:46:02,805 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-05-31 13:46:33,279 INFO osd.py [line:107] osd osd.4 is start successfully
2017-05-31 13:46:33,279 INFO osd.py [line:115] node is  osdnode2
2017-05-31 13:46:33,279 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 4'
2017-05-31 13:46:33,776 INFO osd.py [line:118] oot     32007     1 99 13:46 ?        00:00:43 ceph-osd -i 4
root     32105 32094  0 13:46 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 4'
root     32107 32105  0 13:46 ?        00:00:00 grep ceph-osd -i 4

2017-05-31 13:46:33,776 INFO osd.py [line:127] osd.4has already started
2017-05-31 13:47:04,368 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:47:04,983 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e171: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3067: 768 pgs, 2 pools, 1431 GB data, 375 kobjects
            235 GB used, 6756 GB / 6991 GB avail
                 768 active+clean
  client io 24348 kB/s wr, 0 op/s rd, 3043 op/s wr

2017-05-31 13:47:04,983 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:47:04,986 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:47:04,986 INFO TC39_shutdown_osd_on_single_node.py [line:81] stop osd.4 in cluster successfully
2017-05-31 13:47:04,989 INFO TC39_shutdown_osd_on_single_node.py [line:51] 
Now operate osd on Denali@123
2017-05-31 13:47:05,559 INFO TC39_shutdown_osd_on_single_node.py [line:100] TC39_shutdown_osd_on_single_node runs complete
