2017-05-31 13:47:05,575 INFO TC40_kill_osd_on_single_node.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. kill all osds in sequence
4. start all osds in sequence
5. check the cluster status
6. repeat step 2-5 on the other node

2017-05-31 13:47:06,956 INFO monitors.py [line:126]    "quorum_leader_name": "osdnode2",
stdin: is not a tty

2017-05-31 13:47:06,956 INFO monitors.py [line:129]    "quorum_leader_name": "osdnode2",
2017-05-31 13:47:06,956 INFO node.py [line:97] init osd on node osdnode2
2017-05-31 13:47:07,516 INFO node.py [line:112] osd.0  ---> processId 
2017-05-31 13:47:07,516 INFO node.py [line:112] osd.1  ---> processId 
2017-05-31 13:47:07,516 INFO node.py [line:112] osd.2  ---> processId 
2017-05-31 13:47:07,516 INFO node.py [line:112] osd.3  ---> processId 
2017-05-31 13:47:07,516 INFO node.py [line:112] osd.4  ---> processId 
2017-05-31 13:47:07,516 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:47:07,516 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=0 & sleep 3
2017-05-31 13:47:11,174 ERROR osd.py [line:34] Error when shutdown osdosd.0
2017-05-31 13:47:11,174 ERROR osd.py [line:35] sudo -i stop ceph-osd id=0 & sleep 3
2017-05-31 13:47:11,174 ERROR osd.py [line:36] tdin: is not a tty
stop: Unknown instance: ceph/0

2017-05-31 13:47:16,184 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:47:16,184 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-31 13:47:46,650 INFO osd.py [line:107] osd osd.0 is start successfully
2017-05-31 13:47:46,650 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:47:46,650 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=1 & sleep 3
2017-05-31 13:47:50,148 ERROR osd.py [line:34] Error when shutdown osdosd.1
2017-05-31 13:47:50,148 ERROR osd.py [line:35] sudo -i stop ceph-osd id=1 & sleep 3
2017-05-31 13:47:50,148 ERROR osd.py [line:36] tdin: is not a tty
stop: Unknown instance: ceph/1

2017-05-31 13:47:55,161 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:47:55,161 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-31 13:48:25,644 INFO osd.py [line:107] osd osd.1 is start successfully
2017-05-31 13:48:25,644 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:48:25,644 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=2 & sleep 3
2017-05-31 13:48:29,115 ERROR osd.py [line:34] Error when shutdown osdosd.2
2017-05-31 13:48:29,115 ERROR osd.py [line:35] sudo -i stop ceph-osd id=2 & sleep 3
2017-05-31 13:48:29,115 ERROR osd.py [line:36] tdin: is not a tty
stop: Unknown instance: ceph/2

2017-05-31 13:48:34,118 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:48:34,118 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-31 13:49:04,799 INFO osd.py [line:107] osd osd.2 is start successfully
2017-05-31 13:49:04,802 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:49:04,802 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=3 & sleep 3
2017-05-31 13:49:08,348 ERROR osd.py [line:34] Error when shutdown osdosd.3
2017-05-31 13:49:08,348 ERROR osd.py [line:35] sudo -i stop ceph-osd id=3 & sleep 3
2017-05-31 13:49:08,348 ERROR osd.py [line:36] tdin: is not a tty
stop: Unknown instance: ceph/3

2017-05-31 13:49:13,364 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:49:13,365 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-05-31 13:49:43,828 INFO osd.py [line:107] osd osd.3 is start successfully
2017-05-31 13:49:43,828 INFO osd.py [line:28] node is  osdnode2
2017-05-31 13:49:43,828 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=4 & sleep 3
2017-05-31 13:49:47,289 ERROR osd.py [line:34] Error when shutdown osdosd.4
2017-05-31 13:49:47,289 ERROR osd.py [line:35] sudo -i stop ceph-osd id=4 & sleep 3
2017-05-31 13:49:47,289 ERROR osd.py [line:36] tdin: is not a tty
stop: Unknown instance: ceph/4

2017-05-31 13:49:52,301 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:49:52,301 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-05-31 13:50:22,884 INFO osd.py [line:107] osd osd.4 is start successfully
2017-05-31 13:50:23,404 INFO node.py [line:133] osd.0  ---> processId 31179
2017-05-31 13:50:23,404 INFO node.py [line:133] osd.1  ---> processId 31366
2017-05-31 13:50:23,404 INFO node.py [line:133] osd.2  ---> processId 31588
2017-05-31 13:50:23,404 INFO node.py [line:133] osd.3  ---> processId 31771
2017-05-31 13:50:23,404 INFO node.py [line:133] osd.4  ---> processId 32007
2017-05-31 13:50:23,404 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:50:24,062 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e171: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3234: 768 pgs, 2 pools, 1435 GB data, 375 kobjects
            243 GB used, 6747 GB / 6991 GB avail
                 768 active+clean
  client io 24592 kB/s wr, 0 op/s rd, 3074 op/s wr

2017-05-31 13:50:24,062 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:50:24,062 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:50:24,062 INFO cluster.py [line:302] osd on node osdnode2 were init successfully
2017-05-31 13:50:24,062 INFO node.py [line:97] init osd on node Denali@123
2017-05-31 13:50:25,000 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:50:25,749 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e171: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3235: 768 pgs, 2 pools, 1435 GB data, 375 kobjects
            243 GB used, 6747 GB / 6991 GB avail
                 768 active+clean
  client io 25683 kB/s wr, 0 op/s rd, 3210 op/s wr

2017-05-31 13:50:25,749 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:50:25,749 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:50:25,749 INFO cluster.py [line:302] osd on node Denali@123 were init successfully
2017-05-31 13:50:25,749 INFO TC40_kill_osd_on_single_node.py [line:31] start to check cluster status before case running
2017-05-31 13:50:25,862 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:50:26,471 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e171: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3236: 768 pgs, 2 pools, 1435 GB data, 375 kobjects
            243 GB used, 6747 GB / 6991 GB avail
                 768 active+clean
  client io 26412 kB/s wr, 0 op/s rd, 3301 op/s wr

2017-05-31 13:50:26,471 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:50:26,471 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:50:26,471 INFO TC40_kill_osd_on_single_node.py [line:34] health status is OK
2017-05-31 13:50:26,471 INFO TC40_kill_osd_on_single_node.py [line:39] 
Step1: Check IO from clients
2017-05-31 13:50:27,645 INFO client.py [line:169] ['oot      2463     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg0', 'root      2464  2463 40 04:50 ?        00:23:51 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg0', 'root      2500     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg1', 'root      2502  2500 40 04:50 ?        00:23:51 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg1', 'root      2540     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg2', 'root      2541  2540 39 04:50 ?        00:23:47 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg2', 'root      2573     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg3', 'root      2574  2573 39 04:50 ?        00:23:47 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg3', 'root      2606     1  0 04:50 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg4', 'root      2607  2606 39 04:50 ?        00:23:46 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=linli -pool=reliablityTestPool -rbdname=linlirbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=linlirbdImg4', 'denali    3251  3250  0 05:50 ?        00:00:00 bash -c sudo -i ps -ef | grep fio', 'denali    3253  3251  0 05:50 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-05-31 13:50:27,648 INFO client.py [line:171] IO is running
2017-05-31 13:51:27,658 INFO TC40_kill_osd_on_single_node.py [line:47] 
Step 2: Kill osd and check IO
2017-05-31 13:51:27,658 INFO TC40_kill_osd_on_single_node.py [line:49] 
Now operate node osdnode2
2017-05-31 13:51:27,658 INFO TC40_kill_osd_on_single_node.py [line:52] 
Now operate osd.0
2017-05-31 13:51:27,658 INFO TC40_kill_osd_on_single_node.py [line:54] Set the osd.0 pid for kill
2017-05-31 13:51:28,102 INFO node.py [line:150] osd.0  ---> processId 31179
2017-05-31 13:51:28,102 INFO node.py [line:150] osd.1  ---> processId 31366
2017-05-31 13:51:28,102 INFO node.py [line:150] osd.2  ---> processId 31588
2017-05-31 13:51:28,102 INFO node.py [line:150] osd.3  ---> processId 31771
2017-05-31 13:51:28,102 INFO node.py [line:150] osd.4  ---> processId 32007
2017-05-31 13:51:28,118 INFO TC40_kill_osd_on_single_node.py [line:56] shutdown osd.0 by kill
2017-05-31 13:51:28,118 INFO osd.py [line:40] execute command is sudo -i kill -9 31179 & sleep 3
2017-05-31 13:51:42,213 INFO TC40_kill_osd_on_single_node.py [line:62] start osd.0
2017-05-31 13:51:42,217 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:51:42,217 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-31 13:52:12,684 INFO osd.py [line:107] osd osd.0 is start successfully
2017-05-31 13:52:12,684 INFO osd.py [line:115] node is  osdnode2
2017-05-31 13:52:12,684 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 0'
2017-05-31 13:52:13,328 INFO osd.py [line:118] oot     32488     1 99 13:51 ?        00:00:32 ceph-osd -i 0
root     32592 32581  0 13:52 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 0'
root     32594 32592  0 13:52 ?        00:00:00 grep ceph-osd -i 0

2017-05-31 13:52:13,328 INFO osd.py [line:127] osd.0has already started
2017-05-31 13:52:43,862 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:52:44,520 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e176: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3344: 768 pgs, 2 pools, 1437 GB data, 375 kobjects
            248 GB used, 6742 GB / 6991 GB avail
                 768 active+clean
  client io 17130 kB/s wr, 0 op/s rd, 2141 op/s wr

2017-05-31 13:52:44,520 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:52:44,520 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:52:44,520 INFO TC40_kill_osd_on_single_node.py [line:78] stop osd.0 in cluster successfully
2017-05-31 13:52:44,520 INFO TC40_kill_osd_on_single_node.py [line:52] 
Now operate osd.1
2017-05-31 13:52:44,520 INFO TC40_kill_osd_on_single_node.py [line:54] Set the osd.1 pid for kill
2017-05-31 13:52:45,059 INFO node.py [line:150] osd.0  ---> processId 32488
2017-05-31 13:52:45,059 INFO node.py [line:150] osd.1  ---> processId 31366
2017-05-31 13:52:45,059 INFO node.py [line:150] osd.2  ---> processId 31588
2017-05-31 13:52:45,059 INFO node.py [line:150] osd.3  ---> processId 31771
2017-05-31 13:52:45,059 INFO node.py [line:150] osd.4  ---> processId 32007
2017-05-31 13:52:45,059 INFO TC40_kill_osd_on_single_node.py [line:56] shutdown osd.1 by kill
2017-05-31 13:52:45,059 INFO osd.py [line:40] execute command is sudo -i kill -9 31366 & sleep 3
2017-05-31 13:52:59,163 INFO TC40_kill_osd_on_single_node.py [line:62] start osd.1
2017-05-31 13:52:59,163 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:52:59,163 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-31 13:53:29,454 INFO osd.py [line:107] osd osd.1 is start successfully
2017-05-31 13:53:29,454 INFO osd.py [line:115] node is  osdnode2
2017-05-31 13:53:29,457 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 1'
2017-05-31 13:53:30,068 INFO osd.py [line:118] oot     32679     1 99 13:53 ?        00:00:37 ceph-osd -i 1
root     32776 32765  0 13:53 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 1'
root     32778 32776  0 13:53 ?        00:00:00 grep ceph-osd -i 1

2017-05-31 13:53:30,068 INFO osd.py [line:127] osd.1has already started
2017-05-31 13:54:00,737 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:54:01,388 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e181: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3402: 768 pgs, 2 pools, 1438 GB data, 375 kobjects
            251 GB used, 6740 GB / 6991 GB avail
                 768 active+clean
  client io 27255 kB/s wr, 0 op/s rd, 3406 op/s wr

2017-05-31 13:54:01,391 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:54:01,391 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:54:01,394 INFO TC40_kill_osd_on_single_node.py [line:78] stop osd.1 in cluster successfully
2017-05-31 13:54:01,394 INFO TC40_kill_osd_on_single_node.py [line:52] 
Now operate osd.2
2017-05-31 13:54:01,394 INFO TC40_kill_osd_on_single_node.py [line:54] Set the osd.2 pid for kill
2017-05-31 13:54:01,887 INFO node.py [line:150] osd.0  ---> processId 32488
2017-05-31 13:54:01,890 INFO node.py [line:150] osd.1  ---> processId 32679
2017-05-31 13:54:01,890 INFO node.py [line:150] osd.2  ---> processId 31588
2017-05-31 13:54:01,891 INFO node.py [line:150] osd.3  ---> processId 31771
2017-05-31 13:54:01,894 INFO node.py [line:150] osd.4  ---> processId 32007
2017-05-31 13:54:01,894 INFO TC40_kill_osd_on_single_node.py [line:56] shutdown osd.2 by kill
2017-05-31 13:54:01,897 INFO osd.py [line:40] execute command is sudo -i kill -9 31588 & sleep 3
2017-05-31 13:54:16,161 INFO TC40_kill_osd_on_single_node.py [line:62] start osd.2
2017-05-31 13:54:16,161 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:54:16,161 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-31 13:54:46,586 INFO osd.py [line:107] osd osd.2 is start successfully
2017-05-31 13:54:46,586 INFO osd.py [line:115] node is  osdnode2
2017-05-31 13:54:46,589 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 2'
2017-05-31 13:54:47,098 INFO osd.py [line:118] oot     32867     1 99 13:54 ?        00:00:36 ceph-osd -i 2
root     32964 32953  0 13:54 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 2'
root     32966 32964  0 13:54 ?        00:00:00 grep ceph-osd -i 2

2017-05-31 13:54:47,098 INFO osd.py [line:127] osd.2has already started
2017-05-31 13:55:17,698 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:55:18,404 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e186: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3460: 768 pgs, 2 pools, 1439 GB data, 375 kobjects
            253 GB used, 6737 GB / 6991 GB avail
                 768 active+clean
  client io 31476 kB/s wr, 0 op/s rd, 3934 op/s wr

2017-05-31 13:55:18,404 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:55:18,404 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:55:18,404 INFO TC40_kill_osd_on_single_node.py [line:78] stop osd.2 in cluster successfully
2017-05-31 13:55:18,404 INFO TC40_kill_osd_on_single_node.py [line:52] 
Now operate osd.3
2017-05-31 13:55:18,404 INFO TC40_kill_osd_on_single_node.py [line:54] Set the osd.3 pid for kill
2017-05-31 13:55:18,894 INFO node.py [line:150] osd.0  ---> processId 32488
2017-05-31 13:55:18,894 INFO node.py [line:150] osd.1  ---> processId 32679
2017-05-31 13:55:18,894 INFO node.py [line:150] osd.2  ---> processId 32867
2017-05-31 13:55:18,894 INFO node.py [line:150] osd.3  ---> processId 31771
2017-05-31 13:55:18,894 INFO node.py [line:150] osd.4  ---> processId 32007
2017-05-31 13:55:18,894 INFO TC40_kill_osd_on_single_node.py [line:56] shutdown osd.3 by kill
2017-05-31 13:55:18,894 INFO osd.py [line:40] execute command is sudo -i kill -9 31771 & sleep 3
2017-05-31 13:55:33,118 INFO TC40_kill_osd_on_single_node.py [line:62] start osd.3
2017-05-31 13:55:33,118 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:55:33,118 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-05-31 13:56:03,414 INFO osd.py [line:107] osd osd.3 is start successfully
2017-05-31 13:56:03,414 INFO osd.py [line:115] node is  osdnode2
2017-05-31 13:56:03,414 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 3'
2017-05-31 13:56:03,913 INFO osd.py [line:118] oot     33058     1 99 13:55 ?        00:00:39 ceph-osd -i 3
root     33163 33152  0 13:56 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 3'
root     33165 33163  0 13:56 ?        00:00:00 grep ceph-osd -i 3

2017-05-31 13:56:03,913 INFO osd.py [line:127] osd.3has already started
2017-05-31 13:56:34,496 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:56:35,210 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e190: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3507: 768 pgs, 2 pools, 1440 GB data, 375 kobjects
            256 GB used, 6735 GB / 6991 GB avail
                 768 active+clean
  client io 25090 kB/s wr, 0 op/s rd, 3136 op/s wr

2017-05-31 13:56:35,210 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:56:35,210 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:56:35,210 INFO TC40_kill_osd_on_single_node.py [line:78] stop osd.3 in cluster successfully
2017-05-31 13:56:35,220 INFO TC40_kill_osd_on_single_node.py [line:52] 
Now operate osd.4
2017-05-31 13:56:35,220 INFO TC40_kill_osd_on_single_node.py [line:54] Set the osd.4 pid for kill
2017-05-31 13:56:35,812 INFO node.py [line:150] osd.0  ---> processId 32488
2017-05-31 13:56:35,812 INFO node.py [line:150] osd.1  ---> processId 32679
2017-05-31 13:56:35,815 INFO node.py [line:150] osd.2  ---> processId 32867
2017-05-31 13:56:35,815 INFO node.py [line:150] osd.3  ---> processId 33058
2017-05-31 13:56:35,816 INFO node.py [line:150] osd.4  ---> processId 32007
2017-05-31 13:56:35,819 INFO TC40_kill_osd_on_single_node.py [line:56] shutdown osd.4 by kill
2017-05-31 13:56:35,822 INFO osd.py [line:40] execute command is sudo -i kill -9 32007 & sleep 3
2017-05-31 13:56:50,076 INFO TC40_kill_osd_on_single_node.py [line:62] start osd.4
2017-05-31 13:56:50,076 INFO osd.py [line:102] node is  osdnode2
2017-05-31 13:56:50,076 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-05-31 13:57:20,430 INFO osd.py [line:107] osd osd.4 is start successfully
2017-05-31 13:57:20,430 INFO osd.py [line:115] node is  osdnode2
2017-05-31 13:57:20,430 INFO osd.py [line:116] execute command is ps -ef | grep 'ceph-osd -i 4'
2017-05-31 13:57:20,941 INFO osd.py [line:118] oot     33247     1 99 13:56 ?        00:00:32 ceph-osd -i 4
root     33347 33336  0 13:57 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 4'
root     33349 33347  0 13:57 ?        00:00:00 grep ceph-osd -i 4

2017-05-31 13:57:20,941 INFO osd.py [line:127] osd.4has already started
2017-05-31 13:57:51,502 INFO cluster.py [line:211] execute command is ceph -s
2017-05-31 13:57:52,167 INFO cluster.py [line:213]    cluster 82f4312d-32e6-446d-ad7c-3d845851566c
     health HEALTH_OK
     monmap e1: 1 mons at {osdnode2=192.168.28.237:6789/0}
            election epoch 3, quorum 0 osdnode2
     osdmap e195: 10 osds: 10 up, 10 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v3562: 768 pgs, 2 pools, 1441 GB data, 375 kobjects
            258 GB used, 6732 GB / 6991 GB avail
                 768 active+clean
  client io 25896 kB/s wr, 0 op/s rd, 3237 op/s wr

2017-05-31 13:57:52,168 INFO cluster.py [line:238] PG number is 768
2017-05-31 13:57:52,168 INFO cluster.py [line:239] usefull PG number is 768
2017-05-31 13:57:52,171 INFO TC40_kill_osd_on_single_node.py [line:78] stop osd.4 in cluster successfully
2017-05-31 13:57:52,171 INFO TC40_kill_osd_on_single_node.py [line:49] 
Now operate node Denali@123
2017-05-31 13:57:52,680 INFO TC40_kill_osd_on_single_node.py [line:96] TC40_kill_osd_on_single_node runs complete
