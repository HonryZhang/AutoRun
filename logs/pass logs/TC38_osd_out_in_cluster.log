2017-05-17 18:17:42,417 INFO TC38_osd_out_in_cluster.py [line:25] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. out all osds in sequence
4. stop all osds in sequence
5. start all osds in sequence
6. add in all osds in sequence
7. check the cluster status
8. repeat step 2-7 on the other node

2017-05-17 18:17:44,262 INFO monitors.py [line:123]    "quorum_leader_name": "denali02",

2017-05-17 18:17:44,262 INFO monitors.py [line:126]    "quorum_leader_name": "denali02",
2017-05-17 18:17:44,262 INFO TC38_osd_out_in_cluster.py [line:29] 
Step 1: start IO from clients
2017-05-17 18:17:44,262 INFO base.py [line:19] 
Now start IO on  reliablityTestImage0
2017-05-17 18:17:47,329 INFO client.py [line:56] pid info is 25488
2017-05-17 18:17:47,329 INFO client.py [line:56] pid info is 25942
2017-05-17 18:17:47,329 INFO client.py [line:56] pid info is 26255
2017-05-17 18:17:47,329 INFO base.py [line:19] 
Now start IO on  reliablityTestImage1
2017-05-17 18:17:48,453 INFO client.py [line:56] pid info is 25517
2017-05-17 18:17:48,453 INFO client.py [line:56] pid info is 25952
2017-05-17 18:17:48,453 INFO client.py [line:56] pid info is 26284
2017-05-17 18:17:48,453 INFO base.py [line:19] 
Now start IO on  reliablityTestImage2
2017-05-17 18:17:51,286 INFO client.py [line:56] pid info is 25546
2017-05-17 18:17:51,286 INFO client.py [line:56] pid info is 25999
2017-05-17 18:17:51,286 INFO client.py [line:56] pid info is 26294
2017-05-17 18:17:51,286 INFO base.py [line:19] 
Now start IO on  reliablityTestImage3
2017-05-17 18:17:55,684 INFO client.py [line:56] pid info is 25575
2017-05-17 18:17:55,684 INFO client.py [line:56] pid info is 26029
2017-05-17 18:17:55,684 INFO client.py [line:56] pid info is 26342
2017-05-17 18:17:55,684 INFO base.py [line:19] 
Now start IO on  reliablityTestImage4
2017-05-17 18:18:00,174 INFO client.py [line:56] pid info is 25604
2017-05-17 18:18:00,174 INFO client.py [line:56] pid info is 26055
2017-05-17 18:18:00,174 INFO base.py [line:19] 
Now start IO on  reliablityTestImage5
2017-05-17 18:18:01,269 INFO client.py [line:56] pid info is 25633
2017-05-17 18:18:01,269 INFO client.py [line:56] pid info is 26084
2017-05-17 18:18:01,269 INFO base.py [line:19] 
Now start IO on  reliablityTestImage6
2017-05-17 18:18:02,381 INFO client.py [line:56] pid info is 25662
2017-05-17 18:18:02,381 INFO client.py [line:56] pid info is 26102
2017-05-17 18:18:02,381 INFO base.py [line:19] 
Now start IO on  reliablityTestImage7
2017-05-17 18:18:03,526 INFO client.py [line:56] pid info is 25691
2017-05-17 18:18:03,526 INFO client.py [line:56] pid info is 26146
2017-05-17 18:18:03,526 INFO base.py [line:19] 
Now start IO on  reliablityTestImage8
2017-05-17 18:18:04,744 INFO client.py [line:56] pid info is 25720
2017-05-17 18:18:04,744 INFO client.py [line:56] pid info is 26156
2017-05-17 18:18:04,744 INFO base.py [line:19] 
Now start IO on  reliablityTestImage9
2017-05-17 18:18:05,821 INFO client.py [line:56] pid info is 25749
2017-05-17 18:18:05,821 INFO client.py [line:56] pid info is 26166
2017-05-17 18:19:05,828 INFO TC38_osd_out_in_cluster.py [line:32] 
Step 2: Out the osd and check IO
2017-05-17 18:19:05,828 INFO TC38_osd_out_in_cluster.py [line:36] 
Now operate denali01
2017-05-17 18:19:05,828 INFO TC38_osd_out_in_cluster.py [line:37] 3
2017-05-17 18:19:05,828 INFO TC38_osd_out_in_cluster.py [line:38] 
Now operate osd.0
2017-05-17 18:19:05,828 INFO TC38_osd_out_in_cluster.py [line:39] out osd.0
2017-05-17 18:19:05,828 INFO osd.py [line:59] execute command is ceph osd out osd.0 & sleep 3
2017-05-17 18:19:09,380 INFO osd.py [line:64] osd.0 is already out cluster
2017-05-17 18:19:09,380 INFO TC38_osd_out_in_cluster.py [line:41] check if IO error
2017-05-17 18:19:09,911 INFO client.py [line:90] home/denali

2017-05-17 18:19:10,443 INFO TC38_osd_out_in_cluster.py [line:44] Set the osd.0 pid for kill
2017-05-17 18:19:11,036 INFO node.py [line:165] osd.0  ---> processId 11259
2017-05-17 18:19:11,036 INFO node.py [line:165] osd.1  ---> processId 11661
2017-05-17 18:19:11,036 INFO node.py [line:165] osd.2  ---> processId 12122
2017-05-17 18:19:11,036 INFO osd.py [line:74] execute command is ceph osd in osd.0 & sleep 3
2017-05-17 18:19:14,640 INFO osd.py [line:77] osd.0 is already in cluster
2017-05-17 18:19:45,236 INFO client.py [line:90] home/denali

2017-05-17 18:19:45,848 INFO cluster.py [line:203] execute command is ceph -s
2017-05-17 18:19:46,696 INFO cluster.py [line:206]    cluster 78b6937b-1cd7-40e3-b176-6ac6507d6374
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.12:6789/0,denali03=192.168.28.22:6789/0}
            election epoch 8, quorum 0,1,2 denali02,denali03,denali01
     osdmap e433: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v4519: 2704 pgs, 13 pools, 293 GB data, 83423 objects
            22166 MB used, 317 GB / 338 GB avail
                2704 active+clean
  client io 2523 kB/s wr, 0 op/s rd, 630 op/s wr

2017-05-17 18:19:46,696 INFO cluster.py [line:207]       pgmap v4519: 2704 pgs, 13 pools, 293 GB data, 83423 objects
2017-05-17 18:19:46,696 INFO cluster.py [line:210]                 2704 active+clean
2017-05-17 18:19:46,711 INFO cluster.py [line:212] PG number is 2704
2017-05-17 18:19:46,711 INFO cluster.py [line:214] usefull PG number is 2704
2017-05-17 18:19:46,711 INFO TC38_osd_out_in_cluster.py [line:69] Out/in osd.0 to cluster successfully
2017-05-17 18:19:46,711 INFO TC38_osd_out_in_cluster.py [line:36] 
Now operate denali01
2017-05-17 18:19:46,711 INFO TC38_osd_out_in_cluster.py [line:37] 3
2017-05-17 18:19:46,711 INFO TC38_osd_out_in_cluster.py [line:38] 
Now operate osd.1
2017-05-17 18:19:46,711 INFO TC38_osd_out_in_cluster.py [line:39] out osd.1
2017-05-17 18:19:46,726 INFO osd.py [line:59] execute command is ceph osd out osd.1 & sleep 3
2017-05-17 18:19:50,331 INFO osd.py [line:64] osd.1 is already out cluster
2017-05-17 18:19:50,331 INFO TC38_osd_out_in_cluster.py [line:41] check if IO error
2017-05-17 18:19:50,973 INFO client.py [line:90] home/denali

2017-05-17 18:19:51,506 INFO TC38_osd_out_in_cluster.py [line:44] Set the osd.1 pid for kill
2017-05-17 18:19:52,119 INFO node.py [line:165] osd.0  ---> processId 11259
2017-05-17 18:19:52,119 INFO node.py [line:165] osd.1  ---> processId 11661
2017-05-17 18:19:52,119 INFO node.py [line:165] osd.2  ---> processId 12122
2017-05-17 18:19:52,119 INFO osd.py [line:74] execute command is ceph osd in osd.1 & sleep 3
2017-05-17 18:19:55,746 INFO osd.py [line:77] osd.1 is already in cluster
2017-05-17 18:20:26,301 INFO client.py [line:90] home/denali

2017-05-17 18:20:26,849 INFO cluster.py [line:203] execute command is ceph -s
2017-05-17 18:20:27,540 INFO cluster.py [line:206]    cluster 78b6937b-1cd7-40e3-b176-6ac6507d6374
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.12:6789/0,denali03=192.168.28.22:6789/0}
            election epoch 8, quorum 0,1,2 denali02,denali03,denali01
     osdmap e439: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v4552: 2704 pgs, 13 pools, 293 GB data, 83423 objects
            22164 MB used, 317 GB / 338 GB avail
                2704 active+clean
  client io 2005 B/s rd, 2317 kB/s wr, 2 op/s rd, 579 op/s wr

2017-05-17 18:20:27,540 INFO cluster.py [line:207]       pgmap v4552: 2704 pgs, 13 pools, 293 GB data, 83423 objects
2017-05-17 18:20:27,540 INFO cluster.py [line:210]                 2704 active+clean
2017-05-17 18:20:27,556 INFO cluster.py [line:212] PG number is 2704
2017-05-17 18:20:27,556 INFO cluster.py [line:214] usefull PG number is 2704
2017-05-17 18:20:27,556 INFO TC38_osd_out_in_cluster.py [line:69] Out/in osd.1 to cluster successfully
2017-05-17 18:20:27,556 INFO TC38_osd_out_in_cluster.py [line:36] 
Now operate denali01
2017-05-17 18:20:27,556 INFO TC38_osd_out_in_cluster.py [line:37] 3
2017-05-17 18:20:27,556 INFO TC38_osd_out_in_cluster.py [line:38] 
Now operate osd.2
2017-05-17 18:20:27,572 INFO TC38_osd_out_in_cluster.py [line:39] out osd.2
2017-05-17 18:20:27,572 INFO osd.py [line:59] execute command is ceph osd out osd.2 & sleep 3
2017-05-17 18:20:31,127 INFO osd.py [line:64] osd.2 is already out cluster
2017-05-17 18:20:31,127 INFO TC38_osd_out_in_cluster.py [line:41] check if IO error
2017-05-17 18:20:31,657 INFO client.py [line:90] home/denali

2017-05-17 18:20:32,203 INFO TC38_osd_out_in_cluster.py [line:44] Set the osd.2 pid for kill
2017-05-17 18:20:32,924 INFO node.py [line:165] osd.0  ---> processId 11259
2017-05-17 18:20:32,924 INFO node.py [line:165] osd.1  ---> processId 11661
2017-05-17 18:20:32,924 INFO node.py [line:165] osd.2  ---> processId 12122
2017-05-17 18:20:32,924 INFO osd.py [line:74] execute command is ceph osd in osd.2 & sleep 3
2017-05-17 18:20:36,612 INFO osd.py [line:77] osd.2 is already in cluster
2017-05-17 18:21:07,144 INFO client.py [line:90] home/denali

2017-05-17 18:21:07,647 INFO cluster.py [line:203] execute command is ceph -s
2017-05-17 18:21:08,582 INFO cluster.py [line:206]    cluster 78b6937b-1cd7-40e3-b176-6ac6507d6374
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.12:6789/0,denali03=192.168.28.22:6789/0}
            election epoch 8, quorum 0,1,2 denali02,denali03,denali01
     osdmap e445: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v4584: 2704 pgs, 13 pools, 293 GB data, 83423 objects
            22165 MB used, 317 GB / 338 GB avail
                2704 active+clean
  client io 3804 kB/s wr, 0 op/s rd, 951 op/s wr

2017-05-17 18:21:08,582 INFO cluster.py [line:207]       pgmap v4584: 2704 pgs, 13 pools, 293 GB data, 83423 objects
2017-05-17 18:21:08,582 INFO cluster.py [line:210]                 2704 active+clean
2017-05-17 18:21:08,598 INFO cluster.py [line:212] PG number is 2704
2017-05-17 18:21:08,598 INFO cluster.py [line:214] usefull PG number is 2704
2017-05-17 18:21:08,598 INFO TC38_osd_out_in_cluster.py [line:69] Out/in osd.2 to cluster successfully
2017-05-17 18:21:08,598 INFO TC38_osd_out_in_cluster.py [line:36] 
Now operate denali02
2017-05-17 18:21:08,598 INFO TC38_osd_out_in_cluster.py [line:37] 3
2017-05-17 18:21:08,598 INFO TC38_osd_out_in_cluster.py [line:38] 
Now operate osd.6
2017-05-17 18:21:08,598 INFO TC38_osd_out_in_cluster.py [line:39] out osd.6
2017-05-17 18:21:08,614 INFO osd.py [line:59] execute command is ceph osd out osd.6 & sleep 3
2017-05-17 18:21:12,164 INFO osd.py [line:64] osd.6 is already out cluster
2017-05-17 18:21:12,164 INFO TC38_osd_out_in_cluster.py [line:41] check if IO error
2017-05-17 18:21:12,698 INFO client.py [line:90] home/denali

2017-05-17 18:21:13,213 INFO TC38_osd_out_in_cluster.py [line:44] Set the osd.6 pid for kill
2017-05-17 18:21:13,852 INFO node.py [line:165] osd.6  ---> processId 22547
2017-05-17 18:21:13,852 INFO node.py [line:165] osd.7  ---> processId 23792
2017-05-17 18:21:13,852 INFO node.py [line:165] osd.8  ---> processId 25095
2017-05-17 18:21:13,852 INFO osd.py [line:74] execute command is ceph osd in osd.6 & sleep 3
2017-05-17 18:21:17,463 INFO osd.py [line:77] osd.6 is already in cluster
2017-05-17 18:21:47,986 INFO client.py [line:90] home/denali

2017-05-17 18:21:48,517 INFO cluster.py [line:203] execute command is ceph -s
2017-05-17 18:21:49,457 INFO cluster.py [line:206]    cluster 78b6937b-1cd7-40e3-b176-6ac6507d6374
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.12:6789/0,denali03=192.168.28.22:6789/0}
            election epoch 8, quorum 0,1,2 denali02,denali03,denali01
     osdmap e455: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v4622: 2704 pgs, 13 pools, 293 GB data, 83423 objects
            22167 MB used, 317 GB / 338 GB avail
                2704 active+clean
  client io 7312 B/s rd, 5149 kB/s wr, 7 op/s rd, 1287 op/s wr

2017-05-17 18:21:49,457 INFO cluster.py [line:207]       pgmap v4622: 2704 pgs, 13 pools, 293 GB data, 83423 objects
2017-05-17 18:21:49,457 INFO cluster.py [line:210]                 2704 active+clean
2017-05-17 18:21:49,457 INFO cluster.py [line:212] PG number is 2704
2017-05-17 18:21:49,473 INFO cluster.py [line:214] usefull PG number is 2704
2017-05-17 18:21:49,473 INFO TC38_osd_out_in_cluster.py [line:69] Out/in osd.6 to cluster successfully
2017-05-17 18:21:49,473 INFO TC38_osd_out_in_cluster.py [line:36] 
Now operate denali02
2017-05-17 18:21:49,473 INFO TC38_osd_out_in_cluster.py [line:37] 3
2017-05-17 18:21:49,473 INFO TC38_osd_out_in_cluster.py [line:38] 
Now operate osd.7
2017-05-17 18:21:49,473 INFO TC38_osd_out_in_cluster.py [line:39] out osd.7
2017-05-17 18:21:49,473 INFO osd.py [line:59] execute command is ceph osd out osd.7 & sleep 3
2017-05-17 18:21:53,036 INFO osd.py [line:64] osd.7 is already out cluster
2017-05-17 18:21:53,036 INFO TC38_osd_out_in_cluster.py [line:41] check if IO error
2017-05-17 18:21:53,568 INFO client.py [line:90] home/denali

2017-05-17 18:21:54,098 INFO TC38_osd_out_in_cluster.py [line:44] Set the osd.7 pid for kill
2017-05-17 18:21:54,726 INFO node.py [line:165] osd.6  ---> processId 22547
2017-05-17 18:21:54,726 INFO node.py [line:165] osd.7  ---> processId 23792
2017-05-17 18:21:54,726 INFO node.py [line:165] osd.8  ---> processId 25095
2017-05-17 18:21:54,726 INFO osd.py [line:74] execute command is ceph osd in osd.7 & sleep 3
2017-05-17 18:21:58,453 INFO osd.py [line:77] osd.7 is already in cluster
2017-05-17 18:22:29,062 INFO client.py [line:90] home/denali

2017-05-17 18:22:29,644 INFO cluster.py [line:203] execute command is ceph -s
2017-05-17 18:22:30,677 INFO cluster.py [line:206]    cluster 78b6937b-1cd7-40e3-b176-6ac6507d6374
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.12:6789/0,denali03=192.168.28.22:6789/0}
            election epoch 8, quorum 0,1,2 denali02,denali03,denali01
     osdmap e461: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v4656: 2704 pgs, 13 pools, 293 GB data, 83423 objects
            22169 MB used, 317 GB / 338 GB avail
                2704 active+clean
  client io 4072 kB/s wr, 0 op/s rd, 1018 op/s wr

2017-05-17 18:22:30,677 INFO cluster.py [line:207]       pgmap v4656: 2704 pgs, 13 pools, 293 GB data, 83423 objects
2017-05-17 18:22:30,677 INFO cluster.py [line:210]                 2704 active+clean
2017-05-17 18:22:30,677 INFO cluster.py [line:212] PG number is 2704
2017-05-17 18:22:30,694 INFO cluster.py [line:214] usefull PG number is 2704
2017-05-17 18:22:30,694 INFO TC38_osd_out_in_cluster.py [line:69] Out/in osd.7 to cluster successfully
2017-05-17 18:22:30,694 INFO TC38_osd_out_in_cluster.py [line:36] 
Now operate denali02
2017-05-17 18:22:30,694 INFO TC38_osd_out_in_cluster.py [line:37] 3
2017-05-17 18:22:30,694 INFO TC38_osd_out_in_cluster.py [line:38] 
Now operate osd.8
2017-05-17 18:22:30,694 INFO TC38_osd_out_in_cluster.py [line:39] out osd.8
2017-05-17 18:22:30,694 INFO osd.py [line:59] execute command is ceph osd out osd.8 & sleep 3
2017-05-17 18:22:34,306 INFO osd.py [line:64] osd.8 is already out cluster
2017-05-17 18:22:34,306 INFO TC38_osd_out_in_cluster.py [line:41] check if IO error
2017-05-17 18:22:34,871 INFO client.py [line:90] home/denali

2017-05-17 18:22:35,401 INFO TC38_osd_out_in_cluster.py [line:44] Set the osd.8 pid for kill
2017-05-17 18:22:36,028 INFO node.py [line:165] osd.6  ---> processId 22547
2017-05-17 18:22:36,028 INFO node.py [line:165] osd.7  ---> processId 23792
2017-05-17 18:22:36,028 INFO node.py [line:165] osd.8  ---> processId 25095
2017-05-17 18:22:36,028 INFO osd.py [line:74] execute command is ceph osd in osd.8 & sleep 3
2017-05-17 18:22:39,618 INFO osd.py [line:77] osd.8 is already in cluster
2017-05-17 18:23:10,155 INFO client.py [line:90] home/denali

2017-05-17 18:23:10,670 INFO cluster.py [line:203] execute command is ceph -s
2017-05-17 18:23:11,668 INFO cluster.py [line:206]    cluster 78b6937b-1cd7-40e3-b176-6ac6507d6374
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.12:6789/0,denali03=192.168.28.22:6789/0}
            election epoch 8, quorum 0,1,2 denali02,denali03,denali01
     osdmap e473: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v4688: 2704 pgs, 13 pools, 293 GB data, 83423 objects
            22204 MB used, 317 GB / 338 GB avail
                2704 active+clean
  client io 4073 kB/s wr, 0 op/s rd, 1018 op/s wr

2017-05-17 18:23:11,668 INFO cluster.py [line:207]       pgmap v4688: 2704 pgs, 13 pools, 293 GB data, 83423 objects
2017-05-17 18:23:11,668 INFO cluster.py [line:210]                 2704 active+clean
2017-05-17 18:23:11,668 INFO cluster.py [line:212] PG number is 2704
2017-05-17 18:23:11,668 INFO cluster.py [line:214] usefull PG number is 2704
2017-05-17 18:23:11,668 INFO TC38_osd_out_in_cluster.py [line:69] Out/in osd.8 to cluster successfully
2017-05-17 18:23:11,684 INFO TC38_osd_out_in_cluster.py [line:36] 
Now operate denali03
2017-05-17 18:23:11,684 INFO TC38_osd_out_in_cluster.py [line:37] 3
2017-05-17 18:23:11,684 INFO TC38_osd_out_in_cluster.py [line:38] 
Now operate osd.3
2017-05-17 18:23:11,684 INFO TC38_osd_out_in_cluster.py [line:39] out osd.3
2017-05-17 18:23:11,684 INFO osd.py [line:59] execute command is ceph osd out osd.3 & sleep 3
2017-05-17 18:23:15,239 INFO osd.py [line:64] osd.3 is already out cluster
2017-05-17 18:23:15,239 INFO TC38_osd_out_in_cluster.py [line:41] check if IO error
2017-05-17 18:23:15,740 INFO client.py [line:90] home/denali

2017-05-17 18:23:16,239 INFO TC38_osd_out_in_cluster.py [line:44] Set the osd.3 pid for kill
2017-05-17 18:23:16,869 INFO node.py [line:165] osd.3  ---> processId 11380
2017-05-17 18:23:16,869 INFO node.py [line:165] osd.4  ---> processId 11793
2017-05-17 18:23:16,869 INFO node.py [line:165] osd.5  ---> processId 12177
2017-05-17 18:23:16,869 INFO osd.py [line:74] execute command is ceph osd in osd.3 & sleep 3
2017-05-17 18:23:20,469 INFO osd.py [line:77] osd.3 is already in cluster
2017-05-17 18:23:50,989 INFO client.py [line:90] home/denali

2017-05-17 18:23:51,487 INFO cluster.py [line:203] execute command is ceph -s
2017-05-17 18:23:52,414 INFO cluster.py [line:206]    cluster 78b6937b-1cd7-40e3-b176-6ac6507d6374
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.12:6789/0,denali03=192.168.28.22:6789/0}
            election epoch 8, quorum 0,1,2 denali02,denali03,denali01
     osdmap e480: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v4719: 2704 pgs, 13 pools, 293 GB data, 83423 objects
            22206 MB used, 317 GB / 338 GB avail
                2704 active+clean
  client io 2659 kB/s wr, 0 op/s rd, 664 op/s wr

2017-05-17 18:23:52,414 INFO cluster.py [line:207]       pgmap v4719: 2704 pgs, 13 pools, 293 GB data, 83423 objects
2017-05-17 18:23:52,414 INFO cluster.py [line:210]                 2704 active+clean
2017-05-17 18:23:52,414 INFO cluster.py [line:212] PG number is 2704
2017-05-17 18:23:52,430 INFO cluster.py [line:214] usefull PG number is 2704
2017-05-17 18:23:52,430 INFO TC38_osd_out_in_cluster.py [line:69] Out/in osd.3 to cluster successfully
2017-05-17 18:23:52,430 INFO TC38_osd_out_in_cluster.py [line:36] 
Now operate denali03
2017-05-17 18:23:52,430 INFO TC38_osd_out_in_cluster.py [line:37] 3
2017-05-17 18:23:52,430 INFO TC38_osd_out_in_cluster.py [line:38] 
Now operate osd.4
2017-05-17 18:23:52,430 INFO TC38_osd_out_in_cluster.py [line:39] out osd.4
2017-05-17 18:23:52,446 INFO osd.py [line:59] execute command is ceph osd out osd.4 & sleep 3
2017-05-17 18:23:55,976 INFO osd.py [line:64] osd.4 is already out cluster
2017-05-17 18:23:55,976 INFO TC38_osd_out_in_cluster.py [line:41] check if IO error
2017-05-17 18:23:56,476 INFO client.py [line:90] home/denali

2017-05-17 18:23:56,992 INFO TC38_osd_out_in_cluster.py [line:44] Set the osd.4 pid for kill
2017-05-17 18:23:57,651 INFO node.py [line:165] osd.3  ---> processId 11380
2017-05-17 18:23:57,651 INFO node.py [line:165] osd.4  ---> processId 11793
2017-05-17 18:23:57,651 INFO node.py [line:165] osd.5  ---> processId 12177
2017-05-17 18:23:57,651 INFO osd.py [line:74] execute command is ceph osd in osd.4 & sleep 3
2017-05-17 18:24:01,246 INFO osd.py [line:77] osd.4 is already in cluster
2017-05-17 18:24:31,776 INFO client.py [line:90] home/denali

2017-05-17 18:24:32,305 INFO cluster.py [line:203] execute command is ceph -s
2017-05-17 18:24:33,148 INFO cluster.py [line:206]    cluster 78b6937b-1cd7-40e3-b176-6ac6507d6374
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.12:6789/0,denali03=192.168.28.22:6789/0}
            election epoch 8, quorum 0,1,2 denali02,denali03,denali01
     osdmap e486: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v4748: 2704 pgs, 13 pools, 293 GB data, 83423 objects
            22194 MB used, 317 GB / 338 GB avail
                2704 active+clean
  client io 2242 kB/s wr, 0 op/s rd, 560 op/s wr

2017-05-17 18:24:33,148 INFO cluster.py [line:207]       pgmap v4748: 2704 pgs, 13 pools, 293 GB data, 83423 objects
2017-05-17 18:24:33,148 INFO cluster.py [line:210]                 2704 active+clean
2017-05-17 18:24:33,148 INFO cluster.py [line:212] PG number is 2704
2017-05-17 18:24:33,164 INFO cluster.py [line:214] usefull PG number is 2704
2017-05-17 18:24:33,164 INFO TC38_osd_out_in_cluster.py [line:69] Out/in osd.4 to cluster successfully
2017-05-17 18:24:33,164 INFO TC38_osd_out_in_cluster.py [line:36] 
Now operate denali03
2017-05-17 18:24:33,164 INFO TC38_osd_out_in_cluster.py [line:37] 3
2017-05-17 18:24:33,164 INFO TC38_osd_out_in_cluster.py [line:38] 
Now operate osd.5
2017-05-17 18:24:33,164 INFO TC38_osd_out_in_cluster.py [line:39] out osd.5
2017-05-17 18:24:33,164 INFO osd.py [line:59] execute command is ceph osd out osd.5 & sleep 3
2017-05-17 18:24:36,747 INFO osd.py [line:64] osd.5 is already out cluster
2017-05-17 18:24:36,747 INFO TC38_osd_out_in_cluster.py [line:41] check if IO error
2017-05-17 18:24:37,263 INFO client.py [line:90] home/denali

2017-05-17 18:24:37,762 INFO TC38_osd_out_in_cluster.py [line:44] Set the osd.5 pid for kill
2017-05-17 18:24:38,371 INFO node.py [line:165] osd.3  ---> processId 11380
2017-05-17 18:24:38,371 INFO node.py [line:165] osd.4  ---> processId 11793
2017-05-17 18:24:38,371 INFO node.py [line:165] osd.5  ---> processId 12177
2017-05-17 18:24:38,371 INFO osd.py [line:74] execute command is ceph osd in osd.5 & sleep 3
2017-05-17 18:24:42,003 INFO osd.py [line:77] osd.5 is already in cluster
2017-05-17 18:25:12,871 INFO client.py [line:90] home/denali

2017-05-17 18:25:13,667 INFO cluster.py [line:203] execute command is ceph -s
2017-05-17 18:25:14,635 INFO cluster.py [line:206]    cluster 78b6937b-1cd7-40e3-b176-6ac6507d6374
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.12:6789/0,denali03=192.168.28.22:6789/0}
            election epoch 8, quorum 0,1,2 denali02,denali03,denali01
     osdmap e492: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v4785: 2704 pgs, 13 pools, 293 GB data, 83423 objects
            22180 MB used, 317 GB / 338 GB avail
                2704 active+clean
  client io 3413 kB/s wr, 0 op/s rd, 853 op/s wr

2017-05-17 18:25:14,635 INFO cluster.py [line:207]       pgmap v4785: 2704 pgs, 13 pools, 293 GB data, 83423 objects
2017-05-17 18:25:14,635 INFO cluster.py [line:210]                 2704 active+clean
2017-05-17 18:25:14,635 INFO cluster.py [line:212] PG number is 2704
2017-05-17 18:25:14,651 INFO cluster.py [line:214] usefull PG number is 2704
2017-05-17 18:25:14,651 INFO TC38_osd_out_in_cluster.py [line:69] Out/in osd.5 to cluster successfully
2017-05-17 18:25:14,651 INFO TC38_osd_out_in_cluster.py [line:75] 
Step 3:stop IO from clients
2017-05-17 18:25:15,857 INFO TC38_osd_out_in_cluster.py [line:77] TC38_osd_out_in_cluster runs complete
