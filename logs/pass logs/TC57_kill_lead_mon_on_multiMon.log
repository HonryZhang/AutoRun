2017-06-13 11:15:38,950 INFO TC57_kill_lead_mon_on_multiMon.py [line:29] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the leader monitor, and kill the mon process
3. check cluster status, Io status and cluster quorum status
4. login the new leader monitor, and kill the mon process
5. check cluster status, Io status and cluster quorum status
6. start the first killed monitor
7. check cluster status, does the leader monitor will be back????
8. start the first killed monitor
9. check cluster status, does the leader monitor will be back????

2017-06-13 11:15:39,601 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:15:39,601 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:15:39,601 INFO TC57_kill_lead_mon_on_multiMon.py [line:32] start to check cluster status before case running
2017-06-13 11:15:41,607 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:15:41,961 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 110, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53295: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2210 GB used, 3979 GB / 6189 GB avail
                3216 active+clean
  client io 145 kB/s rd, 194 MB/s wr, 189 op/s rd, 24899 op/s wr
stdin: is not a tty

2017-06-13 11:15:41,961 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:15:41,961 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:15:41,961 INFO TC57_kill_lead_mon_on_multiMon.py [line:35] health status is OK
2017-06-13 11:15:43,967 INFO TC57_kill_lead_mon_on_multiMon.py [line:43] 
Step1: Check IO from clients
2017-06-13 11:15:44,447 INFO client.py [line:172] ['oot      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 73 02:57 ?        00:13:10 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 65 02:57 ?        00:11:37 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 66 02:57 ?        00:11:47 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 65 02:57 ?        00:11:46 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 65 02:57 ?        00:11:36 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali    67156  67155  0 03:15 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali    67158  67156  0 03:15 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 11:15:44,448 INFO client.py [line:174] IO is running
2017-06-13 11:16:44,508 INFO TC57_kill_lead_mon_on_multiMon.py [line:51] 
Step2: kill leader mon 10 times
2017-06-13 11:16:44,508 INFO monitors.py [line:55] mon is  taheo125
2017-06-13 11:16:44,508 INFO monitors.py [line:56] execute command is sudo -i stop ceph-mon id=taheo125 & sleep 5
2017-06-13 11:16:49,729 ERROR monitors.py [line:61] Error when shutdown mon taheo125
2017-06-13 11:16:49,729 ERROR monitors.py [line:62] sudo -i stop ceph-mon id=taheo125 & sleep 5
2017-06-13 11:16:49,729 ERROR monitors.py [line:63] tdin: is not a tty
stop: Unknown instance: ceph/taheo125

2017-06-13 11:16:49,729 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:16:49,730 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:17:19,884 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:17:19,885 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:17:19,885 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:16:53.515145 7f853e3ab4c0 -1 asok(0x7f853a9351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:16:53.522877 7f853e3ab4c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:17:19,885 INFO monitors.py [line:55] mon is  taheo125
2017-06-13 11:17:19,885 INFO monitors.py [line:56] execute command is sudo -i stop ceph-mon id=taheo125 & sleep 5
2017-06-13 11:17:25,105 ERROR monitors.py [line:61] Error when shutdown mon taheo125
2017-06-13 11:17:25,105 ERROR monitors.py [line:62] sudo -i stop ceph-mon id=taheo125 & sleep 5
2017-06-13 11:17:25,105 ERROR monitors.py [line:63] tdin: is not a tty
stop: Unknown instance: ceph/taheo125

2017-06-13 11:17:25,106 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:17:25,106 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:17:55,318 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:17:55,318 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:17:55,318 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:17:28.946266 7f426c6b64c0 -1 asok(0x7f42689351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:17:28.953629 7f426c6b64c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:17:55,585 INFO monitors.py [line:45] ['enali   15583 15531  0 11:17 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   15585 15583  0 11:17 ?        00:00:00 grep ceph-mon', 'root     51733     1  2 10:17 pts/0    00:01:19 ceph-mon -i taheo125', '']
2017-06-13 11:17:55,585 INFO monitors.py [line:51] mon pid is 51733
2017-06-13 11:17:55,585 INFO monitors.py [line:92] execute command is sudo -i kill -9 51733 & sleep 3
2017-06-13 11:18:32,398 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:18:32,398 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:18:32,398 INFO TC57_kill_lead_mon_on_multiMon.py [line:67] now the leader mon is tahoe126
2017-06-13 11:18:32,668 INFO monitors.py [line:45] ['enali   23148 23147  0 11:18 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   23150 23148  0 11:18 ?        00:00:00 grep ceph-mon', '']
2017-06-13 11:18:32,668 INFO monitors.py [line:51] mon pid is 51733
2017-06-13 11:18:32,668 INFO monitors.py [line:92] execute command is sudo -i kill -9 51733 & sleep 3
2017-06-13 11:19:09,487 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:19:09,487 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:19:09,487 INFO TC57_kill_lead_mon_on_multiMon.py [line:78] now the leader mon is tahoe126
2017-06-13 11:19:09,487 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:19:09,487 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:19:39,699 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 11:19:39,699 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:19:39,699 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:19:39,952 INFO monitors.py [line:106] oot     32218     1  3 11:19 ?        00:00:01 ceph-mon -i taheo125
denali   43036 43035  0 11:19 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   43038 43036  0 11:19 ?        00:00:00 grep ceph-mon

2017-06-13 11:19:39,952 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:20:39,969 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:20:40,329 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 114, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53405: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2245 GB used, 3944 GB / 6189 GB avail
                3216 active+clean
  client io 151 kB/s rd, 257 MB/s wr, 197 op/s rd, 32935 op/s wr
stdin: is not a tty

2017-06-13 11:20:40,329 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:20:40,329 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:20:40,329 INFO TC57_kill_lead_mon_on_multiMon.py [line:88] stop mon service on taheo125 in cluster successfully
2017-06-13 11:20:40,885 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:20:40,886 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:20:40,886 INFO TC57_kill_lead_mon_on_multiMon.py [line:104] now the leader mon is taheo125
2017-06-13 11:20:40,886 INFO TC57_kill_lead_mon_on_multiMon.py [line:106] taheo125 is back
2017-06-13 11:20:40,886 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:20:40,886 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:21:11,097 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:21:11,098 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:21:11,098 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:20:44.724881 7f7d837c74c0 -1 asok(0x7f7d7fd351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:20:44.732307 7f7d837c74c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:21:11,098 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:21:11,098 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:21:11,371 INFO monitors.py [line:106] enali    2270  2263  0 11:21 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali    2273  2270  0 11:21 ?        00:00:00 grep ceph-mon
root     32218     1  2 11:19 ?        00:00:02 ceph-mon -i taheo125

2017-06-13 11:21:11,372 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:22:11,604 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:22:11,968 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 114, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53441: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2256 GB used, 3932 GB / 6189 GB avail
                3216 active+clean
  client io 156 kB/s rd, 237 MB/s wr, 202 op/s rd, 30461 op/s wr
stdin: is not a tty

2017-06-13 11:22:11,968 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:22:11,968 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:22:11,968 INFO TC57_kill_lead_mon_on_multiMon.py [line:121] stop mon service on taheo125 in cluster successfully
2017-06-13 11:22:12,524 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:22:12,524 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:22:12,524 INFO TC57_kill_lead_mon_on_multiMon.py [line:138] now the leader mon is taheo125
2017-06-13 11:22:12,524 INFO TC57_kill_lead_mon_on_multiMon.py [line:140] taheo125 is back
2017-06-13 11:22:12,777 INFO client.py [line:172] ['oot      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 73 02:57 ?        00:17:47 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 64 02:57 ?        00:15:41 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 65 02:57 ?        00:15:56 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 65 02:57 ?        00:15:57 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 64 02:57 ?        00:15:43 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali    87600  87599  0 03:22 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali    87602  87600  0 03:22 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 11:22:12,777 INFO client.py [line:174] IO is running
2017-06-13 11:22:13,037 INFO monitors.py [line:45] ['enali   25107 25095  0 11:22 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   25109 25107  0 11:22 ?        00:00:00 grep ceph-mon', 'root     32218     1  2 11:19 ?        00:00:04 ceph-mon -i taheo125', '']
2017-06-13 11:22:13,037 INFO monitors.py [line:51] mon pid is 32218
2017-06-13 11:22:13,037 INFO monitors.py [line:92] execute command is sudo -i kill -9 32218 & sleep 3
2017-06-13 11:22:49,860 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:22:49,860 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:22:49,861 INFO TC57_kill_lead_mon_on_multiMon.py [line:67] now the leader mon is tahoe126
2017-06-13 11:22:50,135 INFO monitors.py [line:45] ['enali   32951 32950  0 11:22 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   32953 32951  0 11:22 ?        00:00:00 grep ceph-mon', '']
2017-06-13 11:22:50,135 INFO monitors.py [line:51] mon pid is 32218
2017-06-13 11:22:50,135 INFO monitors.py [line:92] execute command is sudo -i kill -9 32218 & sleep 3
2017-06-13 11:23:26,939 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:23:26,939 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:23:26,939 INFO TC57_kill_lead_mon_on_multiMon.py [line:78] now the leader mon is tahoe126
2017-06-13 11:23:26,939 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:23:26,939 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:23:57,151 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 11:23:57,151 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:23:57,152 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:23:57,377 INFO monitors.py [line:106] oot     42017     1  3 11:23 ?        00:00:00 ceph-mon -i taheo125
denali   52982 52981  0 11:24 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   52984 52982  0 11:24 ?        00:00:00 grep ceph-mon

2017-06-13 11:23:57,377 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:24:57,430 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:24:57,786 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 118, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53501: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2275 GB used, 3914 GB / 6189 GB avail
                3216 active+clean
  client io 149 kB/s rd, 217 MB/s wr, 192 op/s rd, 27789 op/s wr
stdin: is not a tty

2017-06-13 11:24:57,786 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:24:57,786 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:24:57,786 INFO TC57_kill_lead_mon_on_multiMon.py [line:88] stop mon service on taheo125 in cluster successfully
2017-06-13 11:24:58,291 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:24:58,291 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:24:58,291 INFO TC57_kill_lead_mon_on_multiMon.py [line:104] now the leader mon is taheo125
2017-06-13 11:24:58,291 INFO TC57_kill_lead_mon_on_multiMon.py [line:106] taheo125 is back
2017-06-13 11:24:58,292 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:24:58,292 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:25:28,467 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:25:28,468 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:25:28,468 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:25:02.099756 7f34b0d5b4c0 -1 asok(0x7f34ad1351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:25:02.106452 7f34b0d5b4c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:25:28,468 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:25:28,468 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:25:28,732 INFO monitors.py [line:106] enali   13381 13379  0 11:25 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   13383 13381  0 11:25 ?        00:00:00 grep ceph-mon
root     42017     1  2 11:23 ?        00:00:02 ceph-mon -i taheo125

2017-06-13 11:25:28,732 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:26:28,984 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:26:29,352 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 118, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53537: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2285 GB used, 3904 GB / 6189 GB avail
                3216 active+clean
  client io 150 kB/s rd, 230 MB/s wr, 196 op/s rd, 29445 op/s wr
stdin: is not a tty

2017-06-13 11:26:29,352 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:26:29,352 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:26:29,353 INFO TC57_kill_lead_mon_on_multiMon.py [line:121] stop mon service on taheo125 in cluster successfully
2017-06-13 11:26:29,963 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:26:29,963 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:26:29,963 INFO TC57_kill_lead_mon_on_multiMon.py [line:138] now the leader mon is taheo125
2017-06-13 11:26:29,964 INFO TC57_kill_lead_mon_on_multiMon.py [line:140] taheo125 is back
2017-06-13 11:26:30,216 INFO client.py [line:172] ['oot      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 72 02:57 ?        00:20:46 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 64 02:57 ?        00:18:21 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 65 02:57 ?        00:18:39 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 65 02:57 ?        00:18:40 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 64 02:57 ?        00:18:25 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali   100703 100701  0 03:26 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   100705 100703  0 03:26 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 11:26:30,216 INFO client.py [line:174] IO is running
2017-06-13 11:26:30,442 INFO monitors.py [line:45] ['enali   35787 35782  0 11:26 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   35792 35787  0 11:26 ?        00:00:00 grep ceph-mon', 'root     42017     1  2 11:23 ?        00:00:04 ceph-mon -i taheo125', '']
2017-06-13 11:26:30,442 INFO monitors.py [line:51] mon pid is 42017
2017-06-13 11:26:30,443 INFO monitors.py [line:92] execute command is sudo -i kill -9 42017 & sleep 3
2017-06-13 11:27:04,576 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:27:04,577 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:27:04,577 INFO TC57_kill_lead_mon_on_multiMon.py [line:67] now the leader mon is tahoe126
2017-06-13 11:27:04,843 INFO monitors.py [line:45] ['enali   43140 43134  0 11:27 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   43142 43140  0 11:27 ?        00:00:00 grep ceph-mon', '']
2017-06-13 11:27:04,843 INFO monitors.py [line:51] mon pid is 42017
2017-06-13 11:27:04,843 INFO monitors.py [line:92] execute command is sudo -i kill -9 42017 & sleep 3
2017-06-13 11:27:41,641 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:27:41,641 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:27:41,641 INFO TC57_kill_lead_mon_on_multiMon.py [line:78] now the leader mon is tahoe126
2017-06-13 11:27:41,641 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:27:41,641 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:28:11,821 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 11:28:11,821 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:28:11,822 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:28:12,079 INFO monitors.py [line:106] oot     52463     1  3 11:27 ?        00:00:00 ceph-mon -i taheo125
denali   62935 62934  0 11:28 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   62937 62935  0 11:28 ?        00:00:00 grep ceph-mon

2017-06-13 11:28:12,080 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:29:12,100 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:29:12,451 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 122, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53598: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2304 GB used, 3885 GB / 6189 GB avail
                3216 active+clean
  client io 150 kB/s rd, 196 MB/s wr, 195 op/s rd, 25123 op/s wr
stdin: is not a tty

2017-06-13 11:29:12,451 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:29:12,451 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:29:12,451 INFO TC57_kill_lead_mon_on_multiMon.py [line:88] stop mon service on taheo125 in cluster successfully
2017-06-13 11:29:13,016 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:29:13,016 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:29:13,016 INFO TC57_kill_lead_mon_on_multiMon.py [line:104] now the leader mon is taheo125
2017-06-13 11:29:13,016 INFO TC57_kill_lead_mon_on_multiMon.py [line:106] taheo125 is back
2017-06-13 11:29:13,016 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:29:13,016 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:29:43,228 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:29:43,228 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:29:43,228 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:29:16.859577 7f86a70754c0 -1 asok(0x7f86a35351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:29:16.866825 7f86a70754c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:29:43,229 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:29:43,229 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:29:43,484 INFO monitors.py [line:106] enali   23446 23445  0 11:29 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   23448 23446  0 11:29 ?        00:00:00 grep ceph-mon
root     52463     1  2 11:27 ?        00:00:02 ceph-mon -i taheo125

2017-06-13 11:29:43,484 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:30:43,718 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:30:44,077 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 122, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53633: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2313 GB used, 3876 GB / 6189 GB avail
                3216 active+clean
  client io 151 kB/s rd, 179 MB/s wr, 195 op/s rd, 23035 op/s wr
stdin: is not a tty

2017-06-13 11:30:44,078 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:30:44,078 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:30:44,078 INFO TC57_kill_lead_mon_on_multiMon.py [line:121] stop mon service on taheo125 in cluster successfully
2017-06-13 11:30:44,596 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:30:44,597 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:30:44,597 INFO TC57_kill_lead_mon_on_multiMon.py [line:138] now the leader mon is taheo125
2017-06-13 11:30:44,597 INFO TC57_kill_lead_mon_on_multiMon.py [line:140] taheo125 is back
2017-06-13 11:30:44,856 INFO client.py [line:172] ['oot      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 72 02:57 ?        00:23:47 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 63 02:57 ?        00:21:01 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 64 02:57 ?        00:21:20 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 64 02:57 ?        00:21:21 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 64 02:57 ?        00:21:06 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali   113728 113726  0 03:30 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   113730 113728  0 03:30 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 11:30:44,856 INFO client.py [line:174] IO is running
2017-06-13 11:30:45,088 INFO monitors.py [line:45] ['enali   45795 45794  0 11:30 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   45797 45795  0 11:30 ?        00:00:00 grep ceph-mon', 'root     52463     1  2 11:27 ?        00:00:04 ceph-mon -i taheo125', '']
2017-06-13 11:30:45,088 INFO monitors.py [line:51] mon pid is 52463
2017-06-13 11:30:45,088 INFO monitors.py [line:92] execute command is sudo -i kill -9 52463 & sleep 3
2017-06-13 11:31:21,880 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:31:21,881 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:31:21,881 INFO TC57_kill_lead_mon_on_multiMon.py [line:67] now the leader mon is tahoe126
2017-06-13 11:31:22,139 INFO monitors.py [line:45] ['enali   53239 53234  0 11:31 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   53242 53239  0 11:31 ?        00:00:00 grep ceph-mon', '']
2017-06-13 11:31:22,139 INFO monitors.py [line:51] mon pid is 52463
2017-06-13 11:31:22,139 INFO monitors.py [line:92] execute command is sudo -i kill -9 52463 & sleep 3
2017-06-13 11:31:55,900 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:31:55,901 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:31:55,901 INFO TC57_kill_lead_mon_on_multiMon.py [line:78] now the leader mon is tahoe126
2017-06-13 11:31:55,901 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:31:55,901 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:32:26,108 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 11:32:26,108 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:32:26,108 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:32:26,368 INFO monitors.py [line:106] oot     61430     1  3 11:31 ?        00:00:00 ceph-mon -i taheo125
denali   72370 72369  0 11:32 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   72372 72370  0 11:32 ?        00:00:00 grep ceph-mon

2017-06-13 11:32:26,368 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:33:26,390 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:33:26,731 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 126, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53690: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2331 GB used, 3858 GB / 6189 GB avail
                3216 active+clean
  client io 13418 kB/s rd, 231 MB/s wr, 1851 op/s rd, 29599 op/s wr
stdin: is not a tty

2017-06-13 11:33:26,731 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:33:26,731 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:33:26,732 INFO TC57_kill_lead_mon_on_multiMon.py [line:88] stop mon service on taheo125 in cluster successfully
2017-06-13 11:33:27,252 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:33:27,253 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:33:27,253 INFO TC57_kill_lead_mon_on_multiMon.py [line:104] now the leader mon is taheo125
2017-06-13 11:33:27,253 INFO TC57_kill_lead_mon_on_multiMon.py [line:106] taheo125 is back
2017-06-13 11:33:27,253 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:33:27,253 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:33:57,433 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:33:57,433 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:33:57,433 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:33:31.061501 7f03e34fa4c0 -1 asok(0x7f03df9351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:33:31.068444 7f03e34fa4c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:33:57,433 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:33:57,433 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:33:57,695 INFO monitors.py [line:106] enali   33302 33261  0 11:34 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   33304 33302  0 11:34 ?        00:00:00 grep ceph-mon
root     61430     1  2 11:31 ?        00:00:02 ceph-mon -i taheo125

2017-06-13 11:33:57,695 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:34:57,952 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:34:58,306 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 126, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53725: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2341 GB used, 3848 GB / 6189 GB avail
                3216 active+clean
  client io 13343 kB/s rd, 234 MB/s wr, 1840 op/s rd, 30051 op/s wr
stdin: is not a tty

2017-06-13 11:34:58,306 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:34:58,307 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:34:58,307 INFO TC57_kill_lead_mon_on_multiMon.py [line:121] stop mon service on taheo125 in cluster successfully
2017-06-13 11:34:58,847 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:34:58,847 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:34:58,848 INFO TC57_kill_lead_mon_on_multiMon.py [line:138] now the leader mon is taheo125
2017-06-13 11:34:58,848 INFO TC57_kill_lead_mon_on_multiMon.py [line:140] taheo125 is back
2017-06-13 11:34:59,102 INFO client.py [line:172] ['oot      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 69 02:57 ?        00:25:46 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 64 02:57 ?        00:23:58 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 65 02:57 ?        00:24:20 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 65 02:57 ?        00:24:18 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 64 02:57 ?        00:24:01 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali   126676 126675  0 03:35 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   126678 126676  0 03:35 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 11:34:59,102 INFO client.py [line:174] IO is running
2017-06-13 11:34:59,368 INFO monitors.py [line:45] ['enali   55946 55935  0 11:35 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   55950 55946  0 11:35 ?        00:00:00 grep ceph-mon', 'root     61430     1  2 11:31 ?        00:00:04 ceph-mon -i taheo125', '']
2017-06-13 11:34:59,368 INFO monitors.py [line:51] mon pid is 61430
2017-06-13 11:34:59,368 INFO monitors.py [line:92] execute command is sudo -i kill -9 61430 & sleep 3
2017-06-13 11:35:33,129 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:35:33,130 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:35:33,130 INFO TC57_kill_lead_mon_on_multiMon.py [line:67] now the leader mon is tahoe126
2017-06-13 11:35:33,388 INFO monitors.py [line:45] ['enali   63001 62966  0 11:35 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   63013 63001  0 11:35 ?        00:00:00 grep ceph-mon', '']
2017-06-13 11:35:33,388 INFO monitors.py [line:51] mon pid is 61430
2017-06-13 11:35:33,388 INFO monitors.py [line:92] execute command is sudo -i kill -9 61430 & sleep 3
2017-06-13 11:36:07,138 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:36:07,138 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:36:07,138 INFO TC57_kill_lead_mon_on_multiMon.py [line:78] now the leader mon is tahoe126
2017-06-13 11:36:07,138 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:36:07,138 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:36:37,350 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 11:36:37,350 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:36:37,350 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:36:37,617 INFO monitors.py [line:106] enali    9371  9366  0 11:36 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali    9373  9371  0 11:36 ?        00:00:00 grep ceph-mon
root     70657     1  3 11:36 ?        00:00:00 ceph-mon -i taheo125

2017-06-13 11:36:37,617 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:37:37,619 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:37:37,958 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 130, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53781: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2357 GB used, 3831 GB / 6189 GB avail
                3216 active+clean
  client io 12556 kB/s rd, 272 MB/s wr, 1851 op/s rd, 34933 op/s wr
stdin: is not a tty

2017-06-13 11:37:37,958 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:37:37,958 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:37:37,958 INFO TC57_kill_lead_mon_on_multiMon.py [line:88] stop mon service on taheo125 in cluster successfully
2017-06-13 11:37:38,427 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:37:38,427 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:37:38,428 INFO TC57_kill_lead_mon_on_multiMon.py [line:104] now the leader mon is taheo125
2017-06-13 11:37:38,428 INFO TC57_kill_lead_mon_on_multiMon.py [line:106] taheo125 is back
2017-06-13 11:37:38,428 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:37:38,428 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:38:08,676 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:38:08,676 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:38:08,677 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:37:42.307515 7f33a1fa94c0 -1 asok(0x7f339e5351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:37:42.314885 7f33a1fa94c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:38:08,677 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:38:08,677 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:38:08,962 INFO monitors.py [line:106] enali   43116 43115  0 11:38 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   43118 43116  0 11:38 ?        00:00:00 grep ceph-mon
root     70657     1  2 11:36 ?        00:00:02 ceph-mon -i taheo125

2017-06-13 11:38:08,962 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:39:09,182 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:39:09,536 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 130, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53815: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2367 GB used, 3822 GB / 6189 GB avail
                3216 active+clean
  client io 17606 kB/s rd, 280 MB/s wr, 2381 op/s rd, 35933 op/s wr
stdin: is not a tty

2017-06-13 11:39:09,536 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:39:09,536 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:39:09,536 INFO TC57_kill_lead_mon_on_multiMon.py [line:121] stop mon service on taheo125 in cluster successfully
2017-06-13 11:39:10,076 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:39:10,076 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:39:10,076 INFO TC57_kill_lead_mon_on_multiMon.py [line:138] now the leader mon is taheo125
2017-06-13 11:39:10,076 INFO TC57_kill_lead_mon_on_multiMon.py [line:140] taheo125 is back
2017-06-13 11:39:10,296 INFO client.py [line:172] ['oot      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 64 02:57 ?        00:26:37 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 65 02:57 ?        00:27:09 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 66 02:57 ?        00:27:34 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 66 02:57 ?        00:27:32 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 66 02:57 ?        00:27:16 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali   139641 139640  0 03:39 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   139643 139641  0 03:39 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 11:39:10,296 INFO client.py [line:174] IO is running
2017-06-13 11:39:10,564 INFO monitors.py [line:45] ['enali   65330 65325  0 11:39 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   65332 65330  0 11:39 ?        00:00:00 grep ceph-mon', 'root     70657     1  2 11:36 ?        00:00:04 ceph-mon -i taheo125', '']
2017-06-13 11:39:10,564 INFO monitors.py [line:51] mon pid is 70657
2017-06-13 11:39:10,564 INFO monitors.py [line:92] execute command is sudo -i kill -9 70657 & sleep 3
2017-06-13 11:39:47,394 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:39:47,394 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:39:47,394 INFO TC57_kill_lead_mon_on_multiMon.py [line:67] now the leader mon is tahoe126
2017-06-13 11:39:47,657 INFO monitors.py [line:45] ['enali   72146 72116  0 11:39 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   72149 72146  0 11:39 ?        00:00:00 grep ceph-mon', '']
2017-06-13 11:39:47,657 INFO monitors.py [line:51] mon pid is 70657
2017-06-13 11:39:47,657 INFO monitors.py [line:92] execute command is sudo -i kill -9 70657 & sleep 3
2017-06-13 11:40:22,665 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:40:22,665 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:40:22,665 INFO TC57_kill_lead_mon_on_multiMon.py [line:78] now the leader mon is tahoe126
2017-06-13 11:40:22,665 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:40:22,665 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:40:52,877 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 11:40:52,877 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:40:52,877 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:40:53,111 INFO monitors.py [line:106] oot      9050     1  2 11:40 ?        00:00:00 ceph-mon -i taheo125
denali   20253 20243  0 11:40 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   20255 20253  0 11:40 ?        00:00:00 grep ceph-mon

2017-06-13 11:40:53,111 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:41:53,171 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:41:53,491 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 134, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53872: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2385 GB used, 3804 GB / 6189 GB avail
                3216 active+clean
  client io 10304 kB/s rd, 184 MB/s wr, 1466 op/s rd, 23589 op/s wr
stdin: is not a tty

2017-06-13 11:41:53,491 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:41:53,491 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:41:53,491 INFO TC57_kill_lead_mon_on_multiMon.py [line:88] stop mon service on taheo125 in cluster successfully
2017-06-13 11:41:54,064 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:41:54,064 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:41:54,065 INFO TC57_kill_lead_mon_on_multiMon.py [line:104] now the leader mon is taheo125
2017-06-13 11:41:54,065 INFO TC57_kill_lead_mon_on_multiMon.py [line:106] taheo125 is back
2017-06-13 11:41:54,065 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:41:54,065 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:42:24,214 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:42:24,214 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:42:24,215 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:41:57.844619 7f4dbfeb64c0 -1 asok(0x7f4dbc1351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:41:57.852858 7f4dbfeb64c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:42:24,215 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:42:24,215 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:42:24,477 INFO monitors.py [line:106] oot      9050     1  2 11:40 ?        00:00:02 ceph-mon -i taheo125
denali   53364 53363  0 11:42 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   53366 53364  0 11:42 ?        00:00:00 grep ceph-mon

2017-06-13 11:42:24,477 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:43:24,740 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:43:25,107 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 134, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53907: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2393 GB used, 3796 GB / 6189 GB avail
                3216 active+clean
  client io 14432 kB/s rd, 210 MB/s wr, 1983 op/s rd, 26944 op/s wr
stdin: is not a tty

2017-06-13 11:43:25,108 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:43:25,108 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:43:25,108 INFO TC57_kill_lead_mon_on_multiMon.py [line:121] stop mon service on taheo125 in cluster successfully
2017-06-13 11:43:25,642 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:43:25,642 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:43:25,642 INFO TC57_kill_lead_mon_on_multiMon.py [line:138] now the leader mon is taheo125
2017-06-13 11:43:25,643 INFO TC57_kill_lead_mon_on_multiMon.py [line:140] taheo125 is back
2017-06-13 11:43:25,891 INFO client.py [line:172] ['oot      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 60 02:57 ?        00:27:30 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 66 02:57 ?        00:30:21 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 67 02:57 ?        00:30:50 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 67 02:57 ?        00:30:47 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 66 02:57 ?        00:30:26 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali   152627 152626  0 03:43 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   152629 152627  0 03:43 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 11:43:25,892 INFO client.py [line:174] IO is running
2017-06-13 11:43:26,152 INFO monitors.py [line:45] ['enali    2038  2037  0 11:43 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali    2040  2038  0 11:43 ?        00:00:00 grep ceph-mon', 'root      9050     1  2 11:40 ?        00:00:04 ceph-mon -i taheo125', '']
2017-06-13 11:43:26,152 INFO monitors.py [line:51] mon pid is 9050
2017-06-13 11:43:26,152 INFO monitors.py [line:92] execute command is sudo -i kill -9 9050 & sleep 3
2017-06-13 11:44:02,935 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:44:02,935 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:44:02,935 INFO TC57_kill_lead_mon_on_multiMon.py [line:67] now the leader mon is tahoe126
2017-06-13 11:44:03,197 INFO monitors.py [line:45] ['enali    9918  9908  0 11:44 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali    9922  9918  0 11:44 ?        00:00:00 grep ceph-mon', '']
2017-06-13 11:44:03,198 INFO monitors.py [line:51] mon pid is 9050
2017-06-13 11:44:03,198 INFO monitors.py [line:92] execute command is sudo -i kill -9 9050 & sleep 3
2017-06-13 11:44:40,036 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:44:40,036 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:44:40,036 INFO TC57_kill_lead_mon_on_multiMon.py [line:78] now the leader mon is tahoe126
2017-06-13 11:44:40,036 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:44:40,036 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:45:10,224 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 11:45:10,224 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:45:10,224 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:45:10,452 INFO monitors.py [line:106] oot     19255     1  3 11:44 ?        00:00:00 ceph-mon -i taheo125
denali   30211 30200  0 11:45 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   30213 30211  0 11:45 ?        00:00:00 grep ceph-mon

2017-06-13 11:45:10,452 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:46:10,488 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:46:10,810 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 138, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v53968: 3216 pgs, 13 pools, 1499 GB data, 375 kobjects
            2410 GB used, 3779 GB / 6189 GB avail
                3216 active+clean
  client io 19834 kB/s rd, 185 MB/s wr, 2658 op/s rd, 23800 op/s wr
stdin: is not a tty

2017-06-13 11:46:10,810 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:46:10,810 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:46:10,810 INFO TC57_kill_lead_mon_on_multiMon.py [line:88] stop mon service on taheo125 in cluster successfully
2017-06-13 11:46:11,372 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:46:11,372 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:46:11,373 INFO TC57_kill_lead_mon_on_multiMon.py [line:104] now the leader mon is taheo125
2017-06-13 11:46:11,373 INFO TC57_kill_lead_mon_on_multiMon.py [line:106] taheo125 is back
2017-06-13 11:46:11,373 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:46:11,373 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:46:41,584 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:46:41,585 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:46:41,585 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:46:15.217759 7f1e1f3c34c0 -1 asok(0x7f1e1b9351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:46:15.229825 7f1e1f3c34c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:46:41,585 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:46:41,585 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:46:41,782 INFO monitors.py [line:106] oot     19255     1  2 11:44 ?        00:00:02 ceph-mon -i taheo125
denali   63394 63393  0 11:46 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   63396 63394  0 11:46 ?        00:00:00 grep ceph-mon

2017-06-13 11:46:41,782 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:47:42,037 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:47:42,379 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 138, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v54004: 3216 pgs, 13 pools, 1499 GB data, 375 kobjects
            2419 GB used, 3770 GB / 6189 GB avail
                3216 active+clean
  client io 10060 kB/s rd, 223 MB/s wr, 1435 op/s rd, 28662 op/s wr
stdin: is not a tty

2017-06-13 11:47:42,379 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:47:42,379 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:47:42,380 INFO TC57_kill_lead_mon_on_multiMon.py [line:121] stop mon service on taheo125 in cluster successfully
2017-06-13 11:47:42,970 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:47:42,970 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:47:42,970 INFO TC57_kill_lead_mon_on_multiMon.py [line:138] now the leader mon is taheo125
2017-06-13 11:47:42,970 INFO TC57_kill_lead_mon_on_multiMon.py [line:140] taheo125 is back
2017-06-13 11:47:43,218 INFO client.py [line:172] ['oot      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 56 02:57 ?        00:28:18 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 67 02:57 ?        00:33:35 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 68 02:57 ?        00:34:07 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 68 02:57 ?        00:34:05 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 67 02:57 ?        00:33:40 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali   165834 165833  0 03:47 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   165836 165834  0 03:47 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 11:47:43,219 INFO client.py [line:174] IO is running
2017-06-13 11:47:43,480 INFO monitors.py [line:45] ['enali   13577 13576  0 11:47 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   13579 13577  0 11:47 ?        00:00:00 grep ceph-mon', 'root     19255     1  2 11:44 ?        00:00:04 ceph-mon -i taheo125', '']
2017-06-13 11:47:43,480 INFO monitors.py [line:51] mon pid is 19255
2017-06-13 11:47:43,480 INFO monitors.py [line:92] execute command is sudo -i kill -9 19255 & sleep 3
2017-06-13 11:48:17,260 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:48:17,260 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:48:17,261 INFO TC57_kill_lead_mon_on_multiMon.py [line:67] now the leader mon is tahoe126
2017-06-13 11:48:17,559 INFO monitors.py [line:45] ['enali   20358 20346  0 11:48 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   20362 20358  0 11:48 ?        00:00:00 grep ceph-mon', '']
2017-06-13 11:48:17,560 INFO monitors.py [line:51] mon pid is 19255
2017-06-13 11:48:17,560 INFO monitors.py [line:92] execute command is sudo -i kill -9 19255 & sleep 3
2017-06-13 11:48:51,376 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:48:51,376 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:48:51,376 INFO TC57_kill_lead_mon_on_multiMon.py [line:78] now the leader mon is tahoe126
2017-06-13 11:48:51,376 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:48:51,376 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:49:21,584 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 11:49:21,584 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:49:21,585 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:49:21,835 INFO monitors.py [line:106] oot     28970     1  3 11:48 ?        00:00:00 ceph-mon -i taheo125
denali   39783 39782  0 11:49 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   39785 39783  0 11:49 ?        00:00:00 grep ceph-mon

2017-06-13 11:49:21,835 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:50:21,896 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:50:22,276 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 142, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v54061: 3216 pgs, 13 pools, 1499 GB data, 375 kobjects
            2434 GB used, 3755 GB / 6189 GB avail
                3216 active+clean
  client io 23294 kB/s rd, 174 MB/s wr, 3098 op/s rd, 22397 op/s wr
stdin: is not a tty

2017-06-13 11:50:22,276 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:50:22,276 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:50:22,276 INFO TC57_kill_lead_mon_on_multiMon.py [line:88] stop mon service on taheo125 in cluster successfully
2017-06-13 11:50:22,895 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:50:22,895 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:50:22,895 INFO TC57_kill_lead_mon_on_multiMon.py [line:104] now the leader mon is taheo125
2017-06-13 11:50:22,895 INFO TC57_kill_lead_mon_on_multiMon.py [line:106] taheo125 is back
2017-06-13 11:50:22,895 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:50:22,895 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:50:53,082 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:50:53,082 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:50:53,082 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:50:26.716008 7fbb4b8614c0 -1 asok(0x7fbb47d351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:50:26.724494 7fbb4b8614c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:50:53,082 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:50:53,082 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:50:53,344 INFO monitors.py [line:106] oot     28970     1  2 11:48 ?        00:00:02 ceph-mon -i taheo125
denali   72448 72446  0 11:50 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   72450 72448  0 11:50 ?        00:00:00 grep ceph-mon

2017-06-13 11:50:53,344 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:51:53,611 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:51:53,981 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 142, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v54097: 3216 pgs, 13 pools, 1499 GB data, 375 kobjects
            2442 GB used, 3747 GB / 6189 GB avail
                3216 active+clean
  client io 16941 kB/s rd, 236 MB/s wr, 2298 op/s rd, 30279 op/s wr
stdin: is not a tty

2017-06-13 11:51:53,982 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:51:53,982 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:51:53,982 INFO TC57_kill_lead_mon_on_multiMon.py [line:121] stop mon service on taheo125 in cluster successfully
2017-06-13 11:51:54,565 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:51:54,565 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:51:54,565 INFO TC57_kill_lead_mon_on_multiMon.py [line:138] now the leader mon is taheo125
2017-06-13 11:51:54,565 INFO TC57_kill_lead_mon_on_multiMon.py [line:140] taheo125 is back
2017-06-13 11:51:54,785 INFO client.py [line:172] ['oot      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 53 02:57 ?        00:29:10 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 68 02:57 ?        00:36:45 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 69 02:57 ?        00:37:19 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 68 02:57 ?        00:37:16 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 68 02:57 ?        00:36:49 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali   178779 178778  0 03:51 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   178781 178779  0 03:51 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 11:51:54,785 INFO client.py [line:174] IO is running
2017-06-13 11:51:55,066 INFO monitors.py [line:45] ['enali   22332 22331  0 11:51 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   22335 22332  0 11:51 ?        00:00:00 grep ceph-mon', 'root     28970     1  2 11:48 ?        00:00:04 ceph-mon -i taheo125', '']
2017-06-13 11:51:55,066 INFO monitors.py [line:51] mon pid is 28970
2017-06-13 11:51:55,066 INFO monitors.py [line:92] execute command is sudo -i kill -9 28970 & sleep 3
2017-06-13 11:52:28,846 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:52:28,846 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:52:28,846 INFO TC57_kill_lead_mon_on_multiMon.py [line:67] now the leader mon is tahoe126
2017-06-13 11:52:29,105 INFO monitors.py [line:45] ['enali   27869 27868  0 11:52 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   27871 27869  0 11:52 ?        00:00:00 grep ceph-mon', '']
2017-06-13 11:52:29,106 INFO monitors.py [line:51] mon pid is 28970
2017-06-13 11:52:29,106 INFO monitors.py [line:92] execute command is sudo -i kill -9 28970 & sleep 3
2017-06-13 11:53:05,914 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:53:05,914 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:53:05,914 INFO TC57_kill_lead_mon_on_multiMon.py [line:78] now the leader mon is tahoe126
2017-06-13 11:53:05,914 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:53:05,914 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:53:36,135 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 11:53:36,136 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:53:36,136 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:53:36,407 INFO monitors.py [line:106] oot     36560     1  3 11:53 ?        00:00:00 ceph-mon -i taheo125
denali   47793 47791  0 11:53 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   47796 47793  0 11:53 ?        00:00:00 grep ceph-mon

2017-06-13 11:53:36,407 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:54:36,436 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:54:36,764 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 146, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v54155: 3216 pgs, 13 pools, 1499 GB data, 375 kobjects
            2457 GB used, 3731 GB / 6189 GB avail
                3216 active+clean
  client io 12082 kB/s rd, 164 MB/s wr, 1684 op/s rd, 21097 op/s wr
stdin: is not a tty

2017-06-13 11:54:36,764 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:54:36,764 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:54:36,764 INFO TC57_kill_lead_mon_on_multiMon.py [line:88] stop mon service on taheo125 in cluster successfully
2017-06-13 11:54:37,368 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:54:37,368 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:54:37,368 INFO TC57_kill_lead_mon_on_multiMon.py [line:104] now the leader mon is taheo125
2017-06-13 11:54:37,368 INFO TC57_kill_lead_mon_on_multiMon.py [line:106] taheo125 is back
2017-06-13 11:54:37,368 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:54:37,368 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:55:07,949 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:55:07,949 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:55:07,949 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:54:41.583365 7f33b90734c0 -1 asok(0x7f33b55351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:54:41.590385 7f33b90734c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:55:07,949 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:55:07,949 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:55:08,203 INFO monitors.py [line:106] enali    8573  8572  0 11:55 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali    8611  8573  0 11:55 ?        00:00:00 grep ceph-mon
root     36560     1  2 11:53 ?        00:00:02 ceph-mon -i taheo125

2017-06-13 11:55:08,203 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:56:08,450 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:56:08,801 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 146, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e430: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v54190: 3216 pgs, 13 pools, 1499 GB data, 375 kobjects
            2466 GB used, 3723 GB / 6189 GB avail
                3216 active+clean
  client io 6539 kB/s rd, 179 MB/s wr, 1003 op/s rd, 23015 op/s wr
stdin: is not a tty

2017-06-13 11:56:08,801 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:56:08,801 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:56:08,801 INFO TC57_kill_lead_mon_on_multiMon.py [line:121] stop mon service on taheo125 in cluster successfully
2017-06-13 11:56:09,361 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:56:09,361 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:56:09,361 INFO TC57_kill_lead_mon_on_multiMon.py [line:138] now the leader mon is taheo125
2017-06-13 11:56:09,361 INFO TC57_kill_lead_mon_on_multiMon.py [line:140] taheo125 is back
2017-06-13 11:56:09,615 INFO client.py [line:172] ['oot      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 51 02:57 ?        00:29:58 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 68 02:57 ?        00:39:56 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 69 02:57 ?        00:40:31 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 69 02:57 ?        00:40:32 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 68 02:57 ?        00:40:01 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali   191637 191636  0 03:56 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   191639 191637  0 03:56 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 11:56:09,615 INFO client.py [line:174] IO is running
2017-06-13 11:56:09,884 INFO monitors.py [line:45] ['enali   30925 30866  0 11:56 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   30927 30925  0 11:56 ?        00:00:00 grep ceph-mon', 'root     36560     1  2 11:53 ?        00:00:04 ceph-mon -i taheo125', '']
2017-06-13 11:56:09,884 INFO monitors.py [line:51] mon pid is 36560
2017-06-13 11:56:09,884 INFO monitors.py [line:92] execute command is sudo -i kill -9 36560 & sleep 3
2017-06-13 11:56:43,748 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:56:43,748 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:56:43,749 INFO TC57_kill_lead_mon_on_multiMon.py [line:67] now the leader mon is tahoe126
2017-06-13 11:56:44,014 INFO monitors.py [line:45] ['enali   36242 36241  0 11:56 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   36244 36242  0 11:56 ?        00:00:00 grep ceph-mon', '']
2017-06-13 11:56:44,014 INFO monitors.py [line:51] mon pid is 36560
2017-06-13 11:56:44,014 INFO monitors.py [line:92] execute command is sudo -i kill -9 36560 & sleep 3
2017-06-13 11:57:20,777 INFO monitors.py [line:126]    "quorum_leader_name": "tahoe126",
stdin: is not a tty

2017-06-13 11:57:20,777 INFO monitors.py [line:129]    "quorum_leader_name": "tahoe126",
2017-06-13 11:57:20,777 INFO TC57_kill_lead_mon_on_multiMon.py [line:78] now the leader mon is tahoe126
2017-06-13 11:57:20,777 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:57:20,777 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:57:50,990 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 11:57:50,990 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:57:50,990 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:57:51,256 INFO monitors.py [line:106] oot     46128     1  3 11:57 ?        00:00:00 ceph-mon -i taheo125
denali   57183 57177  0 11:57 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   57185 57183  0 11:57 ?        00:00:00 grep ceph-mon

2017-06-13 11:57:51,256 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 11:58:51,315 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 11:58:51,713 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 150, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e431: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v54245: 3216 pgs, 13 pools, 1499 GB data, 375 kobjects
            2482 GB used, 3707 GB / 6189 GB avail
                3216 active+clean
  client io 17291 kB/s rd, 180 MB/s wr, 2349 op/s rd, 23068 op/s wr
stdin: is not a tty

2017-06-13 11:58:51,713 INFO cluster.py [line:238] PG number is 3216
2017-06-13 11:58:51,713 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 11:58:51,713 INFO TC57_kill_lead_mon_on_multiMon.py [line:88] stop mon service on taheo125 in cluster successfully
2017-06-13 11:58:52,298 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 11:58:52,298 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 11:58:52,298 INFO TC57_kill_lead_mon_on_multiMon.py [line:104] now the leader mon is taheo125
2017-06-13 11:58:52,298 INFO TC57_kill_lead_mon_on_multiMon.py [line:106] taheo125 is back
2017-06-13 11:58:52,299 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 11:58:52,299 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:59:22,511 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 11:59:22,511 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 11:59:22,511 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 11:58:56.145538 7fdc45e2c4c0 -1 asok(0x7fdc421351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 11:58:56.156550 7fdc45e2c4c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 11:59:22,511 INFO monitors.py [line:103] node is  taheo125
2017-06-13 11:59:22,511 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 11:59:22,771 INFO monitors.py [line:106] enali   17703 17671  0 11:59 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   17709 17703  0 11:59 ?        00:00:00 grep ceph-mon
root     46128     1  2 11:57 ?        00:00:03 ceph-mon -i taheo125

2017-06-13 11:59:22,771 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 12:00:23,017 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 12:00:23,361 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 150, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e431: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v54278: 3216 pgs, 13 pools, 1499 GB data, 375 kobjects
            2488 GB used, 3700 GB / 6189 GB avail
                3216 active+clean
  client io 17410 kB/s rd, 185 MB/s wr, 2362 op/s rd, 23747 op/s wr
stdin: is not a tty

2017-06-13 12:00:23,361 INFO cluster.py [line:238] PG number is 3216
2017-06-13 12:00:23,362 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 12:00:23,362 INFO TC57_kill_lead_mon_on_multiMon.py [line:121] stop mon service on taheo125 in cluster successfully
2017-06-13 12:00:23,945 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 12:00:23,945 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 12:00:23,945 INFO TC57_kill_lead_mon_on_multiMon.py [line:138] now the leader mon is taheo125
2017-06-13 12:00:23,945 INFO TC57_kill_lead_mon_on_multiMon.py [line:140] taheo125 is back
2017-06-13 12:00:24,197 INFO client.py [line:172] ['enali     9338   9337  0 04:00 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali     9340   9338  0 04:00 ?        00:00:00 grep fio', 'root      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 49 02:57 ?        00:30:49 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 68 02:57 ?        00:43:05 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 70 02:57 ?        00:43:47 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 69 02:57 ?        00:43:45 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 69 02:57 ?        00:43:11 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'stdin: is not a tty', '']
2017-06-13 12:00:24,198 INFO client.py [line:174] IO is running
2017-06-13 12:00:24,423 INFO TC57_kill_lead_mon_on_multiMon.py [line:152] case runs complete
