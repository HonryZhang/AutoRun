2017-06-13 12:00:24,470 INFO TC60_pause_cluster.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. pause the cluster 
3. check if IO can start or not
4. resume the cluster
5. check if IO can start or not

2017-06-13 12:00:25,114 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 12:00:25,114 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 12:00:27,119 INFO TC60_pause_cluster.py [line:29] start to check cluster status before case running
2017-06-13 12:00:27,119 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 12:00:27,448 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 150, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e431: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v54280: 3216 pgs, 13 pools, 1499 GB data, 375 kobjects
            2489 GB used, 3700 GB / 6189 GB avail
                3216 active+clean
  client io 23016 kB/s rd, 163 MB/s wr, 3051 op/s rd, 20919 op/s wr
stdin: is not a tty

2017-06-13 12:00:27,448 INFO cluster.py [line:238] PG number is 3216
2017-06-13 12:00:27,448 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 12:00:27,448 INFO TC60_pause_cluster.py [line:32] health status is OK
2017-06-13 12:00:27,448 INFO TC60_pause_cluster.py [line:38] 
Step1: Check IO from clients
2017-06-13 12:00:27,925 INFO client.py [line:172] ['enali     9624   9623  0 04:00 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali     9626   9624  0 04:00 ?        00:00:00 grep fio', 'root      15202      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15204  15202 49 02:57 ?        00:30:50 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0', 'root      15269      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15271  15269 68 02:57 ?        00:43:08 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      15339      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15341  15339 70 02:57 ?        00:43:49 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      15409      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15411  15409 69 02:57 ?        00:43:47 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      15481      1  0 02:57 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      15484  15481 69 02:57 ?        00:43:14 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'stdin: is not a tty', '']
2017-06-13 12:00:27,925 INFO client.py [line:174] IO is running
2017-06-13 12:01:27,944 INFO TC60_pause_cluster.py [line:46] 
Step2: pause all osds
2017-06-13 12:01:31,217 INFO node.py [line:227] execute command is sudo -i ceph osd set pause
2017-06-13 12:01:31,217 INFO node.py [line:228] tdin: is not a tty
pauserd,pausewr is set

2017-06-13 12:01:31,218 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 12:01:31,624 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_WARN
            pauserd,pausewr,sortbitwise,require_jewel_osds,require_kraken_osds flag(s) set
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 150, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e432: 18 osds: 18 up, 18 in
            flags pauserd,pausewr,sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v54303: 3216 pgs, 13 pools, 1499 GB data, 375 kobjects
            2495 GB used, 3694 GB / 6189 GB avail
                3216 active+clean
  client io 54940 kB/s rd, 427 MB/s wr, 7309 op/s rd, 54683 op/s wr
stdin: is not a tty

2017-06-13 12:01:31,625 INFO cluster.py [line:238] PG number is 3216
2017-06-13 12:01:31,625 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 12:01:31,625 INFO TC60_pause_cluster.py [line:50] pause cluster successfully
2017-06-13 12:01:31,856 INFO TC60_pause_cluster.py [line:63] 
Step3: resume all osds
2017-06-13 12:01:35,316 INFO node.py [line:239] execute command is sudo -i ceph osd unset pause
2017-06-13 12:01:35,316 INFO node.py [line:240] tdin: is not a tty
pauserd,pausewr is unset

2017-06-13 12:01:35,510 INFO TC60_pause_cluster.py [line:73] 
Case runs successfully
