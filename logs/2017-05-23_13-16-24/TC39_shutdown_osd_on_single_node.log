2017-05-23 13:30:45,209 INFO TC39_shutdown_osd_on_single_node.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. stop all osds in sequence
4. start all osds in sequence
5. check the cluster status
6. repeat step 2-5 on the other node

2017-05-23 13:30:45,209 INFO TC39_shutdown_osd_on_single_node.py [line:25] the timeout is 6000
2017-05-23 13:30:48,641 INFO monitors.py [line:123] raceback (most recent call last):
  File "/usr/local/bin/ceph", line 118, in <module>
    import rados
ImportError: librados.so.2: cannot map zero-fill pages: Cannot allocate memory

2017-05-23 13:30:48,641 INFO monitors.py [line:126] raceback (most recent call last):
2017-05-23 13:30:48,641 INFO monitors.py [line:126]   File "/usr/local/bin/ceph", line 118, in <module>
2017-05-23 13:30:48,641 INFO monitors.py [line:126]     import rados
2017-05-23 13:30:48,641 INFO monitors.py [line:126] ImportError: librados.so.2: cannot map zero-fill pages: Cannot allocate memory
2017-05-23 13:30:48,641 INFO monitors.py [line:126] 
2017-05-23 13:30:48,641 INFO node.py [line:113] init osd on node denali01
2017-05-23 13:30:49,858 INFO node.py [line:128] osd.0  ---> processId 5378
2017-05-23 13:30:49,858 INFO node.py [line:128] osd.1  ---> processId 32089
2017-05-23 13:30:49,858 INFO node.py [line:128] osd.2  ---> processId 29976
2017-05-23 13:30:49,858 INFO node.py [line:128] osd.0  ---> processId 5378
2017-05-23 13:30:49,858 INFO node.py [line:128] osd.1  ---> processId 32089
2017-05-23 13:30:49,858 INFO node.py [line:128] osd.2  ---> processId 29976
2017-05-23 13:30:49,858 INFO osd.py [line:26] node is  denali01
2017-05-23 13:30:49,872 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=0 & sleep 3
2017-05-23 13:30:59,351 INFO osd.py [line:30] osd osd.0 is shutdown successfully
2017-05-23 13:31:04,358 INFO osd.py [line:100] node is  denali01
2017-05-23 13:31:04,358 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-23 13:31:35,107 INFO osd.py [line:105] osd osd.0 is start successfully
2017-05-23 13:31:35,107 INFO osd.py [line:26] node is  denali01
2017-05-23 13:31:35,107 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=1 & sleep 3
2017-05-23 13:31:40,553 INFO osd.py [line:30] osd osd.1 is shutdown successfully
2017-05-23 13:31:45,565 INFO osd.py [line:100] node is  denali01
2017-05-23 13:31:45,565 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-23 13:32:16,142 INFO osd.py [line:105] osd osd.1 is start successfully
2017-05-23 13:32:16,142 INFO osd.py [line:26] node is  denali01
2017-05-23 13:32:16,142 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=2 & sleep 3
2017-05-23 13:32:22,138 INFO osd.py [line:30] osd osd.2 is shutdown successfully
2017-05-23 13:32:27,150 INFO osd.py [line:100] node is  denali01
2017-05-23 13:32:27,150 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-23 13:32:57,756 INFO osd.py [line:105] osd osd.2 is start successfully
2017-05-23 13:32:57,756 INFO osd.py [line:26] node is  denali01
2017-05-23 13:32:57,756 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=0 & sleep 3
2017-05-23 13:33:01,345 ERROR osd.py [line:32] Error when shutdown osdosd.0
2017-05-23 13:33:01,345 ERROR osd.py [line:33] sudo -i stop ceph-osd id=0 & sleep 3
2017-05-23 13:33:01,345 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/0

2017-05-23 13:33:06,354 INFO osd.py [line:100] node is  denali01
2017-05-23 13:33:06,354 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-23 13:33:37,003 INFO osd.py [line:105] osd osd.0 is start successfully
2017-05-23 13:33:37,003 INFO osd.py [line:26] node is  denali01
2017-05-23 13:33:37,003 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=1 & sleep 3
2017-05-23 13:33:40,622 ERROR osd.py [line:32] Error when shutdown osdosd.1
2017-05-23 13:33:40,622 ERROR osd.py [line:33] sudo -i stop ceph-osd id=1 & sleep 3
2017-05-23 13:33:40,622 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/1

2017-05-23 13:33:45,632 INFO osd.py [line:100] node is  denali01
2017-05-23 13:33:45,632 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-23 13:34:16,292 INFO osd.py [line:105] osd osd.1 is start successfully
2017-05-23 13:34:16,292 INFO osd.py [line:26] node is  denali01
2017-05-23 13:34:16,292 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=2 & sleep 3
2017-05-23 13:34:19,960 ERROR osd.py [line:32] Error when shutdown osdosd.2
2017-05-23 13:34:19,960 ERROR osd.py [line:33] sudo -i stop ceph-osd id=2 & sleep 3
2017-05-23 13:34:19,960 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/2

2017-05-23 13:34:24,971 INFO osd.py [line:100] node is  denali01
2017-05-23 13:34:24,971 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-23 13:34:55,661 INFO osd.py [line:105] osd osd.2 is start successfully
2017-05-23 13:34:56,492 INFO node.py [line:149] osd.0  ---> processId 23796
2017-05-23 13:34:56,492 INFO node.py [line:149] osd.1  ---> processId 24811
2017-05-23 13:34:56,492 INFO node.py [line:149] osd.2  ---> processId 26054
2017-05-23 13:34:56,492 INFO node.py [line:149] osd.0  ---> processId 23796
2017-05-23 13:34:56,492 INFO node.py [line:149] osd.1  ---> processId 24811
2017-05-23 13:34:56,492 INFO node.py [line:149] osd.2  ---> processId 26054
2017-05-23 13:34:56,492 INFO node.py [line:113] init osd on node denali02
2017-05-23 13:34:57,349 INFO node.py [line:128] osd.3  ---> processId 5144
2017-05-23 13:34:57,349 INFO node.py [line:128] osd.4  ---> processId 6760
2017-05-23 13:34:57,349 INFO node.py [line:128] osd.5  ---> processId 16021
2017-05-23 13:34:57,349 INFO node.py [line:128] osd.3  ---> processId 5144
2017-05-23 13:34:57,349 INFO node.py [line:128] osd.4  ---> processId 6760
2017-05-23 13:34:57,349 INFO node.py [line:128] osd.5  ---> processId 16021
2017-05-23 13:34:57,349 INFO osd.py [line:26] node is  denali02
2017-05-23 13:34:57,349 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=3 & sleep 3
2017-05-23 13:35:12,648 INFO osd.py [line:30] osd osd.3 is shutdown successfully
2017-05-23 13:35:17,657 INFO osd.py [line:100] node is  denali02
2017-05-23 13:35:17,657 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-05-23 13:35:48,405 INFO osd.py [line:105] osd osd.3 is start successfully
2017-05-23 13:35:48,405 INFO osd.py [line:26] node is  denali02
2017-05-23 13:35:48,405 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=4 & sleep 3
2017-05-23 13:35:54,348 INFO osd.py [line:30] osd osd.4 is shutdown successfully
2017-05-23 13:35:59,351 INFO osd.py [line:100] node is  denali02
2017-05-23 13:35:59,351 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-05-23 13:36:29,982 INFO osd.py [line:105] osd osd.4 is start successfully
2017-05-23 13:36:29,982 INFO osd.py [line:26] node is  denali02
2017-05-23 13:36:29,982 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=5 & sleep 3
2017-05-23 13:36:35,660 INFO osd.py [line:30] osd osd.5 is shutdown successfully
2017-05-23 13:36:40,667 INFO osd.py [line:100] node is  denali02
2017-05-23 13:36:40,667 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 5 & sleep 30
2017-05-23 13:37:11,361 INFO osd.py [line:105] osd osd.5 is start successfully
2017-05-23 13:37:11,361 INFO osd.py [line:26] node is  denali02
2017-05-23 13:37:11,361 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=3 & sleep 3
2017-05-23 13:37:15,015 ERROR osd.py [line:32] Error when shutdown osdosd.3
2017-05-23 13:37:15,015 ERROR osd.py [line:33] sudo -i stop ceph-osd id=3 & sleep 3
2017-05-23 13:37:15,015 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/3

2017-05-23 13:37:20,022 INFO osd.py [line:100] node is  denali02
2017-05-23 13:37:20,022 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-05-23 13:37:50,664 INFO osd.py [line:105] osd osd.3 is start successfully
2017-05-23 13:37:50,664 INFO osd.py [line:26] node is  denali02
2017-05-23 13:37:50,664 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=4 & sleep 3
2017-05-23 13:37:54,256 ERROR osd.py [line:32] Error when shutdown osdosd.4
2017-05-23 13:37:54,256 ERROR osd.py [line:33] sudo -i stop ceph-osd id=4 & sleep 3
2017-05-23 13:37:54,256 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/4

2017-05-23 13:37:59,266 INFO osd.py [line:100] node is  denali02
2017-05-23 13:37:59,266 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-05-23 13:38:29,842 INFO osd.py [line:105] osd osd.4 is start successfully
2017-05-23 13:38:29,842 INFO osd.py [line:26] node is  denali02
2017-05-23 13:38:29,842 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=5 & sleep 3
2017-05-23 13:38:33,417 ERROR osd.py [line:32] Error when shutdown osdosd.5
2017-05-23 13:38:33,417 ERROR osd.py [line:33] sudo -i stop ceph-osd id=5 & sleep 3
2017-05-23 13:38:33,417 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/5

2017-05-23 13:38:38,424 INFO osd.py [line:100] node is  denali02
2017-05-23 13:38:38,424 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 5 & sleep 30
2017-05-23 13:39:08,990 INFO osd.py [line:105] osd osd.5 is start successfully
2017-05-23 13:39:09,631 INFO node.py [line:149] osd.3  ---> processId 7483
2017-05-23 13:39:09,631 INFO node.py [line:149] osd.4  ---> processId 7638
2017-05-23 13:39:09,631 INFO node.py [line:149] osd.5  ---> processId 7794
2017-05-23 13:39:09,631 INFO node.py [line:149] osd.3  ---> processId 7483
2017-05-23 13:39:09,631 INFO node.py [line:149] osd.4  ---> processId 7638
2017-05-23 13:39:09,631 INFO node.py [line:149] osd.5  ---> processId 7794
2017-05-23 13:39:09,631 INFO node.py [line:113] init osd on node denali03
2017-05-23 13:39:10,394 INFO node.py [line:128] osd.6  ---> processId 5068
2017-05-23 13:39:10,394 INFO node.py [line:128] osd.7  ---> processId 5403
2017-05-23 13:39:10,394 INFO node.py [line:128] osd.8  ---> processId 17247
2017-05-23 13:39:10,394 INFO node.py [line:128] osd.6  ---> processId 5068
2017-05-23 13:39:10,394 INFO node.py [line:128] osd.7  ---> processId 5403
2017-05-23 13:39:10,394 INFO node.py [line:128] osd.8  ---> processId 17247
2017-05-23 13:39:10,394 INFO osd.py [line:26] node is  denali03
2017-05-23 13:39:10,394 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=6 & sleep 3
2017-05-23 13:39:26,786 INFO osd.py [line:30] osd osd.6 is shutdown successfully
2017-05-23 13:39:31,798 INFO osd.py [line:100] node is  denali03
2017-05-23 13:39:31,798 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 6 & sleep 30
2017-05-23 13:40:02,378 INFO osd.py [line:105] osd osd.6 is start successfully
2017-05-23 13:40:02,378 INFO osd.py [line:26] node is  denali03
2017-05-23 13:40:02,378 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=7 & sleep 3
2017-05-23 13:40:14,621 INFO osd.py [line:30] osd osd.7 is shutdown successfully
2017-05-23 13:40:19,634 INFO osd.py [line:100] node is  denali03
2017-05-23 13:40:19,634 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 7 & sleep 30
2017-05-23 13:40:50,250 INFO osd.py [line:105] osd osd.7 is start successfully
2017-05-23 13:40:50,250 INFO osd.py [line:26] node is  denali03
2017-05-23 13:40:50,250 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=8 & sleep 3
2017-05-23 13:40:55,757 INFO osd.py [line:30] osd osd.8 is shutdown successfully
2017-05-23 13:41:00,766 INFO osd.py [line:100] node is  denali03
2017-05-23 13:41:00,766 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 8 & sleep 30
2017-05-23 13:41:31,456 INFO osd.py [line:105] osd osd.8 is start successfully
2017-05-23 13:41:31,456 INFO osd.py [line:26] node is  denali03
2017-05-23 13:41:31,456 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=6 & sleep 3
2017-05-23 13:41:35,122 ERROR osd.py [line:32] Error when shutdown osdosd.6
2017-05-23 13:41:35,122 ERROR osd.py [line:33] sudo -i stop ceph-osd id=6 & sleep 3
2017-05-23 13:41:35,122 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/6

2017-05-23 13:41:40,134 INFO osd.py [line:100] node is  denali03
2017-05-23 13:41:40,134 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 6 & sleep 30
2017-05-23 13:42:10,747 INFO osd.py [line:105] osd osd.6 is start successfully
2017-05-23 13:42:10,747 INFO osd.py [line:26] node is  denali03
2017-05-23 13:42:10,747 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=7 & sleep 3
2017-05-23 13:42:14,460 ERROR osd.py [line:32] Error when shutdown osdosd.7
2017-05-23 13:42:14,460 ERROR osd.py [line:33] sudo -i stop ceph-osd id=7 & sleep 3
2017-05-23 13:42:14,460 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/7

2017-05-23 13:42:19,473 INFO osd.py [line:100] node is  denali03
2017-05-23 13:42:19,473 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 7 & sleep 30
2017-05-23 13:42:50,134 INFO osd.py [line:105] osd osd.7 is start successfully
2017-05-23 13:42:50,134 INFO osd.py [line:26] node is  denali03
2017-05-23 13:42:50,134 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=8 & sleep 3
2017-05-23 13:42:53,789 ERROR osd.py [line:32] Error when shutdown osdosd.8
2017-05-23 13:42:53,789 ERROR osd.py [line:33] sudo -i stop ceph-osd id=8 & sleep 3
2017-05-23 13:42:53,789 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/8

2017-05-23 13:42:58,798 INFO osd.py [line:100] node is  denali03
2017-05-23 13:42:58,798 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 8 & sleep 30
2017-05-23 13:43:29,628 INFO osd.py [line:105] osd osd.8 is start successfully
2017-05-23 13:43:30,313 INFO node.py [line:149] osd.6  ---> processId 19806
2017-05-23 13:43:30,313 INFO node.py [line:149] osd.7  ---> processId 19962
2017-05-23 13:43:30,313 INFO node.py [line:149] osd.8  ---> processId 20119
2017-05-23 13:43:30,313 INFO node.py [line:149] osd.6  ---> processId 19806
2017-05-23 13:43:30,313 INFO node.py [line:149] osd.7  ---> processId 19962
2017-05-23 13:43:30,313 INFO node.py [line:149] osd.8  ---> processId 20119
2017-05-23 13:43:30,313 INFO node.py [line:113] init osd on node denali01
2017-05-23 13:43:31,144 INFO node.py [line:128] osd.0  ---> processId 23796
2017-05-23 13:43:31,144 INFO node.py [line:128] osd.1  ---> processId 24811
2017-05-23 13:43:31,144 INFO node.py [line:128] osd.2  ---> processId 26054
2017-05-23 13:43:31,144 INFO node.py [line:128] osd.0  ---> processId 23796
2017-05-23 13:43:31,144 INFO node.py [line:128] osd.1  ---> processId 24811
2017-05-23 13:43:31,144 INFO node.py [line:128] osd.2  ---> processId 26054
2017-05-23 13:43:31,144 INFO osd.py [line:26] node is  denali01
2017-05-23 13:43:31,144 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=0 & sleep 3
2017-05-23 13:43:34,844 ERROR osd.py [line:32] Error when shutdown osdosd.0
2017-05-23 13:43:34,844 ERROR osd.py [line:33] sudo -i stop ceph-osd id=0 & sleep 3
2017-05-23 13:43:34,844 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/0

2017-05-23 13:43:39,852 INFO osd.py [line:100] node is  denali01
2017-05-23 13:43:39,852 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-23 13:44:10,443 INFO osd.py [line:105] osd osd.0 is start successfully
2017-05-23 13:44:10,443 INFO osd.py [line:26] node is  denali01
2017-05-23 13:44:10,443 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=1 & sleep 3
2017-05-23 13:44:14,249 ERROR osd.py [line:32] Error when shutdown osdosd.1
2017-05-23 13:44:14,249 ERROR osd.py [line:33] sudo -i stop ceph-osd id=1 & sleep 3
2017-05-23 13:44:14,249 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/1

2017-05-23 13:44:19,263 INFO osd.py [line:100] node is  denali01
2017-05-23 13:44:19,263 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-23 13:44:50,099 INFO osd.py [line:105] osd osd.1 is start successfully
2017-05-23 13:44:50,099 INFO osd.py [line:26] node is  denali01
2017-05-23 13:44:50,099 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=2 & sleep 3
2017-05-23 13:44:53,769 ERROR osd.py [line:32] Error when shutdown osdosd.2
2017-05-23 13:44:53,769 ERROR osd.py [line:33] sudo -i stop ceph-osd id=2 & sleep 3
2017-05-23 13:44:53,769 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/2

2017-05-23 13:44:58,779 INFO osd.py [line:100] node is  denali01
2017-05-23 13:44:58,779 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-23 13:45:29,344 INFO osd.py [line:105] osd osd.2 is start successfully
2017-05-23 13:45:29,344 INFO osd.py [line:26] node is  denali01
2017-05-23 13:45:29,344 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=0 & sleep 3
2017-05-23 13:45:32,980 ERROR osd.py [line:32] Error when shutdown osdosd.0
2017-05-23 13:45:32,980 ERROR osd.py [line:33] sudo -i stop ceph-osd id=0 & sleep 3
2017-05-23 13:45:32,980 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/0

2017-05-23 13:45:37,989 INFO osd.py [line:100] node is  denali01
2017-05-23 13:45:37,989 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-23 13:46:08,607 INFO osd.py [line:105] osd osd.0 is start successfully
2017-05-23 13:46:08,607 INFO osd.py [line:26] node is  denali01
2017-05-23 13:46:08,607 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=1 & sleep 3
2017-05-23 13:46:12,273 ERROR osd.py [line:32] Error when shutdown osdosd.1
2017-05-23 13:46:12,273 ERROR osd.py [line:33] sudo -i stop ceph-osd id=1 & sleep 3
2017-05-23 13:46:12,273 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/1

2017-05-23 13:46:17,283 INFO osd.py [line:100] node is  denali01
2017-05-23 13:46:17,283 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-23 13:46:47,938 INFO osd.py [line:105] osd osd.1 is start successfully
2017-05-23 13:46:47,938 INFO osd.py [line:26] node is  denali01
2017-05-23 13:46:47,938 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=2 & sleep 3
2017-05-23 13:46:51,598 ERROR osd.py [line:32] Error when shutdown osdosd.2
2017-05-23 13:46:51,598 ERROR osd.py [line:33] sudo -i stop ceph-osd id=2 & sleep 3
2017-05-23 13:46:51,598 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/2

2017-05-23 13:46:56,605 INFO osd.py [line:100] node is  denali01
2017-05-23 13:46:56,605 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-23 13:47:27,358 INFO osd.py [line:105] osd osd.2 is start successfully
2017-05-23 13:47:28,154 INFO node.py [line:149] osd.0  ---> processId 23796
2017-05-23 13:47:28,154 INFO node.py [line:149] osd.1  ---> processId 24811
2017-05-23 13:47:28,154 INFO node.py [line:149] osd.2  ---> processId 26054
2017-05-23 13:47:28,154 INFO node.py [line:149] osd.0  ---> processId 23796
2017-05-23 13:47:28,154 INFO node.py [line:149] osd.1  ---> processId 24811
2017-05-23 13:47:28,154 INFO node.py [line:149] osd.2  ---> processId 26054
2017-05-23 13:47:28,154 INFO node.py [line:113] init osd on node denali02
2017-05-23 13:47:34,177 INFO node.py [line:128] osd.3  ---> processId 7483
2017-05-23 13:47:34,177 INFO node.py [line:128] osd.4  ---> processId 7638
2017-05-23 13:47:34,177 INFO node.py [line:128] osd.5  ---> processId 7794
2017-05-23 13:47:34,177 INFO node.py [line:128] osd.3  ---> processId 7483
2017-05-23 13:47:34,177 INFO node.py [line:128] osd.4  ---> processId 7638
2017-05-23 13:47:34,177 INFO node.py [line:128] osd.5  ---> processId 7794
2017-05-23 13:47:34,177 INFO osd.py [line:26] node is  denali02
2017-05-23 13:47:34,177 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=3 & sleep 3
2017-05-23 13:47:37,799 ERROR osd.py [line:32] Error when shutdown osdosd.3
2017-05-23 13:47:37,799 ERROR osd.py [line:33] sudo -i stop ceph-osd id=3 & sleep 3
2017-05-23 13:47:37,799 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/3

2017-05-23 13:47:42,812 INFO osd.py [line:100] node is  denali02
2017-05-23 13:47:42,812 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-05-23 13:48:13,829 INFO osd.py [line:105] osd osd.3 is start successfully
2017-05-23 13:48:13,829 INFO osd.py [line:26] node is  denali02
2017-05-23 13:48:13,829 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=4 & sleep 3
2017-05-23 13:48:17,530 ERROR osd.py [line:32] Error when shutdown osdosd.4
2017-05-23 13:48:17,530 ERROR osd.py [line:33] sudo -i stop ceph-osd id=4 & sleep 3
2017-05-23 13:48:17,530 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/4

2017-05-23 13:48:22,542 INFO osd.py [line:100] node is  denali02
2017-05-23 13:48:22,542 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-05-23 13:48:53,415 INFO osd.py [line:105] osd osd.4 is start successfully
2017-05-23 13:48:53,415 INFO osd.py [line:26] node is  denali02
2017-05-23 13:48:53,415 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=5 & sleep 3
2017-05-23 13:48:57,118 ERROR osd.py [line:32] Error when shutdown osdosd.5
2017-05-23 13:48:57,118 ERROR osd.py [line:33] sudo -i stop ceph-osd id=5 & sleep 3
2017-05-23 13:48:57,118 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/5

2017-05-23 13:49:02,125 INFO osd.py [line:100] node is  denali02
2017-05-23 13:49:02,125 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 5 & sleep 30
2017-05-23 13:49:32,805 INFO osd.py [line:105] osd osd.5 is start successfully
2017-05-23 13:49:32,805 INFO osd.py [line:26] node is  denali02
2017-05-23 13:49:32,805 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=3 & sleep 3
2017-05-23 13:49:36,441 ERROR osd.py [line:32] Error when shutdown osdosd.3
2017-05-23 13:49:36,441 ERROR osd.py [line:33] sudo -i stop ceph-osd id=3 & sleep 3
2017-05-23 13:49:36,441 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/3

2017-05-23 13:49:41,450 INFO osd.py [line:100] node is  denali02
2017-05-23 13:49:41,450 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-05-23 13:50:12,039 INFO osd.py [line:105] osd osd.3 is start successfully
2017-05-23 13:50:12,039 INFO osd.py [line:26] node is  denali02
2017-05-23 13:50:12,039 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=4 & sleep 3
2017-05-23 13:50:15,644 ERROR osd.py [line:32] Error when shutdown osdosd.4
2017-05-23 13:50:15,644 ERROR osd.py [line:33] sudo -i stop ceph-osd id=4 & sleep 3
2017-05-23 13:50:15,644 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/4

2017-05-23 13:50:20,644 INFO osd.py [line:100] node is  denali02
2017-05-23 13:50:20,644 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-05-23 13:50:51,278 INFO osd.py [line:105] osd osd.4 is start successfully
2017-05-23 13:50:51,278 INFO osd.py [line:26] node is  denali02
2017-05-23 13:50:51,278 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=5 & sleep 3
2017-05-23 13:50:54,913 ERROR osd.py [line:32] Error when shutdown osdosd.5
2017-05-23 13:50:54,913 ERROR osd.py [line:33] sudo -i stop ceph-osd id=5 & sleep 3
2017-05-23 13:50:54,913 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/5

2017-05-23 13:50:59,924 INFO osd.py [line:100] node is  denali02
2017-05-23 13:50:59,924 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 5 & sleep 30
2017-05-23 13:51:30,694 INFO osd.py [line:105] osd osd.5 is start successfully
2017-05-23 13:51:31,380 INFO node.py [line:149] osd.3  ---> processId 7483
2017-05-23 13:51:31,380 INFO node.py [line:149] osd.4  ---> processId 7638
2017-05-23 13:51:31,380 INFO node.py [line:149] osd.5  ---> processId 7794
2017-05-23 13:51:31,380 INFO node.py [line:149] osd.3  ---> processId 7483
2017-05-23 13:51:31,380 INFO node.py [line:149] osd.4  ---> processId 7638
2017-05-23 13:51:31,380 INFO node.py [line:149] osd.5  ---> processId 7794
2017-05-23 13:51:31,380 INFO node.py [line:113] init osd on node denali03
2017-05-23 13:51:32,055 INFO node.py [line:128] osd.6  ---> processId 19806
2017-05-23 13:51:32,055 INFO node.py [line:128] osd.7  ---> processId 19962
2017-05-23 13:51:32,055 INFO node.py [line:128] osd.8  ---> processId 20119
2017-05-23 13:51:32,055 INFO node.py [line:128] osd.6  ---> processId 19806
2017-05-23 13:51:32,055 INFO node.py [line:128] osd.7  ---> processId 19962
2017-05-23 13:51:32,055 INFO node.py [line:128] osd.8  ---> processId 20119
2017-05-23 13:51:32,055 INFO osd.py [line:26] node is  denali03
2017-05-23 13:51:32,055 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=6 & sleep 3
2017-05-23 13:51:35,704 ERROR osd.py [line:32] Error when shutdown osdosd.6
2017-05-23 13:51:35,704 ERROR osd.py [line:33] sudo -i stop ceph-osd id=6 & sleep 3
2017-05-23 13:51:35,704 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/6

2017-05-23 13:51:40,714 INFO osd.py [line:100] node is  denali03
2017-05-23 13:51:40,714 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 6 & sleep 30
2017-05-23 13:52:11,405 INFO osd.py [line:105] osd osd.6 is start successfully
2017-05-23 13:52:11,405 INFO osd.py [line:26] node is  denali03
2017-05-23 13:52:11,405 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=7 & sleep 3
2017-05-23 13:52:15,509 ERROR osd.py [line:32] Error when shutdown osdosd.7
2017-05-23 13:52:15,509 ERROR osd.py [line:33] sudo -i stop ceph-osd id=7 & sleep 3
2017-05-23 13:52:15,509 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/7

2017-05-23 13:52:20,523 INFO osd.py [line:100] node is  denali03
2017-05-23 13:52:20,523 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 7 & sleep 30
2017-05-23 13:52:51,565 INFO osd.py [line:105] osd osd.7 is start successfully
2017-05-23 13:52:51,565 INFO osd.py [line:26] node is  denali03
2017-05-23 13:52:51,565 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=8 & sleep 3
2017-05-23 13:52:55,216 ERROR osd.py [line:32] Error when shutdown osdosd.8
2017-05-23 13:52:55,216 ERROR osd.py [line:33] sudo -i stop ceph-osd id=8 & sleep 3
2017-05-23 13:52:55,216 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/8

2017-05-23 13:53:00,226 INFO osd.py [line:100] node is  denali03
2017-05-23 13:53:00,226 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 8 & sleep 30
2017-05-23 13:53:30,927 INFO osd.py [line:105] osd osd.8 is start successfully
2017-05-23 13:53:30,927 INFO osd.py [line:26] node is  denali03
2017-05-23 13:53:30,927 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=6 & sleep 3
2017-05-23 13:53:34,546 ERROR osd.py [line:32] Error when shutdown osdosd.6
2017-05-23 13:53:34,546 ERROR osd.py [line:33] sudo -i stop ceph-osd id=6 & sleep 3
2017-05-23 13:53:34,546 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/6

2017-05-23 13:53:39,555 INFO osd.py [line:100] node is  denali03
2017-05-23 13:53:39,555 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 6 & sleep 30
2017-05-23 13:54:10,260 INFO osd.py [line:105] osd osd.6 is start successfully
2017-05-23 13:54:10,260 INFO osd.py [line:26] node is  denali03
2017-05-23 13:54:10,260 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=7 & sleep 3
2017-05-23 13:54:13,930 ERROR osd.py [line:32] Error when shutdown osdosd.7
2017-05-23 13:54:13,930 ERROR osd.py [line:33] sudo -i stop ceph-osd id=7 & sleep 3
2017-05-23 13:54:13,930 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/7

2017-05-23 13:54:18,933 INFO osd.py [line:100] node is  denali03
2017-05-23 13:54:18,933 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 7 & sleep 30
2017-05-23 13:54:49,556 INFO osd.py [line:105] osd osd.7 is start successfully
2017-05-23 13:54:49,556 INFO osd.py [line:26] node is  denali03
2017-05-23 13:54:49,556 INFO osd.py [line:27] execute command is sudo -i stop ceph-osd id=8 & sleep 3
2017-05-23 13:54:53,223 ERROR osd.py [line:32] Error when shutdown osdosd.8
2017-05-23 13:54:53,223 ERROR osd.py [line:33] sudo -i stop ceph-osd id=8 & sleep 3
2017-05-23 13:54:53,223 ERROR osd.py [line:34] tdin: is not a tty
stop: Unknown instance: ceph/8

2017-05-23 13:54:58,236 INFO osd.py [line:100] node is  denali03
2017-05-23 13:54:58,236 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 8 & sleep 30
2017-05-23 13:55:28,997 INFO osd.py [line:105] osd osd.8 is start successfully
2017-05-23 13:55:29,668 INFO node.py [line:149] osd.6  ---> processId 19806
2017-05-23 13:55:29,668 INFO node.py [line:149] osd.7  ---> processId 19962
2017-05-23 13:55:29,668 INFO node.py [line:149] osd.8  ---> processId 20119
2017-05-23 13:55:29,668 INFO node.py [line:149] osd.6  ---> processId 19806
2017-05-23 13:55:29,668 INFO node.py [line:149] osd.7  ---> processId 19962
2017-05-23 13:55:29,668 INFO node.py [line:149] osd.8  ---> processId 20119
2017-05-23 13:55:29,668 INFO TC39_shutdown_osd_on_single_node.py [line:31] start to check cluster status before case running
2017-05-23 13:55:29,730 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:55:31,341 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e272: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v80925: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47368 MB used, 292 GB / 338 GB avail
                3016 active+clean
  client io 7173 B/s rd, 2192 kB/s wr, 10 op/s rd, 548 op/s wr

2017-05-23 13:55:31,341 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:55:31,341 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:55:31,341 INFO TC39_shutdown_osd_on_single_node.py [line:34] health status is OK
2017-05-23 13:55:31,341 INFO TC39_shutdown_osd_on_single_node.py [line:39] 
Step 1: start IO from clients
2017-05-23 13:55:31,341 INFO base.py [line:35] 
Now start IO on  clientrbdImg0
2017-05-23 13:55:32,042 INFO client.py [line:125] pid info is 9012
2017-05-23 13:55:32,042 INFO client.py [line:125] pid info is 9013
2017-05-23 13:55:32,042 INFO client.py [line:125] pid info is 10206
2017-05-23 13:55:32,042 INFO client.py [line:125] pid info is 10209
2017-05-23 13:55:32,042 INFO base.py [line:35] 
Now start IO on  clientrbdImg1
2017-05-23 13:55:32,713 INFO client.py [line:125] pid info is 9062
2017-05-23 13:55:32,713 INFO client.py [line:125] pid info is 9063
2017-05-23 13:55:32,713 INFO client.py [line:125] pid info is 10236
2017-05-23 13:55:32,713 INFO client.py [line:125] pid info is 10237
2017-05-23 13:55:32,713 INFO base.py [line:35] 
Now start IO on  clientrbdImg2
2017-05-23 13:55:33,696 INFO client.py [line:125] pid info is 9112
2017-05-23 13:55:33,696 INFO client.py [line:125] pid info is 9113
2017-05-23 13:55:33,696 INFO client.py [line:125] pid info is 10268
2017-05-23 13:55:33,696 INFO client.py [line:125] pid info is 10269
2017-05-23 13:55:33,696 INFO base.py [line:35] 
Now start IO on  clientrbdImg3
2017-05-23 13:55:34,808 INFO client.py [line:125] pid info is 9165
2017-05-23 13:55:34,808 INFO client.py [line:125] pid info is 9166
2017-05-23 13:55:34,808 INFO client.py [line:125] pid info is 10331
2017-05-23 13:55:34,808 INFO client.py [line:125] pid info is 10332
2017-05-23 13:55:34,808 INFO base.py [line:35] 
Now start IO on  clientrbdImg4
2017-05-23 13:55:35,805 INFO client.py [line:125] pid info is 9215
2017-05-23 13:55:35,805 INFO client.py [line:125] pid info is 9216
2017-05-23 13:55:35,805 INFO client.py [line:125] pid info is 10384
2017-05-23 13:55:35,805 INFO client.py [line:125] pid info is 10385
2017-05-23 13:55:35,805 INFO base.py [line:35] 
Now start IO on  clientrbdImg5
2017-05-23 13:55:48,700 INFO client.py [line:125] pid info is 9266
2017-05-23 13:55:48,700 INFO client.py [line:125] pid info is 9267
2017-05-23 13:55:48,700 INFO client.py [line:125] pid info is 10433
2017-05-23 13:55:48,700 INFO client.py [line:125] pid info is 10434
2017-05-23 13:55:48,700 INFO base.py [line:35] 
Now start IO on  clientrbdImg6
2017-05-23 13:55:58,279 INFO client.py [line:125] pid info is 9315
2017-05-23 13:55:58,279 INFO client.py [line:125] pid info is 9316
2017-05-23 13:55:58,279 INFO client.py [line:125] pid info is 10502
2017-05-23 13:55:58,279 INFO base.py [line:35] 
Now start IO on  clientrbdImg7
2017-05-23 13:55:59,184 INFO client.py [line:125] pid info is 9365
2017-05-23 13:55:59,184 INFO client.py [line:125] pid info is 9366
2017-05-23 13:55:59,184 INFO client.py [line:125] pid info is 10531
2017-05-23 13:55:59,184 INFO base.py [line:35] 
Now start IO on  clientrbdImg8
2017-05-23 13:56:00,354 INFO client.py [line:125] pid info is 9413
2017-05-23 13:56:00,354 INFO client.py [line:125] pid info is 9414
2017-05-23 13:56:00,354 INFO client.py [line:125] pid info is 10561
2017-05-23 13:56:00,354 INFO client.py [line:125] pid info is 10562
2017-05-23 13:56:00,354 INFO base.py [line:35] 
Now start IO on  clientrbdImg9
2017-05-23 13:56:01,430 INFO client.py [line:125] pid info is 9463
2017-05-23 13:56:01,430 INFO client.py [line:125] pid info is 9464
2017-05-23 13:56:01,430 INFO client.py [line:125] pid info is 10593
2017-05-23 13:56:01,430 INFO client.py [line:125] pid info is 10594
2017-05-23 13:56:01,430 INFO base.py [line:35] 
Now start IO on  clientrbdImg0
2017-05-23 13:56:02,256 INFO client.py [line:125] pid info is 9012
2017-05-23 13:56:02,256 INFO client.py [line:125] pid info is 9013
2017-05-23 13:56:02,256 INFO client.py [line:125] pid info is 10206
2017-05-23 13:56:02,256 INFO client.py [line:125] pid info is 10209
2017-05-23 13:56:02,256 INFO client.py [line:125] pid info is 10623
2017-05-23 13:56:02,256 INFO client.py [line:125] pid info is 10624
2017-05-23 13:56:02,256 INFO base.py [line:35] 
Now start IO on  clientrbdImg1
2017-05-23 13:56:03,115 INFO client.py [line:125] pid info is 9062
2017-05-23 13:56:03,115 INFO client.py [line:125] pid info is 9063
2017-05-23 13:56:03,115 INFO client.py [line:125] pid info is 10236
2017-05-23 13:56:03,115 INFO client.py [line:125] pid info is 10237
2017-05-23 13:56:03,115 INFO client.py [line:125] pid info is 10653
2017-05-23 13:56:03,115 INFO client.py [line:125] pid info is 10654
2017-05-23 13:56:03,115 INFO base.py [line:35] 
Now start IO on  clientrbdImg2
2017-05-23 13:56:23,108 INFO client.py [line:125] pid info is 9112
2017-05-23 13:56:23,108 INFO client.py [line:125] pid info is 9113
2017-05-23 13:56:23,108 INFO client.py [line:125] pid info is 10268
2017-05-23 13:56:23,108 INFO client.py [line:125] pid info is 10269
2017-05-23 13:56:23,108 INFO client.py [line:125] pid info is 10683
2017-05-23 13:56:23,108 INFO client.py [line:125] pid info is 10687
2017-05-23 13:56:23,108 INFO base.py [line:35] 
Now start IO on  clientrbdImg3
2017-05-23 13:56:35,811 INFO client.py [line:125] pid info is 9165
2017-05-23 13:56:35,811 INFO client.py [line:125] pid info is 9166
2017-05-23 13:56:35,811 INFO client.py [line:125] pid info is 10331
2017-05-23 13:56:35,811 INFO client.py [line:125] pid info is 10332
2017-05-23 13:56:35,811 INFO base.py [line:35] 
Now start IO on  clientrbdImg4
2017-05-23 13:56:36,404 INFO client.py [line:125] pid info is 9215
2017-05-23 13:56:36,404 INFO client.py [line:125] pid info is 9216
2017-05-23 13:56:36,404 INFO client.py [line:125] pid info is 10384
2017-05-23 13:56:36,404 INFO client.py [line:125] pid info is 10385
2017-05-23 13:56:36,404 INFO base.py [line:35] 
Now start IO on  clientrbdImg5
2017-05-23 13:56:37,417 INFO client.py [line:125] pid info is 9266
2017-05-23 13:56:37,417 INFO client.py [line:125] pid info is 9267
2017-05-23 13:56:37,417 INFO client.py [line:125] pid info is 10433
2017-05-23 13:56:37,417 INFO client.py [line:125] pid info is 10434
2017-05-23 13:56:37,417 INFO base.py [line:35] 
Now start IO on  clientrbdImg6
2017-05-23 13:56:38,509 INFO client.py [line:125] pid info is 9315
2017-05-23 13:56:38,509 INFO client.py [line:125] pid info is 9316
2017-05-23 13:56:38,509 INFO client.py [line:125] pid info is 10502
2017-05-23 13:56:38,509 INFO client.py [line:125] pid info is 10547
2017-05-23 13:56:38,509 INFO base.py [line:35] 
Now start IO on  clientrbdImg7
2017-05-23 13:56:39,226 INFO client.py [line:125] pid info is 9365
2017-05-23 13:56:39,226 INFO client.py [line:125] pid info is 9366
2017-05-23 13:56:39,226 INFO client.py [line:125] pid info is 10531
2017-05-23 13:56:39,226 INFO client.py [line:125] pid info is 10548
2017-05-23 13:56:39,226 INFO base.py [line:35] 
Now start IO on  clientrbdImg8
2017-05-23 13:56:40,115 INFO client.py [line:125] pid info is 9413
2017-05-23 13:56:40,115 INFO client.py [line:125] pid info is 9414
2017-05-23 13:56:40,115 INFO client.py [line:125] pid info is 10561
2017-05-23 13:56:40,115 INFO client.py [line:125] pid info is 10562
2017-05-23 13:56:40,115 INFO base.py [line:35] 
Now start IO on  clientrbdImg9
2017-05-23 13:56:40,865 INFO client.py [line:125] pid info is 9463
2017-05-23 13:56:40,865 INFO client.py [line:125] pid info is 9464
2017-05-23 13:56:40,865 INFO client.py [line:125] pid info is 10593
2017-05-23 13:56:40,865 INFO client.py [line:125] pid info is 10594
2017-05-23 13:57:40,869 INFO TC39_shutdown_osd_on_single_node.py [line:43] 
Step 2: stop osd and check IO
2017-05-23 13:57:40,869 INFO TC39_shutdown_osd_on_single_node.py [line:47] 
Now operate osd.0
2017-05-23 13:57:40,869 INFO TC39_shutdown_osd_on_single_node.py [line:49] Set the osd.0 pid for kill
2017-05-23 13:57:41,555 INFO node.py [line:166] osd.0  ---> processId 23796
2017-05-23 13:57:41,555 INFO node.py [line:166] osd.1  ---> processId 24811
2017-05-23 13:57:41,555 INFO node.py [line:166] osd.2  ---> processId 26054
2017-05-23 13:57:41,555 INFO node.py [line:166] osd.0  ---> processId 23796
2017-05-23 13:57:41,555 INFO node.py [line:166] osd.1  ---> processId 24811
2017-05-23 13:57:41,555 INFO node.py [line:166] osd.2  ---> processId 26054
2017-05-23 13:57:41,555 INFO TC39_shutdown_osd_on_single_node.py [line:51] shutdown osd.0 by kill
2017-05-23 13:57:41,555 INFO osd.py [line:51] execute command is sudo -i kill 23796 & sleep 3
2017-05-23 13:57:46,535 INFO client.py [line:159] home/denali

2017-05-23 13:57:49,155 INFO client.py [line:159] home/denali

2017-05-23 13:57:49,670 INFO TC39_shutdown_osd_on_single_node.py [line:56] start osd.0
2017-05-23 13:57:49,670 INFO osd.py [line:100] node is  denali01
2017-05-23 13:57:49,670 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-23 13:58:20,539 INFO osd.py [line:105] osd osd.0 is start successfully
2017-05-23 13:58:20,539 INFO osd.py [line:113] node is  denali01
2017-05-23 13:58:20,539 INFO osd.py [line:114] execute command is ps -ef | grep 'ceph-osd -i 0'
2017-05-23 13:58:21,242 INFO osd.py [line:116] enali    5039  5009  0 13:58 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 0'
denali    5041  5039  0 13:58 ?        00:00:00 grep ceph-osd -i 0

2017-05-23 13:58:21,242 INFO osd.py [line:121] osd.0is not started, start again
2017-05-23 13:58:21,242 INFO osd.py [line:100] node is  denali01
2017-05-23 13:58:21,242 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-23 13:58:52,091 INFO osd.py [line:105] osd osd.0 is start successfully
2017-05-23 13:58:52,091 INFO osd.py [line:113] node is  denali01
2017-05-23 13:58:52,091 INFO osd.py [line:114] execute command is ps -ef | grep 'ceph-osd -i 0'
2017-05-23 13:58:52,825 INFO osd.py [line:116] oot      5113     1 41 13:58 ?        00:00:12 ceph-osd -i 0
denali    6086  6046  0 13:58 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 0'
denali    6088  6086  0 13:58 ?        00:00:00 grep ceph-osd -i 0

2017-05-23 13:58:52,825 INFO osd.py [line:125] osd.0is alrady started
2017-05-23 13:59:23,750 INFO client.py [line:159] home/denali

2017-05-23 13:59:30,775 INFO client.py [line:159] home/denali

2017-05-23 13:59:31,526 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:32,898 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            73 pgs degraded
            2 pgs recovering
            71 pgs recovery_wait
            recovery 2790/174886 objects degraded (1.595%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81089: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            2790/174886 objects degraded (1.595%)
                2941 active+clean
                  71 active+recovery_wait+degraded
                   2 active+recovering+degraded
                   1 active+clean+scrubbing+deep
                   1 active+clean+scrubbing
recovery io 143 MB/s, 75 objects/s
  client io 14138 B/s rd, 1627 kB/s wr, 17 op/s rd, 406 op/s wr

2017-05-23 13:59:32,898 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:32,898 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 13:59:32,914 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:34,085 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            73 pgs degraded
            3 pgs recovering
            70 pgs recovery_wait
            recovery 2744/174886 objects degraded (1.569%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81090: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            2744/174886 objects degraded (1.569%)
                2942 active+clean
                  70 active+recovery_wait+degraded
                   3 active+recovering+degraded
                   1 active+clean+scrubbing
recovery io 191 MB/s, 99 objects/s
  client io 10916 B/s rd, 2484 kB/s wr, 15 op/s rd, 621 op/s wr

2017-05-23 13:59:34,085 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:34,085 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 13:59:34,099 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:35,052 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            70 pgs degraded
            3 pgs recovering
            67 pgs recovery_wait
            recovery 2578/174886 objects degraded (1.474%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81091: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47371 MB used, 292 GB / 338 GB avail
            2578/174886 objects degraded (1.474%)
                2945 active+clean
                  67 active+recovery_wait+degraded
                   3 active+recovering+degraded
                   1 active+clean+scrubbing
recovery io 97690 kB/s, 117 objects/s
  client io 1952 kB/s wr, 0 op/s rd, 488 op/s wr

2017-05-23 13:59:35,052 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:35,052 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 13:59:35,052 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:36,269 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            70 pgs degraded
            3 pgs recovering
            67 pgs recovery_wait
            recovery 2578/174886 objects degraded (1.474%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81091: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47371 MB used, 292 GB / 338 GB avail
            2578/174886 objects degraded (1.474%)
                2945 active+clean
                  67 active+recovery_wait+degraded
                   3 active+recovering+degraded
                   1 active+clean+scrubbing
recovery io 97690 kB/s, 117 objects/s
  client io 1952 kB/s wr, 0 op/s rd, 488 op/s wr

2017-05-23 13:59:36,269 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:36,269 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 13:59:36,269 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:37,674 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            68 pgs degraded
            3 pgs recovering
            65 pgs recovery_wait
            recovery 2510/174886 objects degraded (1.435%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81092: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47371 MB used, 292 GB / 338 GB avail
            2510/174886 objects degraded (1.435%)
                2948 active+clean
                  65 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 84838 kB/s, 85 objects/s
  client io 10437 B/s rd, 2124 kB/s wr, 13 op/s rd, 531 op/s wr

2017-05-23 13:59:37,674 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:37,674 INFO cluster.py [line:231] usefull PG number is 2948
2017-05-23 13:59:37,674 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:39,016 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            62 pgs degraded
            2 pgs recovering
            60 pgs recovery_wait
            recovery 2290/174886 objects degraded (1.309%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81094: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            2290/174886 objects degraded (1.309%)
                2954 active+clean
                  60 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 139 MB/s, 140 objects/s
  client io 2398 kB/s wr, 0 op/s rd, 599 op/s wr

2017-05-23 13:59:39,016 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:39,016 INFO cluster.py [line:231] usefull PG number is 2954
2017-05-23 13:59:39,030 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:40,342 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            62 pgs degraded
            2 pgs recovering
            60 pgs recovery_wait
            recovery 2290/174886 objects degraded (1.309%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81094: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            2290/174886 objects degraded (1.309%)
                2954 active+clean
                  60 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 139 MB/s, 140 objects/s
  client io 2398 kB/s wr, 0 op/s rd, 599 op/s wr

2017-05-23 13:59:40,342 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:40,342 INFO cluster.py [line:231] usefull PG number is 2954
2017-05-23 13:59:40,342 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:41,713 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            62 pgs degraded
            2 pgs recovering
            60 pgs recovery_wait
            recovery 2290/174886 objects degraded (1.309%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81094: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            2290/174886 objects degraded (1.309%)
                2954 active+clean
                  60 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 139 MB/s, 140 objects/s
  client io 2398 kB/s wr, 0 op/s rd, 599 op/s wr

2017-05-23 13:59:41,713 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:41,713 INFO cluster.py [line:231] usefull PG number is 2954
2017-05-23 13:59:41,713 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:43,232 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            59 pgs degraded
            1 pgs recovering
            58 pgs recovery_wait
            recovery 2163/174886 objects degraded (1.237%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81096: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            2163/174886 objects degraded (1.237%)
                2957 active+clean
                  58 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 115 MB/s, 61 objects/s
  client io 9107 B/s rd, 1618 kB/s wr, 11 op/s rd, 404 op/s wr

2017-05-23 13:59:43,232 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:43,232 INFO cluster.py [line:231] usefull PG number is 2957
2017-05-23 13:59:43,232 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:44,884 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            56 pgs degraded
            1 pgs recovering
            55 pgs recovery_wait
            recovery 2005/174886 objects degraded (1.146%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81097: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47371 MB used, 292 GB / 338 GB avail
            2005/174886 objects degraded (1.146%)
                2960 active+clean
                  55 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 119 MB/s, 127 objects/s
  client io 2355 kB/s wr, 0 op/s rd, 588 op/s wr

2017-05-23 13:59:44,884 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:44,884 INFO cluster.py [line:231] usefull PG number is 2960
2017-05-23 13:59:44,884 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:47,401 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            56 pgs degraded
            1 pgs recovering
            55 pgs recovery_wait
            recovery 2001/174886 objects degraded (1.144%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81098: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47371 MB used, 292 GB / 338 GB avail
            2001/174886 objects degraded (1.144%)
                2960 active+clean
                  55 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 28290 kB/s, 57 objects/s
  client io 4890 B/s rd, 1287 kB/s wr, 4 op/s rd, 321 op/s wr

2017-05-23 13:59:47,401 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:47,401 INFO cluster.py [line:231] usefull PG number is 2960
2017-05-23 13:59:47,401 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:48,683 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            54 pgs degraded
            1 pgs recovering
            53 pgs recovery_wait
            recovery 1931/174886 objects degraded (1.104%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81099: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47371 MB used, 292 GB / 338 GB avail
            1931/174886 objects degraded (1.104%)
                2962 active+clean
                  53 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 74923 kB/s, 38 objects/s
  client io 9770 B/s rd, 1418 kB/s wr, 12 op/s rd, 354 op/s wr

2017-05-23 13:59:48,683 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:48,683 INFO cluster.py [line:231] usefull PG number is 2962
2017-05-23 13:59:48,683 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:49,822 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            51 pgs degraded
            1 pgs recovering
            50 pgs recovery_wait
            recovery 1800/174886 objects degraded (1.029%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81100: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1800/174886 objects degraded (1.029%)
                2965 active+clean
                  50 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 152 MB/s, 109 objects/s
  client io 7811 B/s rd, 2309 kB/s wr, 11 op/s rd, 577 op/s wr

2017-05-23 13:59:49,822 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:49,822 INFO cluster.py [line:231] usefull PG number is 2965
2017-05-23 13:59:49,822 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:51,273 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            51 pgs degraded
            2 pgs recovering
            49 pgs recovery_wait
            recovery 1756/174886 objects degraded (1.004%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81101: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1756/174886 objects degraded (1.004%)
                2965 active+clean
                  49 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 108 MB/s, 83 objects/s
  client io 4859 B/s rd, 1077 kB/s wr, 4 op/s rd, 269 op/s wr

2017-05-23 13:59:51,273 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:51,273 INFO cluster.py [line:231] usefull PG number is 2965
2017-05-23 13:59:51,273 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:52,055 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            51 pgs degraded
            2 pgs recovering
            49 pgs recovery_wait
            recovery 1756/174886 objects degraded (1.004%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81101: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1756/174886 objects degraded (1.004%)
                2965 active+clean
                  49 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 108 MB/s, 83 objects/s
  client io 4859 B/s rd, 1077 kB/s wr, 4 op/s rd, 269 op/s wr

2017-05-23 13:59:52,055 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:52,055 INFO cluster.py [line:231] usefull PG number is 2965
2017-05-23 13:59:52,055 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:53,318 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            50 pgs degraded
            2 pgs recovering
            48 pgs recovery_wait
            recovery 1712/174886 objects degraded (0.979%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81102: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1712/174886 objects degraded (0.979%)
                2966 active+clean
                  48 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 104 MB/s, 54 objects/s
  client io 11732 B/s rd, 1589 kB/s wr, 14 op/s rd, 397 op/s wr

2017-05-23 13:59:53,318 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:53,318 INFO cluster.py [line:231] usefull PG number is 2966
2017-05-23 13:59:53,318 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:54,536 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            47 pgs degraded
            4 pgs recovering
            43 pgs recovery_wait
            recovery 1542/174886 objects degraded (0.882%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81104: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47371 MB used, 292 GB / 338 GB avail
            1542/174886 objects degraded (0.882%)
                2969 active+clean
                  43 active+recovery_wait+degraded
                   4 active+recovering+degraded
recovery io 63835 kB/s, 91 objects/s
  client io 2381 kB/s wr, 0 op/s rd, 595 op/s wr

2017-05-23 13:59:54,536 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:54,536 INFO cluster.py [line:231] usefull PG number is 2969
2017-05-23 13:59:54,536 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:55,862 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            47 pgs degraded
            4 pgs recovering
            43 pgs recovery_wait
            recovery 1542/174886 objects degraded (0.882%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81104: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47371 MB used, 292 GB / 338 GB avail
            1542/174886 objects degraded (0.882%)
                2969 active+clean
                  43 active+recovery_wait+degraded
                   4 active+recovering+degraded
recovery io 63835 kB/s, 91 objects/s
  client io 2381 kB/s wr, 0 op/s rd, 595 op/s wr

2017-05-23 13:59:55,862 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:55,862 INFO cluster.py [line:231] usefull PG number is 2969
2017-05-23 13:59:55,878 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:57,002 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            46 pgs degraded
            3 pgs recovering
            43 pgs recovery_wait
            recovery 1523/174886 objects degraded (0.871%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81105: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1523/174886 objects degraded (0.871%)
                2970 active+clean
                  43 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 24364 kB/s, 12 objects/s
  client io 12388 B/s rd, 1710 kB/s wr, 15 op/s rd, 427 op/s wr

2017-05-23 13:59:57,002 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:57,002 INFO cluster.py [line:231] usefull PG number is 2970
2017-05-23 13:59:57,002 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:58,421 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            45 pgs degraded
            3 pgs recovering
            42 pgs recovery_wait
            recovery 1475/174886 objects degraded (0.843%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81106: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1475/174886 objects degraded (0.843%)
                2971 active+clean
                  42 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 84951 kB/s, 43 objects/s
  client io 12420 B/s rd, 2168 kB/s wr, 15 op/s rd, 542 op/s wr

2017-05-23 13:59:58,421 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:58,421 INFO cluster.py [line:231] usefull PG number is 2971
2017-05-23 13:59:58,437 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:59:59,793 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            39 pgs degraded
            3 pgs recovering
            36 pgs recovery_wait
            recovery 1211/174886 objects degraded (0.692%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81108: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1211/174886 objects degraded (0.692%)
                2976 active+clean
                  36 active+recovery_wait+degraded
                   3 active+recovering+degraded
                   1 active+clean+scrubbing+deep
recovery io 173 MB/s, 172 objects/s
  client io 2361 kB/s wr, 0 op/s rd, 590 op/s wr

2017-05-23 13:59:59,793 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:59:59,793 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 13:59:59,793 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:01,151 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            39 pgs degraded
            3 pgs recovering
            36 pgs recovery_wait
            recovery 1211/174886 objects degraded (0.692%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81108: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1211/174886 objects degraded (0.692%)
                2976 active+clean
                  36 active+recovery_wait+degraded
                   3 active+recovering+degraded
                   1 active+clean+scrubbing+deep
recovery io 173 MB/s, 172 objects/s
  client io 2361 kB/s wr, 0 op/s rd, 590 op/s wr

2017-05-23 14:00:01,151 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:01,151 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:00:01,167 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:02,213 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            38 pgs degraded
            3 pgs recovering
            35 pgs recovery_wait
            recovery 1180/174886 objects degraded (0.675%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81109: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1180/174886 objects degraded (0.675%)
                2977 active+clean
                  35 active+recovery_wait+degraded
                   3 active+recovering+degraded
                   1 active+clean+scrubbing+deep
recovery io 37804 kB/s, 20 objects/s
  client io 12247 B/s rd, 1683 kB/s wr, 15 op/s rd, 420 op/s wr

2017-05-23 14:00:02,213 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:02,213 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:00:02,213 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:02,993 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            38 pgs degraded
            3 pgs recovering
            35 pgs recovery_wait
            recovery 1180/174886 objects degraded (0.675%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81109: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1180/174886 objects degraded (0.675%)
                2977 active+clean
                  35 active+recovery_wait+degraded
                   3 active+recovering+degraded
                   1 active+clean+scrubbing+deep
recovery io 37804 kB/s, 20 objects/s
  client io 12247 B/s rd, 1683 kB/s wr, 15 op/s rd, 420 op/s wr

2017-05-23 14:00:02,993 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:02,993 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:00:02,993 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:03,928 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            37 pgs degraded
            3 pgs recovering
            34 pgs recovery_wait
            recovery 1162/174886 objects degraded (0.664%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81110: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            1162/174886 objects degraded (0.664%)
                2978 active+clean
                  34 active+recovery_wait+degraded
                   3 active+recovering+degraded
                   1 active+clean+scrubbing+deep
recovery io 45733 kB/s, 23 objects/s
  client io 9209 B/s rd, 1541 kB/s wr, 11 op/s rd, 385 op/s wr

2017-05-23 14:00:03,928 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:03,928 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:00:03,928 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:05,196 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            31 pgs degraded
            2 pgs recovering
            29 pgs recovery_wait
            recovery 961/174886 objects degraded (0.550%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81111: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            961/174886 objects degraded (0.550%)
                2985 active+clean
                  29 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 107 MB/s, 97 objects/s
  client io 1934 kB/s wr, 0 op/s rd, 483 op/s wr

2017-05-23 14:00:05,196 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:05,196 INFO cluster.py [line:231] usefull PG number is 2985
2017-05-23 14:00:05,196 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:06,381 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            31 pgs degraded
            2 pgs recovering
            29 pgs recovery_wait
            recovery 961/174886 objects degraded (0.550%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81111: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            961/174886 objects degraded (0.550%)
                2985 active+clean
                  29 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 107 MB/s, 97 objects/s
  client io 1934 kB/s wr, 0 op/s rd, 483 op/s wr

2017-05-23 14:00:06,381 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:06,381 INFO cluster.py [line:231] usefull PG number is 2985
2017-05-23 14:00:06,381 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:07,441 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            28 pgs degraded
            1 pgs recovering
            27 pgs recovery_wait
            recovery 869/174886 objects degraded (0.497%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81112: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            869/174886 objects degraded (0.497%)
                2988 active+clean
                  27 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 196 MB/s, 146 objects/s
  client io 12307 B/s rd, 2632 kB/s wr, 15 op/s rd, 658 op/s wr

2017-05-23 14:00:07,441 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:07,441 INFO cluster.py [line:231] usefull PG number is 2988
2017-05-23 14:00:07,441 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:08,628 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            28 pgs degraded
            1 pgs recovering
            27 pgs recovery_wait
            recovery 869/174886 objects degraded (0.497%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81113: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            869/174886 objects degraded (0.497%)
                2988 active+clean
                  27 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 111 MB/s, 60 objects/s
  client io 12367 B/s rd, 1720 kB/s wr, 15 op/s rd, 430 op/s wr

2017-05-23 14:00:08,628 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:08,628 INFO cluster.py [line:231] usefull PG number is 2988
2017-05-23 14:00:08,628 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:09,862 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            23 pgs degraded
            2 pgs recovering
            21 pgs recovery_wait
            recovery 635/174886 objects degraded (0.363%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81114: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            635/174886 objects degraded (0.363%)
                2993 active+clean
                  21 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 134 MB/s, 146 objects/s
  client io 2651 kB/s wr, 0 op/s rd, 662 op/s wr

2017-05-23 14:00:09,862 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:09,862 INFO cluster.py [line:231] usefull PG number is 2993
2017-05-23 14:00:09,862 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:11,204 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            22 pgs degraded
            3 pgs recovering
            19 pgs recovery_wait
            recovery 571/174886 objects degraded (0.326%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81115: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            571/174886 objects degraded (0.326%)
                2994 active+clean
                  19 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 154 MB/s, 128 objects/s
  client io 4552 B/s rd, 1832 kB/s wr, 4 op/s rd, 458 op/s wr

2017-05-23 14:00:11,204 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:11,204 INFO cluster.py [line:231] usefull PG number is 2994
2017-05-23 14:00:11,204 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:12,437 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            21 pgs degraded
            3 pgs recovering
            18 pgs recovery_wait
            recovery 538/174886 objects degraded (0.308%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81116: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            538/174886 objects degraded (0.308%)
                2995 active+clean
                  18 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 108 MB/s, 57 objects/s
  client io 11275 B/s rd, 1922 kB/s wr, 14 op/s rd, 480 op/s wr

2017-05-23 14:00:12,437 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:12,437 INFO cluster.py [line:231] usefull PG number is 2995
2017-05-23 14:00:12,437 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:13,497 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            21 pgs degraded
            3 pgs recovering
            18 pgs recovery_wait
            recovery 538/174886 objects degraded (0.308%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81117: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            538/174886 objects degraded (0.308%)
                2995 active+clean
                  18 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 62199 kB/s, 32 objects/s
  client io 10924 B/s rd, 3611 kB/s wr, 16 op/s rd, 902 op/s wr

2017-05-23 14:00:13,497 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:13,497 INFO cluster.py [line:231] usefull PG number is 2995
2017-05-23 14:00:13,497 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:14,776 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            14 pgs degraded
            3 pgs recovering
            11 pgs recovery_wait
            recovery 335/174886 objects degraded (0.192%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81118: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            335/174886 objects degraded (0.192%)
                3002 active+clean
                  11 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 101389 kB/s, 123 objects/s
  client io 3120 kB/s wr, 0 op/s rd, 780 op/s wr

2017-05-23 14:00:14,778 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:14,778 INFO cluster.py [line:231] usefull PG number is 3002
2017-05-23 14:00:14,778 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:16,026 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            14 pgs degraded
            3 pgs recovering
            11 pgs recovery_wait
            recovery 335/174886 objects degraded (0.192%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81118: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            335/174886 objects degraded (0.192%)
                3002 active+clean
                  11 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 101389 kB/s, 123 objects/s
  client io 3120 kB/s wr, 0 op/s rd, 780 op/s wr

2017-05-23 14:00:16,026 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:16,026 INFO cluster.py [line:231] usefull PG number is 3002
2017-05-23 14:00:16,026 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:17,105 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            12 pgs degraded
            2 pgs recovering
            10 pgs recovery_wait
            recovery 297/174886 objects degraded (0.170%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81119: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            297/174886 objects degraded (0.170%)
                3004 active+clean
                  10 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 111 MB/s, 106 objects/s
  client io 12216 B/s rd, 2990 kB/s wr, 15 op/s rd, 747 op/s wr

2017-05-23 14:00:17,105 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:17,105 INFO cluster.py [line:231] usefull PG number is 3004
2017-05-23 14:00:17,105 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:18,384 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            9 pgs degraded
            2 pgs recovering
            7 pgs recovery_wait
            recovery 236/174886 objects degraded (0.135%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81120: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            236/174886 objects degraded (0.135%)
                3006 active+clean
                   7 active+recovery_wait+degraded
                   2 active+recovering+degraded
                   1 active+clean+scrubbing
recovery io 118 MB/s, 63 objects/s
  client io 12158 B/s rd, 2677 kB/s wr, 15 op/s rd, 669 op/s wr

2017-05-23 14:00:18,384 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:18,384 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:00:18,384 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:19,132 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            2 pgs degraded
            1 pgs recovering
            1 pgs recovery_wait
            recovery 44/174886 objects degraded (0.025%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81121: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            44/174886 objects degraded (0.025%)
                3013 active+clean
                   1 active+recovering+degraded
                   1 active+clean+scrubbing
                   1 active+recovery_wait+degraded
recovery io 150 MB/s, 159 objects/s
  client io 3315 kB/s wr, 0 op/s rd, 828 op/s wr

2017-05-23 14:00:19,132 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:19,132 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:00:19,132 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:20,365 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            2 pgs degraded
            1 pgs recovering
            1 pgs recovery_wait
            recovery 44/174886 objects degraded (0.025%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81122: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            44/174886 objects degraded (0.025%)
                3013 active+clean
                   1 active+recovering+degraded
                   1 active+clean+scrubbing
                   1 active+recovery_wait+degraded
recovery io 34863 kB/s, 82 objects/s
  client io 2071 kB/s wr, 0 op/s rd, 517 op/s wr

2017-05-23 14:00:20,365 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:20,365 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:00:20,365 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:21,832 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            2 pgs degraded
            1 pgs recovering
            1 pgs recovery_wait
            recovery 44/174886 objects degraded (0.025%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81122: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            44/174886 objects degraded (0.025%)
                3013 active+clean
                   1 active+recovering+degraded
                   1 active+clean+scrubbing
                   1 active+recovery_wait+degraded
recovery io 34863 kB/s, 82 objects/s
  client io 2071 kB/s wr, 0 op/s rd, 517 op/s wr

2017-05-23 14:00:21,832 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:21,832 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:00:21,832 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:23,049 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            2 pgs degraded
            1 pgs recovering
            1 pgs recovery_wait
            recovery 44/174886 objects degraded (0.025%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81124: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47372 MB used, 292 GB / 338 GB avail
            44/174886 objects degraded (0.025%)
                3014 active+clean
                   1 active+recovering+degraded
                   1 active+recovery_wait+degraded
  client io 2628 B/s rd, 2358 kB/s wr, 3 op/s rd, 589 op/s wr

2017-05-23 14:00:23,049 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:23,049 INFO cluster.py [line:231] usefull PG number is 3014
2017-05-23 14:00:23,049 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:00:24,203 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e277: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81125: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47371 MB used, 292 GB / 338 GB avail
                3016 active+clean
recovery io 1232 kB/s, 20 objects/s
  client io 2626 kB/s wr, 0 op/s rd, 656 op/s wr

2017-05-23 14:00:24,203 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:00:24,203 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 14:00:24,203 INFO TC39_shutdown_osd_on_single_node.py [line:72] stop osd.0 in cluster successfully
2017-05-23 14:00:24,203 INFO TC39_shutdown_osd_on_single_node.py [line:47] 
Now operate osd.1
2017-05-23 14:00:24,203 INFO TC39_shutdown_osd_on_single_node.py [line:49] Set the osd.1 pid for kill
2017-05-23 14:00:24,890 INFO node.py [line:166] osd.0  ---> processId 5113
2017-05-23 14:00:24,890 INFO node.py [line:166] osd.1  ---> processId 24811
2017-05-23 14:00:24,890 INFO node.py [line:166] osd.2  ---> processId 26054
2017-05-23 14:00:24,890 INFO node.py [line:166] osd.0  ---> processId 5113
2017-05-23 14:00:24,890 INFO node.py [line:166] osd.1  ---> processId 24811
2017-05-23 14:00:24,890 INFO node.py [line:166] osd.2  ---> processId 26054
2017-05-23 14:00:24,890 INFO TC39_shutdown_osd_on_single_node.py [line:51] shutdown osd.1 by kill
2017-05-23 14:00:24,890 INFO osd.py [line:51] execute command is sudo -i kill 24811 & sleep 3
2017-05-23 14:00:28,953 INFO client.py [line:159] home/denali

2017-05-23 14:00:29,578 INFO client.py [line:159] home/denali

2017-05-23 14:00:29,904 INFO TC39_shutdown_osd_on_single_node.py [line:56] start osd.1
2017-05-23 14:00:29,904 INFO osd.py [line:100] node is  denali01
2017-05-23 14:00:29,904 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-23 14:01:01,197 INFO osd.py [line:105] osd osd.1 is start successfully
2017-05-23 14:01:01,197 INFO osd.py [line:113] node is  denali01
2017-05-23 14:01:01,197 INFO osd.py [line:114] execute command is ps -ef | grep 'ceph-osd -i 1'
2017-05-23 14:01:02,789 INFO osd.py [line:116] enali   10924 10923  0 14:01 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 1'
denali   10926 10924  0 14:01 ?        00:00:00 grep ceph-osd -i 1

2017-05-23 14:01:02,789 INFO osd.py [line:121] osd.1is not started, start again
2017-05-23 14:01:02,789 INFO osd.py [line:100] node is  denali01
2017-05-23 14:01:02,789 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-23 14:01:33,446 INFO osd.py [line:105] osd osd.1 is start successfully
2017-05-23 14:01:33,446 INFO osd.py [line:113] node is  denali01
2017-05-23 14:01:33,446 INFO osd.py [line:114] execute command is ps -ef | grep 'ceph-osd -i 1'
2017-05-23 14:01:34,131 INFO osd.py [line:116] oot     10955     1 37 14:01 ?        00:00:11 ceph-osd -i 1
denali   11952 11947  0 14:01 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 1'
denali   11954 11952  0 14:01 ?        00:00:00 grep ceph-osd -i 1

2017-05-23 14:01:34,131 INFO osd.py [line:125] osd.1is alrady started
2017-05-23 14:02:04,812 INFO client.py [line:159] home/denali

2017-05-23 14:02:06,796 INFO client.py [line:159] home/denali

2017-05-23 14:02:07,170 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:08,214 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            103 pgs degraded
            2 pgs recovering
            101 pgs recovery_wait
            recovery 5923/174886 objects degraded (3.387%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81201: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            5923/174886 objects degraded (3.387%)
                2913 active+clean
                 101 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 183 MB/s, 96 objects/s
  client io 4084 B/s rd, 1228 kB/s wr, 3 op/s rd, 307 op/s wr

2017-05-23 14:02:08,214 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:08,214 INFO cluster.py [line:231] usefull PG number is 2913
2017-05-23 14:02:08,214 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:09,493 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            101 pgs degraded
            3 pgs recovering
            98 pgs recovery_wait
            recovery 5579/174886 objects degraded (3.190%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81202: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            5579/174886 objects degraded (3.190%)
                2915 active+clean
                  98 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 250 MB/s, 227 objects/s
  client io 3087 kB/s wr, 0 op/s rd, 771 op/s wr

2017-05-23 14:02:09,493 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:09,493 INFO cluster.py [line:231] usefull PG number is 2915
2017-05-23 14:02:09,509 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:10,736 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            101 pgs degraded
            3 pgs recovering
            98 pgs recovery_wait
            recovery 5579/174886 objects degraded (3.190%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81202: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            5579/174886 objects degraded (3.190%)
                2915 active+clean
                  98 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 250 MB/s, 227 objects/s
  client io 3087 kB/s wr, 0 op/s rd, 771 op/s wr

2017-05-23 14:02:10,736 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:10,736 INFO cluster.py [line:231] usefull PG number is 2915
2017-05-23 14:02:10,736 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:11,828 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            101 pgs degraded
            3 pgs recovering
            98 pgs recovery_wait
            recovery 5579/174886 objects degraded (3.190%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81202: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            5579/174886 objects degraded (3.190%)
                2915 active+clean
                  98 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 250 MB/s, 227 objects/s
  client io 3087 kB/s wr, 0 op/s rd, 771 op/s wr

2017-05-23 14:02:11,828 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:11,828 INFO cluster.py [line:231] usefull PG number is 2915
2017-05-23 14:02:11,828 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:13,013 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            100 pgs degraded
            2 pgs recovering
            98 pgs recovery_wait
            recovery 5545/174886 objects degraded (3.171%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81203: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            5545/174886 objects degraded (3.171%)
                2916 active+clean
                  98 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 97162 kB/s, 110 objects/s
  client io 3547 B/s rd, 1934 kB/s wr, 3 op/s rd, 483 op/s wr

2017-05-23 14:02:13,013 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:13,013 INFO cluster.py [line:231] usefull PG number is 2916
2017-05-23 14:02:13,029 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:14,138 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            98 pgs degraded
            1 pgs recovering
            97 pgs recovery_wait
            recovery 5442/174886 objects degraded (3.112%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81204: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            5442/174886 objects degraded (3.112%)
                2918 active+clean
                  97 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 115 MB/s, 60 objects/s
  client io 3377 B/s rd, 1508 kB/s wr, 3 op/s rd, 377 op/s wr

2017-05-23 14:02:14,138 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:14,138 INFO cluster.py [line:231] usefull PG number is 2918
2017-05-23 14:02:14,138 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:16,030 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            95 pgs degraded
            1 pgs recovering
            94 pgs recovery_wait
            recovery 5213/174886 objects degraded (2.981%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81205: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            5213/174886 objects degraded (2.981%)
                2921 active+clean
                  94 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 164 MB/s, 172 objects/s
  client io 2322 kB/s wr, 0 op/s rd, 580 op/s wr

2017-05-23 14:02:16,030 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:16,030 INFO cluster.py [line:231] usefull PG number is 2921
2017-05-23 14:02:16,030 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:17,841 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            94 pgs degraded
            2 pgs recovering
            92 pgs recovery_wait
            recovery 5082/174886 objects degraded (2.906%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81206: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            5082/174886 objects degraded (2.906%)
                2922 active+clean
                  92 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 156 MB/s, 148 objects/s
  client io 4639 B/s rd, 1594 kB/s wr, 4 op/s rd, 398 op/s wr

2017-05-23 14:02:17,841 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:17,841 INFO cluster.py [line:231] usefull PG number is 2922
2017-05-23 14:02:17,841 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:19,398 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            93 pgs degraded
            3 pgs recovering
            90 pgs recovery_wait
            recovery 4873/174886 objects degraded (2.786%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81208: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4873/174886 objects degraded (2.786%)
                2923 active+clean
                  90 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 57007 kB/s, 111 objects/s
  client io 4405 B/s rd, 2780 kB/s wr, 4 op/s rd, 697 op/s wr

2017-05-23 14:02:19,398 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:19,398 INFO cluster.py [line:231] usefull PG number is 2923
2017-05-23 14:02:19,398 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:20,598 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            93 pgs degraded
            3 pgs recovering
            90 pgs recovery_wait
            recovery 4873/174886 objects degraded (2.786%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81208: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4873/174886 objects degraded (2.786%)
                2923 active+clean
                  90 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 57007 kB/s, 111 objects/s
  client io 4405 B/s rd, 2780 kB/s wr, 4 op/s rd, 697 op/s wr

2017-05-23 14:02:20,598 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:20,598 INFO cluster.py [line:231] usefull PG number is 2923
2017-05-23 14:02:20,598 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:22,190 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            93 pgs degraded
            3 pgs recovering
            90 pgs recovery_wait
            recovery 4873/174886 objects degraded (2.786%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81208: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4873/174886 objects degraded (2.786%)
                2923 active+clean
                  90 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 57007 kB/s, 111 objects/s
  client io 4405 B/s rd, 2780 kB/s wr, 4 op/s rd, 697 op/s wr

2017-05-23 14:02:22,190 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:22,190 INFO cluster.py [line:231] usefull PG number is 2923
2017-05-23 14:02:22,207 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:24,032 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            90 pgs degraded
            1 pgs recovering
            89 pgs recovery_wait
            recovery 4724/174886 objects degraded (2.701%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81210: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4724/174886 objects degraded (2.701%)
                2926 active+clean
                  89 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 137 MB/s, 72 objects/s
  client io 55684 B/s rd, 1296 kB/s wr, 54 op/s rd, 357 op/s wr

2017-05-23 14:02:24,032 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:24,032 INFO cluster.py [line:231] usefull PG number is 2926
2017-05-23 14:02:24,046 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:25,811 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            88 pgs degraded
            2 pgs recovering
            86 pgs recovery_wait
            recovery 4478/174886 objects degraded (2.561%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81211: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4478/174886 objects degraded (2.561%)
                2928 active+clean
                  86 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 149 MB/s, 145 objects/s
  client io 94525 B/s rd, 2086 kB/s wr, 92 op/s rd, 583 op/s wr

2017-05-23 14:02:25,811 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:25,811 INFO cluster.py [line:231] usefull PG number is 2928
2017-05-23 14:02:25,826 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:27,028 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            87 pgs degraded
            3 pgs recovering
            84 pgs recovery_wait
            recovery 4393/174886 objects degraded (2.512%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81212: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4393/174886 objects degraded (2.512%)
                2929 active+clean
                  84 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 133 MB/s, 120 objects/s
  client io 47433 B/s rd, 1691 kB/s wr, 46 op/s rd, 451 op/s wr

2017-05-23 14:02:27,028 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:27,028 INFO cluster.py [line:231] usefull PG number is 2929
2017-05-23 14:02:27,028 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:28,059 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            87 pgs degraded
            3 pgs recovering
            84 pgs recovery_wait
            recovery 4390/174886 objects degraded (2.510%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81213: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4390/174886 objects degraded (2.510%)
                2929 active+clean
                  84 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 107 MB/s, 57 objects/s
  client io 4990 B/s rd, 1031 kB/s wr, 4 op/s rd, 257 op/s wr

2017-05-23 14:02:28,059 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:28,059 INFO cluster.py [line:231] usefull PG number is 2929
2017-05-23 14:02:28,059 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:28,776 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            87 pgs degraded
            3 pgs recovering
            84 pgs recovery_wait
            recovery 4390/174886 objects degraded (2.510%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81213: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4390/174886 objects degraded (2.510%)
                2929 active+clean
                  84 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 107 MB/s, 57 objects/s
  client io 4990 B/s rd, 1031 kB/s wr, 4 op/s rd, 257 op/s wr

2017-05-23 14:02:28,776 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:28,776 INFO cluster.py [line:231] usefull PG number is 2929
2017-05-23 14:02:28,776 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:29,572 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            87 pgs degraded
            3 pgs recovering
            84 pgs recovery_wait
            recovery 4390/174886 objects degraded (2.510%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81213: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4390/174886 objects degraded (2.510%)
                2929 active+clean
                  84 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 107 MB/s, 57 objects/s
  client io 4990 B/s rd, 1031 kB/s wr, 4 op/s rd, 257 op/s wr

2017-05-23 14:02:29,572 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:29,572 INFO cluster.py [line:231] usefull PG number is 2929
2017-05-23 14:02:29,572 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:31,117 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            85 pgs degraded
            2 pgs recovering
            83 pgs recovery_wait
            recovery 4318/174886 objects degraded (2.469%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81214: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4318/174886 objects degraded (2.469%)
                2931 active+clean
                  83 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 76744 kB/s, 39 objects/s
  client io 984 kB/s wr, 0 op/s rd, 246 op/s wr

2017-05-23 14:02:31,117 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:31,117 INFO cluster.py [line:231] usefull PG number is 2931
2017-05-23 14:02:31,117 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:33,066 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            82 pgs degraded
            2 pgs recovering
            80 pgs recovery_wait
            recovery 4117/174886 objects degraded (2.354%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81215: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4117/174886 objects degraded (2.354%)
                2934 active+clean
                  80 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 83094 kB/s, 91 objects/s
  client io 4000 B/s rd, 1397 kB/s wr, 3 op/s rd, 349 op/s wr

2017-05-23 14:02:33,066 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:33,066 INFO cluster.py [line:231] usefull PG number is 2934
2017-05-23 14:02:33,066 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:34,174 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            80 pgs degraded
            1 pgs recovering
            79 pgs recovery_wait
            recovery 4011/174886 objects degraded (2.293%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81216: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            4011/174886 objects degraded (2.293%)
                2936 active+clean
                  79 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 144 MB/s, 137 objects/s
  client io 5021 B/s rd, 1556 kB/s wr, 4 op/s rd, 389 op/s wr

2017-05-23 14:02:34,174 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:34,174 INFO cluster.py [line:231] usefull PG number is 2936
2017-05-23 14:02:34,174 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:35,391 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            77 pgs degraded
            1 pgs recovering
            76 pgs recovery_wait
            recovery 3807/174886 objects degraded (2.177%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81217: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            3807/174886 objects degraded (2.177%)
                2939 active+clean
                  76 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 137 MB/s, 135 objects/s
  client io 1213 kB/s wr, 0 op/s rd, 303 op/s wr

2017-05-23 14:02:35,391 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:35,391 INFO cluster.py [line:231] usefull PG number is 2939
2017-05-23 14:02:35,391 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:36,236 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            77 pgs degraded
            1 pgs recovering
            76 pgs recovery_wait
            recovery 3807/174886 objects degraded (2.177%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81217: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            3807/174886 objects degraded (2.177%)
                2939 active+clean
                  76 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 137 MB/s, 135 objects/s
  client io 1213 kB/s wr, 0 op/s rd, 303 op/s wr

2017-05-23 14:02:36,236 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:36,236 INFO cluster.py [line:231] usefull PG number is 2939
2017-05-23 14:02:36,236 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:37,187 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            77 pgs degraded
            1 pgs recovering
            76 pgs recovery_wait
            recovery 3807/174886 objects degraded (2.177%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81217: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            3807/174886 objects degraded (2.177%)
                2939 active+clean
                  76 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 137 MB/s, 135 objects/s
  client io 1213 kB/s wr, 0 op/s rd, 303 op/s wr

2017-05-23 14:02:37,187 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:37,187 INFO cluster.py [line:231] usefull PG number is 2939
2017-05-23 14:02:37,187 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:38,404 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            75 pgs degraded
            1 pgs recovering
            74 pgs recovery_wait
            recovery 3685/174886 objects degraded (2.107%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81219: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            3685/174886 objects degraded (2.107%)
                2941 active+clean
                  74 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 133 MB/s, 69 objects/s
  client io 4399 B/s rd, 1186 kB/s wr, 4 op/s rd, 296 op/s wr

2017-05-23 14:02:38,404 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:38,404 INFO cluster.py [line:231] usefull PG number is 2941
2017-05-23 14:02:38,404 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:39,464 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            73 pgs degraded
            1 pgs recovering
            72 pgs recovery_wait
            recovery 3503/174886 objects degraded (2.003%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81220: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47413 MB used, 292 GB / 338 GB avail
            3503/174886 objects degraded (2.003%)
                2943 active+clean
                  72 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 96762 kB/s, 129 objects/s
  client io 2067 kB/s wr, 0 op/s rd, 516 op/s wr

2017-05-23 14:02:39,464 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:39,464 INFO cluster.py [line:231] usefull PG number is 2943
2017-05-23 14:02:39,464 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:40,621 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            73 pgs degraded
            1 pgs recovering
            72 pgs recovery_wait
            recovery 3503/174886 objects degraded (2.003%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81220: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47413 MB used, 292 GB / 338 GB avail
            3503/174886 objects degraded (2.003%)
                2943 active+clean
                  72 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 96762 kB/s, 129 objects/s
  client io 2067 kB/s wr, 0 op/s rd, 516 op/s wr

2017-05-23 14:02:40,621 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:40,621 INFO cluster.py [line:231] usefull PG number is 2943
2017-05-23 14:02:40,621 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:42,153 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            72 pgs degraded
            2 pgs recovering
            70 pgs recovery_wait
            recovery 3433/174886 objects degraded (1.963%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81221: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47413 MB used, 292 GB / 338 GB avail
            3433/174886 objects degraded (1.963%)
                2944 active+clean
                  70 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 91950 kB/s, 105 objects/s
  client io 5003 B/s rd, 1712 kB/s wr, 4 op/s rd, 428 op/s wr

2017-05-23 14:02:42,153 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:42,153 INFO cluster.py [line:231] usefull PG number is 2944
2017-05-23 14:02:42,153 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:43,542 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            72 pgs degraded
            2 pgs recovering
            70 pgs recovery_wait
            recovery 3433/174886 objects degraded (1.963%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81222: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47413 MB used, 292 GB / 338 GB avail
            3433/174886 objects degraded (1.963%)
                2944 active+clean
                  70 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 83541 kB/s, 43 objects/s
  client io 4775 B/s rd, 973 kB/s wr, 4 op/s rd, 243 op/s wr

2017-05-23 14:02:43,542 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:43,542 INFO cluster.py [line:231] usefull PG number is 2944
2017-05-23 14:02:43,542 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:44,680 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            71 pgs degraded
            2 pgs recovering
            69 pgs recovery_wait
            recovery 3371/174886 objects degraded (1.928%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81223: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47413 MB used, 292 GB / 338 GB avail
            3371/174886 objects degraded (1.928%)
                2945 active+clean
                  69 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 97276 kB/s, 50 objects/s
  client io 1982 kB/s wr, 0 op/s rd, 495 op/s wr

2017-05-23 14:02:44,680 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:44,680 INFO cluster.py [line:231] usefull PG number is 2945
2017-05-23 14:02:44,680 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:45,803 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            69 pgs degraded
            2 pgs recovering
            67 pgs recovery_wait
            recovery 3202/174886 objects degraded (1.831%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81224: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            3202/174886 objects degraded (1.831%)
                2947 active+clean
                  67 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 104 MB/s, 124 objects/s
  client io 2576 kB/s wr, 0 op/s rd, 644 op/s wr

2017-05-23 14:02:45,803 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:45,803 INFO cluster.py [line:231] usefull PG number is 2947
2017-05-23 14:02:45,803 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:47,052 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            68 pgs degraded
            2 pgs recovering
            66 pgs recovery_wait
            recovery 3120/174886 objects degraded (1.784%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81225: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            3120/174886 objects degraded (1.784%)
                2948 active+clean
                  66 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 102 MB/s, 105 objects/s
  client io 4895 B/s rd, 1709 kB/s wr, 4 op/s rd, 427 op/s wr

2017-05-23 14:02:47,052 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:47,052 INFO cluster.py [line:231] usefull PG number is 2948
2017-05-23 14:02:47,052 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:47,846 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            68 pgs degraded
            2 pgs recovering
            66 pgs recovery_wait
            recovery 3120/174886 objects degraded (1.784%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81225: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            3120/174886 objects degraded (1.784%)
                2948 active+clean
                  66 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 102 MB/s, 105 objects/s
  client io 4895 B/s rd, 1709 kB/s wr, 4 op/s rd, 427 op/s wr

2017-05-23 14:02:47,846 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:47,846 INFO cluster.py [line:231] usefull PG number is 2948
2017-05-23 14:02:47,862 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:49,516 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            67 pgs degraded
            2 pgs recovering
            65 pgs recovery_wait
            recovery 3071/174886 objects degraded (1.756%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81226: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            3071/174886 objects degraded (1.756%)
                2949 active+clean
                  65 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 112 MB/s, 59 objects/s
  client io 3476 B/s rd, 1161 kB/s wr, 3 op/s rd, 290 op/s wr

2017-05-23 14:02:49,516 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:49,516 INFO cluster.py [line:231] usefull PG number is 2949
2017-05-23 14:02:49,516 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:50,828 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            65 pgs degraded
            2 pgs recovering
            63 pgs recovery_wait
            recovery 2902/174886 objects degraded (1.659%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81227: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2902/174886 objects degraded (1.659%)
                2951 active+clean
                  63 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 60136 kB/s, 79 objects/s
  client io 1761 kB/s wr, 0 op/s rd, 440 op/s wr

2017-05-23 14:02:50,828 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:50,828 INFO cluster.py [line:231] usefull PG number is 2951
2017-05-23 14:02:50,828 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:51,717 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            63 pgs degraded
            1 pgs recovering
            62 pgs recovery_wait
            recovery 2826/174886 objects degraded (1.616%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81228: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2826/174886 objects degraded (1.616%)
                2953 active+clean
                  62 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 144 MB/s, 153 objects/s
  client io 7343 B/s rd, 2620 kB/s wr, 7 op/s rd, 655 op/s wr

2017-05-23 14:02:51,717 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:51,717 INFO cluster.py [line:231] usefull PG number is 2953
2017-05-23 14:02:51,717 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:52,934 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            63 pgs degraded
            1 pgs recovering
            62 pgs recovery_wait
            recovery 2826/174886 objects degraded (1.616%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81229: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2826/174886 objects degraded (1.616%)
                2953 active+clean
                  62 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 138 MB/s, 72 objects/s
  client io 7315 B/s rd, 1053 kB/s wr, 7 op/s rd, 263 op/s wr

2017-05-23 14:02:52,934 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:52,934 INFO cluster.py [line:231] usefull PG number is 2953
2017-05-23 14:02:52,934 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:54,091 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            63 pgs degraded
            2 pgs recovering
            61 pgs recovery_wait
            recovery 2767/174886 objects degraded (1.582%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81230: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2767/174886 objects degraded (1.582%)
                2953 active+clean
                  61 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 107 MB/s, 56 objects/s
  client io 1438 kB/s wr, 0 op/s rd, 359 op/s wr

2017-05-23 14:02:54,091 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:54,091 INFO cluster.py [line:231] usefull PG number is 2953
2017-05-23 14:02:54,091 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:55,183 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            63 pgs degraded
            2 pgs recovering
            61 pgs recovery_wait
            recovery 2767/174886 objects degraded (1.582%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81230: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2767/174886 objects degraded (1.582%)
                2953 active+clean
                  61 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 107 MB/s, 56 objects/s
  client io 1438 kB/s wr, 0 op/s rd, 359 op/s wr

2017-05-23 14:02:55,183 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:55,183 INFO cluster.py [line:231] usefull PG number is 2953
2017-05-23 14:02:55,183 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:56,651 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            60 pgs degraded
            3 pgs recovering
            57 pgs recovery_wait
            recovery 2576/174886 objects degraded (1.473%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81232: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2576/174886 objects degraded (1.473%)
                2956 active+clean
                  57 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 12570 kB/s, 71 objects/s
  client io 5625 B/s rd, 1687 kB/s wr, 5 op/s rd, 421 op/s wr

2017-05-23 14:02:56,651 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:56,651 INFO cluster.py [line:231] usefull PG number is 2956
2017-05-23 14:02:56,651 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:02:58,381 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            58 pgs degraded
            3 pgs recovering
            55 pgs recovery_wait
            recovery 2497/174886 objects degraded (1.428%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81234: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2497/174886 objects degraded (1.428%)
                2958 active+clean
                  55 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 145 MB/s, 77 objects/s
  client io 1788 kB/s wr, 0 op/s rd, 447 op/s wr

2017-05-23 14:02:58,381 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:02:58,381 INFO cluster.py [line:231] usefull PG number is 2958
2017-05-23 14:02:58,381 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:00,223 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            56 pgs degraded
            3 pgs recovering
            53 pgs recovery_wait
            recovery 2356/174886 objects degraded (1.347%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81235: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2356/174886 objects degraded (1.347%)
                2960 active+clean
                  53 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 37996 kB/s, 84 objects/s
  client io 2653 kB/s wr, 0 op/s rd, 663 op/s wr

2017-05-23 14:03:00,223 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:00,223 INFO cluster.py [line:231] usefull PG number is 2960
2017-05-23 14:03:00,223 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:01,065 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            56 pgs degraded
            3 pgs recovering
            53 pgs recovery_wait
            recovery 2356/174886 objects degraded (1.347%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81235: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2356/174886 objects degraded (1.347%)
                2960 active+clean
                  53 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 37996 kB/s, 84 objects/s
  client io 2653 kB/s wr, 0 op/s rd, 663 op/s wr

2017-05-23 14:03:01,065 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:01,065 INFO cluster.py [line:231] usefull PG number is 2960
2017-05-23 14:03:01,065 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:02,878 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            55 pgs degraded
            3 pgs recovering
            52 pgs recovery_wait
            recovery 2272/174886 objects degraded (1.299%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81237: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2272/174886 objects degraded (1.299%)
                2961 active+clean
                  52 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 106 MB/s, 56 objects/s
  client io 5124 B/s rd, 1082 kB/s wr, 5 op/s rd, 270 op/s wr

2017-05-23 14:03:02,878 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:02,878 INFO cluster.py [line:231] usefull PG number is 2961
2017-05-23 14:03:02,894 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:04,033 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            54 pgs degraded
            3 pgs recovering
            51 pgs recovery_wait
            recovery 2207/174886 objects degraded (1.262%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81238: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2207/174886 objects degraded (1.262%)
                2962 active+clean
                  51 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 195 MB/s, 104 objects/s
  client io 2092 kB/s wr, 0 op/s rd, 523 op/s wr

2017-05-23 14:03:04,033 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:04,033 INFO cluster.py [line:231] usefull PG number is 2962
2017-05-23 14:03:04,033 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:05,516 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            50 pgs degraded
            3 pgs recovering
            47 pgs recovery_wait
            recovery 2023/174886 objects degraded (1.157%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81239: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2023/174886 objects degraded (1.157%)
                2966 active+clean
                  47 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 123 MB/s, 151 objects/s
  client io 3222 kB/s wr, 0 op/s rd, 805 op/s wr

2017-05-23 14:03:05,516 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:05,516 INFO cluster.py [line:231] usefull PG number is 2966
2017-05-23 14:03:05,516 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:06,391 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            50 pgs degraded
            3 pgs recovering
            47 pgs recovery_wait
            recovery 2023/174886 objects degraded (1.157%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81239: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            2023/174886 objects degraded (1.157%)
                2966 active+clean
                  47 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 123 MB/s, 151 objects/s
  client io 3222 kB/s wr, 0 op/s rd, 805 op/s wr

2017-05-23 14:03:06,391 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:06,391 INFO cluster.py [line:231] usefull PG number is 2966
2017-05-23 14:03:06,391 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:08,513 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            48 pgs degraded
            3 pgs recovering
            45 pgs recovery_wait
            recovery 1948/174886 objects degraded (1.114%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81240: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1948/174886 objects degraded (1.114%)
                2968 active+clean
                  45 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 97963 kB/s, 107 objects/s
  client io 4960 B/s rd, 2485 kB/s wr, 4 op/s rd, 621 op/s wr

2017-05-23 14:03:08,513 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:08,513 INFO cluster.py [line:231] usefull PG number is 2968
2017-05-23 14:03:08,513 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:09,605 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            43 pgs degraded
            2 pgs recovering
            41 pgs recovery_wait
            recovery 1754/174886 objects degraded (1.003%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81241: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1754/174886 objects degraded (1.003%)
                2973 active+clean
                  41 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 102 MB/s, 82 objects/s
  client io 3203 B/s rd, 2037 kB/s wr, 3 op/s rd, 509 op/s wr

2017-05-23 14:03:09,605 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:09,605 INFO cluster.py [line:231] usefull PG number is 2973
2017-05-23 14:03:09,605 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:10,901 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            43 pgs degraded
            2 pgs recovering
            41 pgs recovery_wait
            recovery 1754/174886 objects degraded (1.003%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81241: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1754/174886 objects degraded (1.003%)
                2973 active+clean
                  41 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 102 MB/s, 82 objects/s
  client io 3203 B/s rd, 2037 kB/s wr, 3 op/s rd, 509 op/s wr

2017-05-23 14:03:10,901 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:10,901 INFO cluster.py [line:231] usefull PG number is 2973
2017-05-23 14:03:10,901 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:11,917 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            43 pgs degraded
            2 pgs recovering
            41 pgs recovery_wait
            recovery 1754/174886 objects degraded (1.003%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81242: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1754/174886 objects degraded (1.003%)
                2973 active+clean
                  41 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 44028 kB/s, 50 objects/s
  client io 3170 B/s rd, 1569 kB/s wr, 3 op/s rd, 392 op/s wr

2017-05-23 14:03:11,917 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:11,917 INFO cluster.py [line:231] usefull PG number is 2973
2017-05-23 14:03:11,917 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:13,210 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            42 pgs degraded
            2 pgs recovering
            40 pgs recovery_wait
            recovery 1696/174886 objects degraded (0.970%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81243: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1696/174886 objects degraded (0.970%)
                2974 active+clean
                  40 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 68805 kB/s, 35 objects/s
  client io 4695 B/s rd, 791 kB/s wr, 4 op/s rd, 197 op/s wr

2017-05-23 14:03:13,210 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:13,210 INFO cluster.py [line:231] usefull PG number is 2974
2017-05-23 14:03:13,226 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:14,459 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            41 pgs degraded
            3 pgs recovering
            38 pgs recovery_wait
            recovery 1636/174886 objects degraded (0.935%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81244: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1636/174886 objects degraded (0.935%)
                2975 active+clean
                  38 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 198 MB/s, 104 objects/s
  client io 2170 kB/s wr, 0 op/s rd, 542 op/s wr

2017-05-23 14:03:14,459 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:14,459 INFO cluster.py [line:231] usefull PG number is 2975
2017-05-23 14:03:14,459 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:15,769 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            38 pgs degraded
            3 pgs recovering
            35 pgs recovery_wait
            recovery 1442/174886 objects degraded (0.825%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81245: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1442/174886 objects degraded (0.825%)
                2978 active+clean
                  35 active+recovery_wait+degraded
                   3 active+recovering+degraded
recovery io 113 MB/s, 147 objects/s
  client io 3455 kB/s wr, 0 op/s rd, 863 op/s wr

2017-05-23 14:03:15,769 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:15,769 INFO cluster.py [line:231] usefull PG number is 2978
2017-05-23 14:03:15,769 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:17,551 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            35 pgs degraded
            2 pgs recovering
            33 pgs recovery_wait
            recovery 1334/174886 objects degraded (0.763%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81246: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1334/174886 objects degraded (0.763%)
                2981 active+clean
                  33 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 135 MB/s, 131 objects/s
  client io 4935 B/s rd, 2254 kB/s wr, 4 op/s rd, 563 op/s wr

2017-05-23 14:03:17,551 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:17,551 INFO cluster.py [line:231] usefull PG number is 2981
2017-05-23 14:03:17,551 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:20,125 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            35 pgs degraded
            2 pgs recovering
            33 pgs recovery_wait
            recovery 1334/174886 objects degraded (0.763%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81247: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1334/174886 objects degraded (0.763%)
                2981 active+clean
                  33 active+recovery_wait+degraded
                   2 active+recovering+degraded
recovery io 87377 kB/s, 44 objects/s
  client io 3198 B/s rd, 1097 kB/s wr, 3 op/s rd, 274 op/s wr

2017-05-23 14:03:20,125 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:20,125 INFO cluster.py [line:231] usefull PG number is 2981
2017-05-23 14:03:20,125 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:21,747 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            29 pgs degraded
            1 pgs recovering
            28 pgs recovery_wait
            recovery 1120/174886 objects degraded (0.640%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81249: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1120/174886 objects degraded (0.640%)
                2987 active+clean
                  28 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 170 MB/s, 144 objects/s
  client io 7185 B/s rd, 3390 kB/s wr, 7 op/s rd, 847 op/s wr

2017-05-23 14:03:21,747 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:21,747 INFO cluster.py [line:231] usefull PG number is 2987
2017-05-23 14:03:21,747 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:23,713 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            29 pgs degraded
            1 pgs recovering
            28 pgs recovery_wait
            recovery 1120/174886 objects degraded (0.640%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81250: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            1120/174886 objects degraded (0.640%)
                2987 active+clean
                  28 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 142 MB/s, 76 objects/s
  client io 7306 B/s rd, 1546 kB/s wr, 7 op/s rd, 386 op/s wr

2017-05-23 14:03:23,713 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:23,713 INFO cluster.py [line:231] usefull PG number is 2987
2017-05-23 14:03:23,713 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:25,039 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            24 pgs degraded
            1 pgs recovering
            23 pgs recovery_wait
            recovery 906/174886 objects degraded (0.518%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81252: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            906/174886 objects degraded (0.518%)
                2992 active+clean
                  23 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 6524 kB/s, 103 objects/s
  client io 2981 kB/s wr, 0 op/s rd, 745 op/s wr

2017-05-23 14:03:25,039 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:25,039 INFO cluster.py [line:231] usefull PG number is 2992
2017-05-23 14:03:25,039 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:26,349 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            24 pgs degraded
            1 pgs recovering
            23 pgs recovery_wait
            recovery 906/174886 objects degraded (0.518%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81252: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47412 MB used, 292 GB / 338 GB avail
            906/174886 objects degraded (0.518%)
                2992 active+clean
                  23 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 6524 kB/s, 103 objects/s
  client io 2981 kB/s wr, 0 op/s rd, 745 op/s wr

2017-05-23 14:03:26,349 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:26,349 INFO cluster.py [line:231] usefull PG number is 2992
2017-05-23 14:03:26,349 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:27,519 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            24 pgs degraded
            1 pgs recovering
            23 pgs recovery_wait
            recovery 906/174886 objects degraded (0.518%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81253: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47413 MB used, 292 GB / 338 GB avail
            906/174886 objects degraded (0.518%)
                2992 active+clean
                  23 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 4395 kB/s, 69 objects/s
  client io 4963 B/s rd, 2479 kB/s wr, 4 op/s rd, 619 op/s wr

2017-05-23 14:03:27,519 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:27,519 INFO cluster.py [line:231] usefull PG number is 2992
2017-05-23 14:03:27,519 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:28,911 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            24 pgs degraded
            1 pgs recovering
            23 pgs recovery_wait
            recovery 906/174886 objects degraded (0.518%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81255: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47414 MB used, 292 GB / 338 GB avail
            906/174886 objects degraded (0.518%)
                2992 active+clean
                  23 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 3662 kB/s wr, 0 op/s rd, 915 op/s wr

2017-05-23 14:03:28,911 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:28,911 INFO cluster.py [line:231] usefull PG number is 2992
2017-05-23 14:03:28,911 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:30,035 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            19 pgs degraded
            1 pgs recovering
            18 pgs recovery_wait
            recovery 683/174886 objects degraded (0.391%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81256: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47414 MB used, 292 GB / 338 GB avail
            683/174886 objects degraded (0.391%)
                2997 active+clean
                  18 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 6875 kB/s, 106 objects/s
  client io 3839 kB/s wr, 0 op/s rd, 959 op/s wr

2017-05-23 14:03:30,035 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:30,035 INFO cluster.py [line:231] usefull PG number is 2997
2017-05-23 14:03:30,035 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:31,548 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            19 pgs degraded
            1 pgs recovering
            18 pgs recovery_wait
            recovery 683/174886 objects degraded (0.391%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81256: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47414 MB used, 292 GB / 338 GB avail
            683/174886 objects degraded (0.391%)
                2997 active+clean
                  18 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 6875 kB/s, 106 objects/s
  client io 3839 kB/s wr, 0 op/s rd, 959 op/s wr

2017-05-23 14:03:31,548 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:31,548 INFO cluster.py [line:231] usefull PG number is 2997
2017-05-23 14:03:31,563 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:32,661 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            19 pgs degraded
            1 pgs recovering
            18 pgs recovery_wait
            recovery 683/174886 objects degraded (0.391%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81257: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47415 MB used, 292 GB / 338 GB avail
            683/174886 objects degraded (0.391%)
                2997 active+clean
                  18 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 4629 kB/s, 71 objects/s
  client io 4921 B/s rd, 2614 kB/s wr, 4 op/s rd, 653 op/s wr

2017-05-23 14:03:32,661 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:32,661 INFO cluster.py [line:231] usefull PG number is 2997
2017-05-23 14:03:32,661 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:34,003 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            19 pgs degraded
            1 pgs recovering
            18 pgs recovery_wait
            recovery 683/174886 objects degraded (0.391%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81259: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47417 MB used, 292 GB / 338 GB avail
            683/174886 objects degraded (0.391%)
                2997 active+clean
                  18 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 2410 kB/s wr, 0 op/s rd, 602 op/s wr

2017-05-23 14:03:34,003 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:34,003 INFO cluster.py [line:231] usefull PG number is 2997
2017-05-23 14:03:34,003 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:34,940 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            19 pgs degraded
            1 pgs recovering
            18 pgs recovery_wait
            recovery 683/174886 objects degraded (0.391%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81259: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47417 MB used, 292 GB / 338 GB avail
            683/174886 objects degraded (0.391%)
                2997 active+clean
                  18 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 2410 kB/s wr, 0 op/s rd, 602 op/s wr

2017-05-23 14:03:34,940 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:34,940 INFO cluster.py [line:231] usefull PG number is 2997
2017-05-23 14:03:34,940 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:35,969 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            13 pgs degraded
            1 pgs recovering
            12 pgs recovery_wait
            recovery 436/174886 objects degraded (0.249%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81260: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47417 MB used, 292 GB / 338 GB avail
            436/174886 objects degraded (0.249%)
                3003 active+clean
                  12 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 7270 kB/s, 119 objects/s
  client io 3270 kB/s wr, 0 op/s rd, 817 op/s wr

2017-05-23 14:03:35,969 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:35,969 INFO cluster.py [line:231] usefull PG number is 3003
2017-05-23 14:03:35,969 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:36,951 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            13 pgs degraded
            1 pgs recovering
            12 pgs recovery_wait
            recovery 436/174886 objects degraded (0.249%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81260: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47417 MB used, 292 GB / 338 GB avail
            436/174886 objects degraded (0.249%)
                3003 active+clean
                  12 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 7270 kB/s, 119 objects/s
  client io 3270 kB/s wr, 0 op/s rd, 817 op/s wr

2017-05-23 14:03:36,967 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:36,967 INFO cluster.py [line:231] usefull PG number is 3003
2017-05-23 14:03:36,967 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:38,075 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            13 pgs degraded
            1 pgs recovering
            12 pgs recovery_wait
            recovery 436/174886 objects degraded (0.249%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81261: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47418 MB used, 292 GB / 338 GB avail
            436/174886 objects degraded (0.249%)
                3003 active+clean
                  12 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 4884 kB/s, 80 objects/s
  client io 4994 B/s rd, 2009 kB/s wr, 4 op/s rd, 502 op/s wr

2017-05-23 14:03:38,075 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:38,075 INFO cluster.py [line:231] usefull PG number is 3003
2017-05-23 14:03:38,075 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:39,467 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            13 pgs degraded
            1 pgs recovering
            12 pgs recovery_wait
            recovery 436/174886 objects degraded (0.249%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81263: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47419 MB used, 292 GB / 338 GB avail
            436/174886 objects degraded (0.249%)
                3003 active+clean
                  12 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 2042 kB/s wr, 0 op/s rd, 510 op/s wr

2017-05-23 14:03:39,467 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:39,467 INFO cluster.py [line:231] usefull PG number is 3003
2017-05-23 14:03:39,467 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:40,683 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            5 pgs degraded
            1 pgs recovering
            4 pgs recovery_wait
            recovery 169/174886 objects degraded (0.097%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81264: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47420 MB used, 292 GB / 338 GB avail
            169/174886 objects degraded (0.097%)
                3011 active+clean
                   4 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 6793 kB/s, 120 objects/s
  client io 1770 kB/s wr, 0 op/s rd, 442 op/s wr

2017-05-23 14:03:40,683 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:40,683 INFO cluster.py [line:231] usefull PG number is 3011
2017-05-23 14:03:40,683 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:41,930 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            5 pgs degraded
            1 pgs recovering
            4 pgs recovery_wait
            recovery 169/174886 objects degraded (0.097%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81265: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47420 MB used, 292 GB / 338 GB avail
            169/174886 objects degraded (0.097%)
                3011 active+clean
                   4 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 6837 kB/s, 121 objects/s
  client io 6976 B/s rd, 1141 kB/s wr, 6 op/s rd, 285 op/s wr

2017-05-23 14:03:41,930 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:41,930 INFO cluster.py [line:231] usefull PG number is 3011
2017-05-23 14:03:41,930 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:43,121 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            5 pgs degraded
            1 pgs recovering
            4 pgs recovery_wait
            recovery 169/174886 objects degraded (0.097%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81266: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47421 MB used, 292 GB / 338 GB avail
            169/174886 objects degraded (0.097%)
                3011 active+clean
                   4 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 7449 B/s rd, 1680 kB/s wr, 7 op/s rd, 420 op/s wr

2017-05-23 14:03:43,121 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:43,121 INFO cluster.py [line:231] usefull PG number is 3011
2017-05-23 14:03:43,121 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:44,167 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            5 pgs degraded
            1 pgs recovering
            4 pgs recovery_wait
            recovery 169/174886 objects degraded (0.097%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81267: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47422 MB used, 292 GB / 338 GB avail
            169/174886 objects degraded (0.097%)
                3011 active+clean
                   4 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 1958 kB/s wr, 0 op/s rd, 489 op/s wr

2017-05-23 14:03:44,167 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:44,167 INFO cluster.py [line:231] usefull PG number is 3011
2017-05-23 14:03:44,167 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:03:45,118 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e282: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81268: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47423 MB used, 292 GB / 338 GB avail
                3016 active+clean
recovery io 4668 kB/s, 80 objects/s
  client io 2949 kB/s wr, 0 op/s rd, 737 op/s wr

2017-05-23 14:03:45,118 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:03:45,118 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 14:03:45,118 INFO TC39_shutdown_osd_on_single_node.py [line:72] stop osd.1 in cluster successfully
2017-05-23 14:03:45,118 INFO TC39_shutdown_osd_on_single_node.py [line:47] 
Now operate osd.2
2017-05-23 14:03:45,134 INFO TC39_shutdown_osd_on_single_node.py [line:49] Set the osd.2 pid for kill
2017-05-23 14:03:46,372 INFO node.py [line:166] osd.0  ---> processId 5113
2017-05-23 14:03:46,372 INFO node.py [line:166] osd.1  ---> processId 10955
2017-05-23 14:03:46,372 INFO node.py [line:166] osd.2  ---> processId 26054
2017-05-23 14:03:46,372 INFO node.py [line:166] osd.0  ---> processId 5113
2017-05-23 14:03:46,372 INFO node.py [line:166] osd.1  ---> processId 10955
2017-05-23 14:03:46,372 INFO node.py [line:166] osd.2  ---> processId 26054
2017-05-23 14:03:46,372 INFO TC39_shutdown_osd_on_single_node.py [line:51] shutdown osd.2 by kill
2017-05-23 14:03:46,388 INFO osd.py [line:51] execute command is sudo -i kill 26054 & sleep 3
2017-05-23 14:03:50,430 INFO client.py [line:159] home/denali

2017-05-23 14:03:51,101 INFO client.py [line:159] home/denali

2017-05-23 14:03:51,552 INFO TC39_shutdown_osd_on_single_node.py [line:56] start osd.2
2017-05-23 14:03:51,552 INFO osd.py [line:100] node is  denali01
2017-05-23 14:03:51,552 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-23 14:04:22,381 INFO osd.py [line:105] osd osd.2 is start successfully
2017-05-23 14:04:22,381 INFO osd.py [line:113] node is  denali01
2017-05-23 14:04:22,381 INFO osd.py [line:114] execute command is ps -ef | grep 'ceph-osd -i 2'
2017-05-23 14:04:23,378 INFO osd.py [line:116] oot     17457     1 35 14:03 ?        00:00:10 ceph-osd -i 2
denali   18416 18415  0 14:04 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 2'
denali   18418 18416  0 14:04 ?        00:00:00 grep ceph-osd -i 2

2017-05-23 14:04:23,378 INFO osd.py [line:125] osd.2is alrady started
2017-05-23 14:04:53,855 INFO client.py [line:159] home/denali

2017-05-23 14:04:54,759 INFO client.py [line:159] home/denali

2017-05-23 14:04:55,914 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:04:57,272 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            2 pgs degraded
            2 pgs recovery_wait
            recovery 36/174888 objects degraded (0.021%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81325: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47462 MB used, 292 GB / 338 GB avail
            36/174888 objects degraded (0.021%)
                3013 active+clean
                   2 active+recovery_wait+degraded
                   1 active+clean+scrubbing+deep
recovery io 119 MB/s, 62 objects/s
  client io 2570 B/s rd, 1679 kB/s wr, 2 op/s rd, 419 op/s wr

2017-05-23 14:04:57,272 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:04:57,272 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:04:57,272 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:04:58,503 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            1 pgs degraded
            1 pgs recovering
            recovery 11/174888 objects degraded (0.006%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81327: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47463 MB used, 292 GB / 338 GB avail
            11/174888 objects degraded (0.006%)
                3015 active+clean
                   1 active+recovering+degraded
recovery io 44355 kB/s, 24 objects/s
  client io 617 kB/s wr, 0 op/s rd, 154 op/s wr

2017-05-23 14:04:58,503 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:04:58,503 INFO cluster.py [line:231] usefull PG number is 3015
2017-05-23 14:04:58,503 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:04:59,457 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            1 pgs degraded
            1 pgs recovering
            recovery 11/174888 objects degraded (0.006%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81327: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47463 MB used, 292 GB / 338 GB avail
            11/174888 objects degraded (0.006%)
                3015 active+clean
                   1 active+recovering+degraded
recovery io 44355 kB/s, 24 objects/s
  client io 617 kB/s wr, 0 op/s rd, 154 op/s wr

2017-05-23 14:04:59,457 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:04:59,457 INFO cluster.py [line:231] usefull PG number is 3015
2017-05-23 14:04:59,457 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:05:00,237 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            1 pgs degraded
            1 pgs recovering
            recovery 11/174888 objects degraded (0.006%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81328: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47464 MB used, 292 GB / 338 GB avail
            11/174888 objects degraded (0.006%)
                3015 active+clean
                   1 active+recovering+degraded
  client io 1819 kB/s wr, 0 op/s rd, 454 op/s wr

2017-05-23 14:05:00,237 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:05:00,237 INFO cluster.py [line:231] usefull PG number is 3015
2017-05-23 14:05:00,237 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:05:01,391 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            1 pgs degraded
            1 pgs recovering
            recovery 11/174888 objects degraded (0.006%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81328: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47464 MB used, 292 GB / 338 GB avail
            11/174888 objects degraded (0.006%)
                3015 active+clean
                   1 active+recovering+degraded
  client io 1819 kB/s wr, 0 op/s rd, 454 op/s wr

2017-05-23 14:05:01,391 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:05:01,391 INFO cluster.py [line:231] usefull PG number is 3015
2017-05-23 14:05:01,391 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:05:02,671 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81330: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47467 MB used, 292 GB / 338 GB avail
                3015 active+clean
                   1 active+clean+scrubbing
recovery io 13583 kB/s, 7 objects/s
  client io 9812 B/s rd, 1331 kB/s wr, 9 op/s rd, 332 op/s wr

2017-05-23 14:05:02,671 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:05:02,671 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:05:02,671 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:05:03,575 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81330: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47467 MB used, 292 GB / 338 GB avail
                3015 active+clean
                   1 active+clean+scrubbing
recovery io 13583 kB/s, 7 objects/s
  client io 9812 B/s rd, 1331 kB/s wr, 9 op/s rd, 332 op/s wr

2017-05-23 14:05:03,575 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:05:03,575 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:05:03,575 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:05:04,418 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81331: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47469 MB used, 292 GB / 338 GB avail
                3015 active+clean
                   1 active+clean+scrubbing
  client io 1813 kB/s wr, 0 op/s rd, 453 op/s wr

2017-05-23 14:05:04,418 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:05:04,418 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:05:04,418 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:05:05,246 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81332: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47471 MB used, 292 GB / 338 GB avail
                3015 active+clean
                   1 active+clean+scrubbing
  client io 2418 kB/s wr, 0 op/s rd, 604 op/s wr

2017-05-23 14:05:05,246 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:05:05,246 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:05:05,246 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:05:06,385 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81332: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47471 MB used, 292 GB / 338 GB avail
                3015 active+clean
                   1 active+clean+scrubbing
  client io 2418 kB/s wr, 0 op/s rd, 604 op/s wr

2017-05-23 14:05:06,385 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:05:06,385 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:05:06,385 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:05:07,572 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81333: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47473 MB used, 292 GB / 338 GB avail
                3015 active+clean
                   1 active+clean+scrubbing
  client io 5002 B/s rd, 1796 kB/s wr, 4 op/s rd, 449 op/s wr

2017-05-23 14:05:07,572 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:05:07,572 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:05:07,572 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:05:08,618 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e287: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81334: 3016 pgs, 13 pools, 311 GB data, 87444 objects
            47475 MB used, 292 GB / 338 GB avail
                3016 active+clean
  client io 4950 B/s rd, 1429 kB/s wr, 4 op/s rd, 357 op/s wr

2017-05-23 14:05:08,618 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:05:08,618 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 14:05:08,618 INFO TC39_shutdown_osd_on_single_node.py [line:72] stop osd.2 in cluster successfully
2017-05-23 14:05:08,618 INFO TC39_shutdown_osd_on_single_node.py [line:47] 
Now operate osd.0
2017-05-23 14:05:08,618 INFO TC39_shutdown_osd_on_single_node.py [line:49] Set the osd.0 pid for kill
2017-05-23 14:05:09,492 INFO node.py [line:166] osd.0  ---> processId 5113
2017-05-23 14:05:09,492 INFO node.py [line:166] osd.1  ---> processId 10955
2017-05-23 14:05:09,492 INFO node.py [line:166] osd.2  ---> processId 17457
2017-05-23 14:05:09,492 INFO node.py [line:166] osd.0  ---> processId 5113
2017-05-23 14:05:09,492 INFO node.py [line:166] osd.1  ---> processId 10955
2017-05-23 14:05:09,492 INFO node.py [line:166] osd.2  ---> processId 17457
2017-05-23 14:05:09,492 INFO TC39_shutdown_osd_on_single_node.py [line:51] shutdown osd.0 by kill
2017-05-23 14:05:09,506 INFO osd.py [line:51] execute command is sudo -i kill 5113 & sleep 3
2017-05-23 14:05:13,926 INFO client.py [line:159] home/denali

2017-05-23 14:05:14,752 INFO client.py [line:159] home/denali

2017-05-23 14:05:15,298 INFO TC39_shutdown_osd_on_single_node.py [line:56] start osd.0
2017-05-23 14:05:15,298 INFO osd.py [line:100] node is  denali01
2017-05-23 14:05:15,298 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-05-23 14:05:46,065 INFO osd.py [line:105] osd osd.0 is start successfully
2017-05-23 14:05:46,065 INFO osd.py [line:113] node is  denali01
2017-05-23 14:05:46,065 INFO osd.py [line:114] execute command is ps -ef | grep 'ceph-osd -i 0'
2017-05-23 14:05:46,750 INFO osd.py [line:116] oot     20132     1 33 14:05 ?        00:00:09 ceph-osd -i 0
denali   21037 21033  0 14:05 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 0'
denali   21039 21037  0 14:05 ?        00:00:00 grep ceph-osd -i 0

2017-05-23 14:05:46,750 INFO osd.py [line:125] osd.0is alrady started
2017-05-23 14:06:17,957 INFO client.py [line:159] home/denali

2017-05-23 14:06:18,846 INFO client.py [line:159] home/denali

2017-05-23 14:06:19,206 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:06:20,207 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            8 pgs degraded
            1 pgs recovering
            7 pgs recovery_wait
            recovery 150/174890 objects degraded (0.086%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e292: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81390: 3016 pgs, 13 pools, 312 GB data, 87445 objects
            47648 MB used, 292 GB / 338 GB avail
            150/174890 objects degraded (0.086%)
                3008 active+clean
                   7 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 2748 kB/s wr, 0 op/s rd, 687 op/s wr

2017-05-23 14:06:20,207 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:06:20,207 INFO cluster.py [line:231] usefull PG number is 3008
2017-05-23 14:06:20,207 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:06:21,331 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            8 pgs degraded
            1 pgs recovering
            7 pgs recovery_wait
            recovery 150/174890 objects degraded (0.086%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e292: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81390: 3016 pgs, 13 pools, 312 GB data, 87445 objects
            47648 MB used, 292 GB / 338 GB avail
            150/174890 objects degraded (0.086%)
                3008 active+clean
                   7 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 2748 kB/s wr, 0 op/s rd, 687 op/s wr

2017-05-23 14:06:21,331 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:06:21,331 INFO cluster.py [line:231] usefull PG number is 3008
2017-05-23 14:06:21,331 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:06:22,845 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e292: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81391: 3016 pgs, 13 pools, 312 GB data, 87445 objects
            47661 MB used, 292 GB / 338 GB avail
                3016 active+clean
recovery io 3141 kB/s, 44 objects/s
  client io 4588 B/s rd, 3097 kB/s wr, 4 op/s rd, 774 op/s wr

2017-05-23 14:06:22,845 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:06:22,845 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 14:06:22,845 INFO TC39_shutdown_osd_on_single_node.py [line:72] stop osd.0 in cluster successfully
2017-05-23 14:06:22,845 INFO TC39_shutdown_osd_on_single_node.py [line:47] 
Now operate osd.1
2017-05-23 14:06:22,861 INFO TC39_shutdown_osd_on_single_node.py [line:49] Set the osd.1 pid for kill
2017-05-23 14:06:23,796 INFO node.py [line:166] osd.0  ---> processId 20132
2017-05-23 14:06:23,796 INFO node.py [line:166] osd.1  ---> processId 10955
2017-05-23 14:06:23,796 INFO node.py [line:166] osd.2  ---> processId 17457
2017-05-23 14:06:23,796 INFO node.py [line:166] osd.0  ---> processId 20132
2017-05-23 14:06:23,796 INFO node.py [line:166] osd.1  ---> processId 10955
2017-05-23 14:06:23,796 INFO node.py [line:166] osd.2  ---> processId 17457
2017-05-23 14:06:23,796 INFO TC39_shutdown_osd_on_single_node.py [line:51] shutdown osd.1 by kill
2017-05-23 14:06:23,812 INFO osd.py [line:51] execute command is sudo -i kill 10955 & sleep 3
2017-05-23 14:06:28,230 INFO client.py [line:159] home/denali

2017-05-23 14:06:29,290 INFO client.py [line:159] home/denali

2017-05-23 14:06:29,914 INFO TC39_shutdown_osd_on_single_node.py [line:56] start osd.1
2017-05-23 14:06:29,914 INFO osd.py [line:100] node is  denali01
2017-05-23 14:06:29,914 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-05-23 14:07:00,513 INFO osd.py [line:105] osd osd.1 is start successfully
2017-05-23 14:07:00,513 INFO osd.py [line:113] node is  denali01
2017-05-23 14:07:00,513 INFO osd.py [line:114] execute command is ps -ef | grep 'ceph-osd -i 1'
2017-05-23 14:07:01,137 INFO osd.py [line:116] oot     22403     1 35 14:06 ?        00:00:11 ceph-osd -i 1
denali   23390 23389  0 14:07 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 1'
denali   23392 23390  0 14:07 ?        00:00:00 grep ceph-osd -i 1

2017-05-23 14:07:01,137 INFO osd.py [line:125] osd.1is alrady started
2017-05-23 14:07:33,463 INFO client.py [line:159] home/denali

2017-05-23 14:07:40,657 INFO client.py [line:159] home/denali

2017-05-23 14:07:45,368 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:07:48,062 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            53 pgs degraded
            2 pgs recovering
            51 pgs recovery_wait
            recovery 1247/174894 objects degraded (0.713%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81455: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47914 MB used, 292 GB / 338 GB avail
            1247/174894 objects degraded (0.713%)
                2962 active+clean
                  51 active+recovery_wait+degraded
                   2 active+recovering+degraded
                   1 active+clean+scrubbing+deep
recovery io 98627 kB/s, 136 objects/s
  client io 1483 kB/s wr, 0 op/s rd, 370 op/s wr

2017-05-23 14:07:48,062 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:07:48,062 INFO cluster.py [line:231] usefull PG number is 1
2017-05-23 14:07:48,062 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:07:49,403 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            45 pgs degraded
            1 pgs recovering
            44 pgs recovery_wait
            recovery 1017/174894 objects degraded (0.581%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81457: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47931 MB used, 292 GB / 338 GB avail
            1017/174894 objects degraded (0.581%)
                2971 active+clean
                  44 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 101154 kB/s, 73 objects/s
  client io 3212 B/s rd, 1927 kB/s wr, 3 op/s rd, 481 op/s wr

2017-05-23 14:07:49,403 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:07:49,403 INFO cluster.py [line:231] usefull PG number is 2971
2017-05-23 14:07:49,403 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:07:50,651 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            45 pgs degraded
            1 pgs recovering
            44 pgs recovery_wait
            recovery 1017/174894 objects degraded (0.581%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81458: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47933 MB used, 292 GB / 338 GB avail
            1017/174894 objects degraded (0.581%)
                2971 active+clean
                  44 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 92505 kB/s, 97 objects/s
  client io 1812 kB/s wr, 0 op/s rd, 453 op/s wr

2017-05-23 14:07:50,651 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:07:50,651 INFO cluster.py [line:231] usefull PG number is 2971
2017-05-23 14:07:50,651 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:07:51,855 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            45 pgs degraded
            1 pgs recovering
            44 pgs recovery_wait
            recovery 1017/174894 objects degraded (0.581%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81458: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47933 MB used, 292 GB / 338 GB avail
            1017/174894 objects degraded (0.581%)
                2971 active+clean
                  44 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 92505 kB/s, 97 objects/s
  client io 1812 kB/s wr, 0 op/s rd, 453 op/s wr

2017-05-23 14:07:51,855 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:07:51,855 INFO cluster.py [line:231] usefull PG number is 2971
2017-05-23 14:07:51,855 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:07:52,730 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            41 pgs degraded
            1 pgs recovering
            40 pgs recovery_wait
            recovery 925/174894 objects degraded (0.529%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81459: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47943 MB used, 292 GB / 338 GB avail
            925/174894 objects degraded (0.529%)
                2975 active+clean
                  40 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 114 MB/s, 59 objects/s
  client io 4998 B/s rd, 1559 kB/s wr, 4 op/s rd, 389 op/s wr

2017-05-23 14:07:52,730 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:07:52,730 INFO cluster.py [line:231] usefull PG number is 2975
2017-05-23 14:07:52,730 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:07:53,884 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            41 pgs degraded
            1 pgs recovering
            40 pgs recovery_wait
            recovery 925/174894 objects degraded (0.529%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81460: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47944 MB used, 292 GB / 338 GB avail
            925/174894 objects degraded (0.529%)
                2975 active+clean
                  40 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 112 MB/s, 58 objects/s
  client io 4912 B/s rd, 1660 kB/s wr, 4 op/s rd, 415 op/s wr

2017-05-23 14:07:53,884 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:07:53,884 INFO cluster.py [line:231] usefull PG number is 2975
2017-05-23 14:07:53,884 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:07:54,976 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            35 pgs degraded
            1 pgs recovering
            34 pgs recovery_wait
            recovery 764/174894 objects degraded (0.437%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81461: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47954 MB used, 292 GB / 338 GB avail
            764/174894 objects degraded (0.437%)
                2981 active+clean
                  34 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 6608 kB/s, 76 objects/s
  client io 2973 kB/s wr, 0 op/s rd, 743 op/s wr

2017-05-23 14:07:54,976 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:07:54,976 INFO cluster.py [line:231] usefull PG number is 2981
2017-05-23 14:07:54,976 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:07:56,256 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            35 pgs degraded
            1 pgs recovering
            34 pgs recovery_wait
            recovery 764/174894 objects degraded (0.437%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81461: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47954 MB used, 292 GB / 338 GB avail
            764/174894 objects degraded (0.437%)
                2981 active+clean
                  34 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 6608 kB/s, 76 objects/s
  client io 2973 kB/s wr, 0 op/s rd, 743 op/s wr

2017-05-23 14:07:56,256 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:07:56,256 INFO cluster.py [line:231] usefull PG number is 2981
2017-05-23 14:07:56,256 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:07:57,598 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            35 pgs degraded
            1 pgs recovering
            34 pgs recovery_wait
            recovery 764/174894 objects degraded (0.437%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81462: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47966 MB used, 292 GB / 338 GB avail
            764/174894 objects degraded (0.437%)
                2981 active+clean
                  34 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 3365 kB/s, 39 objects/s
  client io 3730 B/s rd, 2672 kB/s wr, 3 op/s rd, 668 op/s wr

2017-05-23 14:07:57,598 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:07:57,598 INFO cluster.py [line:231] usefull PG number is 2981
2017-05-23 14:07:57,598 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:07:58,877 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            35 pgs degraded
            1 pgs recovering
            34 pgs recovery_wait
            recovery 764/174894 objects degraded (0.437%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81463: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47971 MB used, 292 GB / 338 GB avail
            764/174894 objects degraded (0.437%)
                2981 active+clean
                  34 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 3686 B/s rd, 2024 kB/s wr, 3 op/s rd, 506 op/s wr

2017-05-23 14:07:58,877 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:07:58,877 INFO cluster.py [line:231] usefull PG number is 2981
2017-05-23 14:07:58,877 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:00,280 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            28 pgs degraded
            1 pgs recovering
            27 pgs recovery_wait
            recovery 587/174894 objects degraded (0.336%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81464: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47979 MB used, 292 GB / 338 GB avail
            587/174894 objects degraded (0.336%)
                2988 active+clean
                  27 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 7321 kB/s, 84 objects/s
  client io 3455 kB/s wr, 0 op/s rd, 863 op/s wr

2017-05-23 14:08:00,280 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:00,280 INFO cluster.py [line:231] usefull PG number is 2988
2017-05-23 14:08:00,280 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:01,529 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            28 pgs degraded
            1 pgs recovering
            27 pgs recovery_wait
            recovery 587/174894 objects degraded (0.336%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81464: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47979 MB used, 292 GB / 338 GB avail
            587/174894 objects degraded (0.336%)
                2988 active+clean
                  27 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 7321 kB/s, 84 objects/s
  client io 3455 kB/s wr, 0 op/s rd, 863 op/s wr

2017-05-23 14:08:01,529 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:01,529 INFO cluster.py [line:231] usefull PG number is 2988
2017-05-23 14:08:01,529 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:02,778 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            28 pgs degraded
            1 pgs recovering
            27 pgs recovery_wait
            recovery 587/174894 objects degraded (0.336%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81466: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47990 MB used, 291 GB / 338 GB avail
            587/174894 objects degraded (0.336%)
                2988 active+clean
                  27 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 5018 B/s rd, 2003 kB/s wr, 4 op/s rd, 500 op/s wr

2017-05-23 14:08:02,778 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:02,778 INFO cluster.py [line:231] usefull PG number is 2988
2017-05-23 14:08:02,778 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:03,621 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            28 pgs degraded
            1 pgs recovering
            27 pgs recovery_wait
            recovery 587/174894 objects degraded (0.336%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81466: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47990 MB used, 291 GB / 338 GB avail
            587/174894 objects degraded (0.336%)
                2988 active+clean
                  27 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 5018 B/s rd, 2003 kB/s wr, 4 op/s rd, 500 op/s wr

2017-05-23 14:08:03,621 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:03,621 INFO cluster.py [line:231] usefull PG number is 2988
2017-05-23 14:08:03,621 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:04,884 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            20 pgs degraded
            1 pgs recovering
            19 pgs recovery_wait
            recovery 421/174894 objects degraded (0.241%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81467: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            47998 MB used, 291 GB / 338 GB avail
            421/174894 objects degraded (0.241%)
                2996 active+clean
                  19 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 5844 kB/s, 78 objects/s
  client io 4124 kB/s wr, 0 op/s rd, 1031 op/s wr

2017-05-23 14:08:04,884 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:04,884 INFO cluster.py [line:231] usefull PG number is 2996
2017-05-23 14:08:04,900 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:05,914 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            20 pgs degraded
            1 pgs recovering
            19 pgs recovery_wait
            recovery 421/174894 objects degraded (0.241%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81468: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48004 MB used, 291 GB / 338 GB avail
            421/174894 objects degraded (0.241%)
                2996 active+clean
                  19 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 5286 kB/s, 71 objects/s
  client io 2778 kB/s wr, 0 op/s rd, 694 op/s wr

2017-05-23 14:08:05,914 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:05,914 INFO cluster.py [line:231] usefull PG number is 2996
2017-05-23 14:08:05,914 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:07,131 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            20 pgs degraded
            1 pgs recovering
            19 pgs recovery_wait
            recovery 421/174894 objects degraded (0.241%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81469: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48014 MB used, 291 GB / 338 GB avail
            421/174894 objects degraded (0.241%)
                2996 active+clean
                  19 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 4617 B/s rd, 1914 kB/s wr, 4 op/s rd, 478 op/s wr

2017-05-23 14:08:07,131 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:07,131 INFO cluster.py [line:231] usefull PG number is 2996
2017-05-23 14:08:07,131 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:08,410 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            20 pgs degraded
            1 pgs recovering
            19 pgs recovery_wait
            recovery 421/174894 objects degraded (0.241%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81470: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48021 MB used, 291 GB / 338 GB avail
            421/174894 objects degraded (0.241%)
                2996 active+clean
                  19 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 4880 B/s rd, 2056 kB/s wr, 4 op/s rd, 514 op/s wr

2017-05-23 14:08:08,410 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:08,410 INFO cluster.py [line:231] usefull PG number is 2996
2017-05-23 14:08:08,410 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:09,267 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            20 pgs degraded
            1 pgs recovering
            19 pgs recovery_wait
            recovery 421/174894 objects degraded (0.241%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81470: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48021 MB used, 291 GB / 338 GB avail
            421/174894 objects degraded (0.241%)
                2996 active+clean
                  19 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 4880 B/s rd, 2056 kB/s wr, 4 op/s rd, 514 op/s wr

2017-05-23 14:08:09,267 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:09,267 INFO cluster.py [line:231] usefull PG number is 2996
2017-05-23 14:08:09,267 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:13,073 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            20 pgs degraded
            1 pgs recovering
            19 pgs recovery_wait
            recovery 421/174894 objects degraded (0.241%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81470: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48021 MB used, 291 GB / 338 GB avail
            421/174894 objects degraded (0.241%)
                2996 active+clean
                  19 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 4880 B/s rd, 2056 kB/s wr, 4 op/s rd, 514 op/s wr

2017-05-23 14:08:13,073 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:13,073 INFO cluster.py [line:231] usefull PG number is 2996
2017-05-23 14:08:13,073 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:14,961 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            13 pgs degraded
            1 pgs recovering
            12 pgs recovery_wait
            recovery 241/174894 objects degraded (0.138%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81472: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48040 MB used, 291 GB / 338 GB avail
            241/174894 objects degraded (0.138%)
                3003 active+clean
                  12 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 2188 kB/s, 26 objects/s
  client io 2290 B/s rd, 1322 kB/s wr, 2 op/s rd, 330 op/s wr

2017-05-23 14:08:14,961 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:14,961 INFO cluster.py [line:231] usefull PG number is 3003
2017-05-23 14:08:14,961 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:16,023 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            13 pgs degraded
            1 pgs recovering
            12 pgs recovery_wait
            recovery 241/174894 objects degraded (0.138%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81473: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48043 MB used, 291 GB / 338 GB avail
            241/174894 objects degraded (0.138%)
                3003 active+clean
                  12 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 50758 B/s, 0 objects/s
  client io 7320 B/s rd, 2059 kB/s wr, 7 op/s rd, 514 op/s wr

2017-05-23 14:08:16,023 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:16,023 INFO cluster.py [line:231] usefull PG number is 3003
2017-05-23 14:08:16,023 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:17,305 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            13 pgs degraded
            1 pgs recovering
            12 pgs recovery_wait
            recovery 241/174894 objects degraded (0.138%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81474: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48047 MB used, 291 GB / 338 GB avail
            241/174894 objects degraded (0.138%)
                3003 active+clean
                  12 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 7399 B/s rd, 1269 kB/s wr, 7 op/s rd, 317 op/s wr

2017-05-23 14:08:17,305 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:17,305 INFO cluster.py [line:231] usefull PG number is 3003
2017-05-23 14:08:17,321 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:18,131 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            13 pgs degraded
            1 pgs recovering
            12 pgs recovery_wait
            recovery 241/174894 objects degraded (0.138%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81475: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48050 MB used, 291 GB / 338 GB avail
            241/174894 objects degraded (0.138%)
                3003 active+clean
                  12 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 7423 B/s rd, 2172 kB/s wr, 7 op/s rd, 543 op/s wr

2017-05-23 14:08:18,131 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:18,131 INFO cluster.py [line:231] usefull PG number is 3003
2017-05-23 14:08:18,131 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:19,286 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            6 pgs degraded
            1 pgs recovering
            5 pgs recovery_wait
            recovery 89/174894 objects degraded (0.051%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81476: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48055 MB used, 291 GB / 338 GB avail
            89/174894 objects degraded (0.051%)
                3010 active+clean
                   5 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 5298 kB/s, 73 objects/s
  client io 2365 kB/s wr, 0 op/s rd, 591 op/s wr

2017-05-23 14:08:19,286 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:19,286 INFO cluster.py [line:231] usefull PG number is 3010
2017-05-23 14:08:19,302 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:20,456 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            6 pgs degraded
            1 pgs recovering
            5 pgs recovery_wait
            recovery 89/174894 objects degraded (0.051%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81477: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48057 MB used, 291 GB / 338 GB avail
            89/174894 objects degraded (0.051%)
                3010 active+clean
                   5 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 5267 kB/s, 73 objects/s
  client io 2427 kB/s wr, 0 op/s rd, 606 op/s wr

2017-05-23 14:08:20,456 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:20,456 INFO cluster.py [line:231] usefull PG number is 3010
2017-05-23 14:08:20,456 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:21,595 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            6 pgs degraded
            1 pgs recovering
            5 pgs recovery_wait
            recovery 89/174894 objects degraded (0.051%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81477: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48057 MB used, 291 GB / 338 GB avail
            89/174894 objects degraded (0.051%)
                3010 active+clean
                   5 active+recovery_wait+degraded
                   1 active+recovering+degraded
recovery io 5267 kB/s, 73 objects/s
  client io 2427 kB/s wr, 0 op/s rd, 606 op/s wr

2017-05-23 14:08:21,595 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:21,595 INFO cluster.py [line:231] usefull PG number is 3010
2017-05-23 14:08:21,611 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:23,140 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            6 pgs degraded
            1 pgs recovering
            5 pgs recovery_wait
            recovery 89/174894 objects degraded (0.051%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81478: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48068 MB used, 291 GB / 338 GB avail
            89/174894 objects degraded (0.051%)
                3010 active+clean
                   5 active+recovery_wait+degraded
                   1 active+recovering+degraded
  client io 4721 B/s rd, 2315 kB/s wr, 4 op/s rd, 578 op/s wr

2017-05-23 14:08:23,140 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:23,140 INFO cluster.py [line:231] usefull PG number is 3010
2017-05-23 14:08:23,155 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 14:08:24,591 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e297: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v81480: 3016 pgs, 13 pools, 313 GB data, 87447 objects
            48076 MB used, 291 GB / 338 GB avail
                3016 active+clean
recovery io 2974 kB/s, 41 objects/s
  client io 2070 kB/s wr, 0 op/s rd, 517 op/s wr

2017-05-23 14:08:24,591 INFO cluster.py [line:230] PG number is 3016
2017-05-23 14:08:24,591 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 14:08:24,591 INFO TC39_shutdown_osd_on_single_node.py [line:72] stop osd.1 in cluster successfully
2017-05-23 14:08:24,607 INFO TC39_shutdown_osd_on_single_node.py [line:47] 
Now operate osd.2
2017-05-23 14:08:24,607 INFO TC39_shutdown_osd_on_single_node.py [line:49] Set the osd.2 pid for kill
2017-05-23 14:08:25,319 INFO node.py [line:166] osd.0  ---> processId 20132
2017-05-23 14:08:25,319 INFO node.py [line:166] osd.1  ---> processId 22403
2017-05-23 14:08:25,319 INFO node.py [line:166] osd.2  ---> processId 17457
2017-05-23 14:08:25,319 INFO node.py [line:166] osd.0  ---> processId 20132
2017-05-23 14:08:25,319 INFO node.py [line:166] osd.1  ---> processId 22403
2017-05-23 14:08:25,319 INFO node.py [line:166] osd.2  ---> processId 17457
2017-05-23 14:08:25,319 INFO TC39_shutdown_osd_on_single_node.py [line:51] shutdown osd.2 by kill
2017-05-23 14:08:25,334 INFO osd.py [line:51] execute command is sudo -i kill 17457 & sleep 3
2017-05-23 14:08:33,553 INFO client.py [line:159] home/denali

2017-05-23 14:08:46,680 INFO client.py [line:159] home/denali

2017-05-23 14:08:53,618 INFO TC39_shutdown_osd_on_single_node.py [line:56] start osd.2
2017-05-23 14:08:53,618 INFO osd.py [line:100] node is  denali01
2017-05-23 14:08:53,618 INFO osd.py [line:101] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-05-23 14:09:24,339 INFO osd.py [line:105] osd osd.2 is start successfully
2017-05-23 14:09:24,339 INFO osd.py [line:113] node is  denali01
2017-05-23 14:09:24,339 INFO osd.py [line:114] execute command is ps -ef | grep 'ceph-osd -i 2'
2017-05-23 14:09:25,119 INFO osd.py [line:116] oot     27189     1 26 14:08 ?        00:00:08 ceph-osd -i 2
denali   28121 28120  0 14:09 ?        00:00:00 bash -c ps -ef | grep 'ceph-osd -i 2'
denali   28123 28121  0 14:09 ?        00:00:00 grep ceph-osd -i 2

2017-05-23 14:09:25,119 INFO osd.py [line:125] osd.2is alrady started
