2017-05-23 13:16:25,153 INFO TC38_osd_out_in_cluster.py [line:25] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. out all osds in sequence
4. stop all osds in sequence
5. start all osds in sequence
6. add in all osds in sequence
7. check the cluster status
8. repeat step 2-7 on the other node

2017-05-23 13:16:27,227 INFO monitors.py [line:123]    "quorum_leader_name": "denali01",

2017-05-23 13:16:27,227 INFO monitors.py [line:126]    "quorum_leader_name": "denali01",
2017-05-23 13:16:27,227 INFO TC38_osd_out_in_cluster.py [line:30] start to check cluster status before case running
2017-05-23 13:16:27,275 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:16:28,117 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 8, quorum 0,1,2 denali01,denali02,denali03
     osdmap e98: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79267: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47386 MB used, 292 GB / 338 GB avail
                3016 active+clean

2017-05-23 13:16:28,117 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:16:28,117 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:16:28,117 INFO TC38_osd_out_in_cluster.py [line:33] health status is OK
2017-05-23 13:16:28,117 INFO TC38_osd_out_in_cluster.py [line:38] 
Step 1: start IO from clients
2017-05-23 13:16:28,132 INFO base.py [line:35] 
Now start IO on  clientrbdImg0
2017-05-23 13:16:38,779 INFO client.py [line:125] pid info is 9012
2017-05-23 13:16:38,779 INFO client.py [line:125] pid info is 9013
2017-05-23 13:16:38,779 INFO base.py [line:35] 
Now start IO on  clientrbdImg1
2017-05-23 13:16:49,803 INFO client.py [line:125] pid info is 9062
2017-05-23 13:16:49,803 INFO client.py [line:125] pid info is 9063
2017-05-23 13:16:49,803 INFO base.py [line:35] 
Now start IO on  clientrbdImg2
2017-05-23 13:17:05,556 INFO client.py [line:125] pid info is 9112
2017-05-23 13:17:05,556 INFO client.py [line:125] pid info is 9113
2017-05-23 13:17:05,556 INFO base.py [line:35] 
Now start IO on  clientrbdImg3
2017-05-23 13:17:17,306 INFO client.py [line:125] pid info is 9165
2017-05-23 13:17:17,306 INFO client.py [line:125] pid info is 9166
2017-05-23 13:17:17,306 INFO base.py [line:35] 
Now start IO on  clientrbdImg4
2017-05-23 13:17:29,894 INFO client.py [line:125] pid info is 9215
2017-05-23 13:17:29,894 INFO client.py [line:125] pid info is 9216
2017-05-23 13:17:29,894 INFO base.py [line:35] 
Now start IO on  clientrbdImg5
2017-05-23 13:17:41,270 INFO client.py [line:125] pid info is 9266
2017-05-23 13:17:41,270 INFO client.py [line:125] pid info is 9267
2017-05-23 13:17:41,270 INFO base.py [line:35] 
Now start IO on  clientrbdImg6
2017-05-23 13:17:52,132 INFO client.py [line:125] pid info is 9315
2017-05-23 13:17:52,132 INFO client.py [line:125] pid info is 9316
2017-05-23 13:17:52,132 INFO base.py [line:35] 
Now start IO on  clientrbdImg7
2017-05-23 13:17:53,069 INFO client.py [line:125] pid info is 9365
2017-05-23 13:17:53,069 INFO client.py [line:125] pid info is 9366
2017-05-23 13:17:53,069 INFO base.py [line:35] 
Now start IO on  clientrbdImg8
2017-05-23 13:17:58,815 INFO client.py [line:125] pid info is 9413
2017-05-23 13:17:58,815 INFO client.py [line:125] pid info is 9414
2017-05-23 13:17:58,815 INFO base.py [line:35] 
Now start IO on  clientrbdImg9
2017-05-23 13:17:59,813 INFO client.py [line:125] pid info is 9463
2017-05-23 13:17:59,813 INFO client.py [line:125] pid info is 9464
2017-05-23 13:18:59,825 INFO TC38_osd_out_in_cluster.py [line:42] 
Step 2: Out the osd and check IO
2017-05-23 13:18:59,825 INFO TC38_osd_out_in_cluster.py [line:46] 
Now operate denali01
2017-05-23 13:18:59,825 INFO TC38_osd_out_in_cluster.py [line:47] 3
2017-05-23 13:18:59,825 INFO TC38_osd_out_in_cluster.py [line:48] 
Now operate osd.0
2017-05-23 13:18:59,825 INFO TC38_osd_out_in_cluster.py [line:49] out osd.0
2017-05-23 13:18:59,825 INFO osd.py [line:60] execute command is ceph osd out osd.0 & sleep 3
2017-05-23 13:19:05,456 INFO osd.py [line:65] osd.0 is already out cluster
2017-05-23 13:19:05,456 INFO TC38_osd_out_in_cluster.py [line:51] check if IO error
2017-05-23 13:19:11,046 INFO client.py [line:159] home/denali

2017-05-23 13:19:11,732 INFO TC38_osd_out_in_cluster.py [line:55] Set the osd.0 pid for kill
2017-05-23 13:19:22,441 INFO node.py [line:166] osd.0  ---> processId 
2017-05-23 13:19:22,441 INFO node.py [line:166] osd.1  ---> processId 
2017-05-23 13:19:22,441 INFO node.py [line:166] osd.2  ---> processId 
2017-05-23 13:19:22,441 INFO osd.py [line:75] execute command is ceph osd in osd.0 & sleep 3
2017-05-23 13:19:33,851 ERROR osd.py [line:81] Error when mark osd.0 in
2017-05-23 13:19:33,851 ERROR osd.py [line:82] ceph osd in osd.0 & sleep 3
2017-05-23 13:19:33,851 ERROR osd.py [line:83] 
2017-05-23 13:20:04,388 INFO client.py [line:159] home/denali

2017-05-23 13:20:05,107 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:20:08,868 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 8, quorum 0,1,2 denali01,denali02,denali03
     osdmap e124: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79423: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47491 MB used, 292 GB / 338 GB avail
                3016 active+clean
  client io 3721 B/s rd, 2356 kB/s wr, 3 op/s rd, 589 op/s wr

2017-05-23 13:20:08,868 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:20:08,868 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:20:08,868 INFO TC38_osd_out_in_cluster.py [line:81] stop osd.0 in cluster successfully
2017-05-23 13:20:08,868 INFO TC38_osd_out_in_cluster.py [line:46] 
Now operate denali01
2017-05-23 13:20:08,884 INFO TC38_osd_out_in_cluster.py [line:47] 3
2017-05-23 13:20:08,884 INFO TC38_osd_out_in_cluster.py [line:48] 
Now operate osd.1
2017-05-23 13:20:08,884 INFO TC38_osd_out_in_cluster.py [line:49] out osd.1
2017-05-23 13:20:08,884 INFO osd.py [line:60] execute command is ceph osd out osd.1 & sleep 3
2017-05-23 13:20:12,549 INFO osd.py [line:65] osd.1 is already out cluster
2017-05-23 13:20:12,549 INFO TC38_osd_out_in_cluster.py [line:51] check if IO error
2017-05-23 13:20:13,079 INFO client.py [line:159] home/denali

2017-05-23 13:20:13,611 INFO TC38_osd_out_in_cluster.py [line:55] Set the osd.1 pid for kill
2017-05-23 13:20:15,374 INFO node.py [line:166] osd.0  ---> processId 
2017-05-23 13:20:15,374 INFO node.py [line:166] osd.1  ---> processId 
2017-05-23 13:20:15,374 INFO node.py [line:166] osd.2  ---> processId 
2017-05-23 13:20:15,374 INFO osd.py [line:75] execute command is ceph osd in osd.1 & sleep 3
2017-05-23 13:20:19,101 INFO osd.py [line:78] osd.1 is already in cluster
2017-05-23 13:20:49,532 INFO client.py [line:159] home/denali

2017-05-23 13:20:49,937 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:20:50,734 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 8, quorum 0,1,2 denali01,denali02,denali03
     osdmap e133: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79461: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47493 MB used, 292 GB / 338 GB avail
                3016 active+clean
  client io 2064 kB/s wr, 0 op/s rd, 516 op/s wr

2017-05-23 13:20:50,734 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:20:50,734 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:20:50,734 INFO TC38_osd_out_in_cluster.py [line:81] stop osd.1 in cluster successfully
2017-05-23 13:20:50,750 INFO TC38_osd_out_in_cluster.py [line:46] 
Now operate denali01
2017-05-23 13:20:50,750 INFO TC38_osd_out_in_cluster.py [line:47] 3
2017-05-23 13:20:50,750 INFO TC38_osd_out_in_cluster.py [line:48] 
Now operate osd.2
2017-05-23 13:20:50,750 INFO TC38_osd_out_in_cluster.py [line:49] out osd.2
2017-05-23 13:20:50,750 INFO osd.py [line:60] execute command is ceph osd out osd.2 & sleep 3
2017-05-23 13:20:54,433 INFO osd.py [line:65] osd.2 is already out cluster
2017-05-23 13:20:54,433 INFO TC38_osd_out_in_cluster.py [line:51] check if IO error
2017-05-23 13:20:54,903 INFO client.py [line:159] home/denali

2017-05-23 13:20:55,464 INFO TC38_osd_out_in_cluster.py [line:55] Set the osd.2 pid for kill
2017-05-23 13:20:58,947 INFO node.py [line:166] osd.0  ---> processId 
2017-05-23 13:20:58,947 INFO node.py [line:166] osd.1  ---> processId 
2017-05-23 13:20:58,947 INFO node.py [line:166] osd.2  ---> processId 
2017-05-23 13:20:58,947 INFO osd.py [line:75] execute command is ceph osd in osd.2 & sleep 3
2017-05-23 13:21:04,832 ERROR osd.py [line:81] Error when mark osd.2 in
2017-05-23 13:21:04,832 ERROR osd.py [line:82] ceph osd in osd.2 & sleep 3
2017-05-23 13:21:04,832 ERROR osd.py [line:83] 
2017-05-23 13:21:35,263 INFO client.py [line:159] home/denali

2017-05-23 13:21:35,841 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:21:38,480 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 8, quorum 0,1,2 denali01,denali02,denali03
     osdmap e146: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79504: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47351 MB used, 292 GB / 338 GB avail
                3016 active+clean
  client io 4168 B/s rd, 2913 kB/s wr, 4 op/s rd, 728 op/s wr

2017-05-23 13:21:38,480 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:21:38,480 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:21:38,480 INFO TC38_osd_out_in_cluster.py [line:81] stop osd.2 in cluster successfully
2017-05-23 13:21:38,480 INFO TC38_osd_out_in_cluster.py [line:46] 
Now operate denali02
2017-05-23 13:21:38,496 INFO TC38_osd_out_in_cluster.py [line:47] 3
2017-05-23 13:21:38,496 INFO TC38_osd_out_in_cluster.py [line:48] 
Now operate osd.3
2017-05-23 13:21:38,496 INFO TC38_osd_out_in_cluster.py [line:49] out osd.3
2017-05-23 13:21:38,496 INFO osd.py [line:60] execute command is ceph osd out osd.3 & sleep 3
2017-05-23 13:21:48,190 INFO osd.py [line:65] osd.3 is already out cluster
2017-05-23 13:21:48,190 INFO TC38_osd_out_in_cluster.py [line:51] check if IO error
2017-05-23 13:21:48,628 INFO client.py [line:159] home/denali

2017-05-23 13:21:48,954 INFO TC38_osd_out_in_cluster.py [line:55] Set the osd.3 pid for kill
2017-05-23 13:22:23,441 INFO node.py [line:166] osd.3  ---> processId 
2017-05-23 13:22:23,441 INFO node.py [line:166] osd.4  ---> processId 
2017-05-23 13:22:23,441 INFO node.py [line:166] osd.5  ---> processId 
2017-05-23 13:22:23,441 INFO osd.py [line:75] execute command is ceph osd in osd.3 & sleep 3
2017-05-23 13:22:46,789 ERROR osd.py [line:81] Error when mark osd.3 in
2017-05-23 13:22:46,789 ERROR osd.py [line:82] ceph osd in osd.3 & sleep 3
2017-05-23 13:22:46,789 ERROR osd.py [line:83] 
2017-05-23 13:23:17,247 INFO client.py [line:159] home/denali

2017-05-23 13:23:17,559 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:23:18,668 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_ERR
            11 pgs are stuck inactive for more than 300 seconds
            1 pgs degraded
            11 pgs stuck inactive
            4 pgs stuck unclean
            recovery 2/176286 objects degraded (0.001%)
            recovery 1030/176286 objects misplaced (0.584%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e152: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79533: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47265 MB used, 292 GB / 338 GB avail
            2/176286 objects degraded (0.001%)
            1030/176286 objects misplaced (0.584%)
                2993 active+clean
                  11 activating
                   5 activating+remapped
                   3 active+remapped
                   3 active
                   1 active+degraded
recovery io 79991 kB/s, 42 objects/s
  client io 8269 B/s rd, 830 kB/s wr, 8 op/s rd, 212 op/s wr

2017-05-23 13:23:18,668 INFO cluster.py [line:207] Now status is HEALTH_ERR, sleep 60s and try again 
2017-05-23 13:24:18,700 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:24:20,056 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e152: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79572: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47218 MB used, 292 GB / 338 GB avail
                3016 active+clean
  client io 30082 B/s rd, 3919 kB/s wr, 29 op/s rd, 979 op/s wr

2017-05-23 13:24:20,056 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:24:20,056 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:24:20,056 INFO TC38_osd_out_in_cluster.py [line:81] stop osd.3 in cluster successfully
2017-05-23 13:24:20,056 INFO TC38_osd_out_in_cluster.py [line:46] 
Now operate denali02
2017-05-23 13:24:20,056 INFO TC38_osd_out_in_cluster.py [line:47] 3
2017-05-23 13:24:20,072 INFO TC38_osd_out_in_cluster.py [line:48] 
Now operate osd.4
2017-05-23 13:24:20,072 INFO TC38_osd_out_in_cluster.py [line:49] out osd.4
2017-05-23 13:24:20,072 INFO osd.py [line:60] execute command is ceph osd out osd.4 & sleep 3
2017-05-23 13:24:23,806 INFO osd.py [line:65] osd.4 is already out cluster
2017-05-23 13:24:23,806 INFO TC38_osd_out_in_cluster.py [line:51] check if IO error
2017-05-23 13:24:24,227 INFO client.py [line:159] home/denali

2017-05-23 13:24:24,539 INFO TC38_osd_out_in_cluster.py [line:55] Set the osd.4 pid for kill
2017-05-23 13:24:25,243 INFO node.py [line:166] osd.3  ---> processId 
2017-05-23 13:24:25,243 INFO node.py [line:166] osd.4  ---> processId 
2017-05-23 13:24:25,243 INFO node.py [line:166] osd.5  ---> processId 
2017-05-23 13:24:25,243 INFO osd.py [line:75] execute command is ceph osd in osd.4 & sleep 3
2017-05-23 13:24:40,605 ERROR osd.py [line:81] Error when mark osd.4 in
2017-05-23 13:24:40,605 ERROR osd.py [line:82] ceph osd in osd.4 & sleep 3
2017-05-23 13:24:40,605 ERROR osd.py [line:83] 
2017-05-23 13:25:10,931 INFO client.py [line:159] home/denali

2017-05-23 13:25:11,259 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:25:17,503 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_ERR
            10 pgs are stuck inactive for more than 300 seconds
            1 pgs backfill_wait
            570 pgs degraded
            10 pgs stuck inactive
            318 pgs stuck unclean
            570 pgs undersized
            recovery 17224/174886 objects degraded (9.849%)
            recovery 2/174886 objects misplaced (0.001%)
            1/9 in osds are down
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e165: 9 osds: 8 up, 9 in; 666 remapped pgs
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79597: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47195 MB used, 292 GB / 338 GB avail
            17224/174886 objects degraded (9.849%)
            2/174886 objects misplaced (0.001%)
                2349 active+clean
                 570 active+undersized+degraded
                  86 active+remapped
                  10 activating+remapped
                   1 active+remapped+backfill_wait
recovery io 459 B/s, 0 objects/s
  client io 35962 B/s wr, 0 op/s rd, 8 op/s wr

2017-05-23 13:25:17,503 INFO cluster.py [line:207] Now status is HEALTH_ERR, sleep 60s and try again 
2017-05-23 13:26:17,520 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:26:20,019 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e169: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79636: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47197 MB used, 292 GB / 338 GB avail
                3016 active+clean
  client io 1211 kB/s wr, 0 op/s rd, 302 op/s wr

2017-05-23 13:26:20,019 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:26:20,019 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:26:20,019 INFO TC38_osd_out_in_cluster.py [line:81] stop osd.4 in cluster successfully
2017-05-23 13:26:20,035 INFO TC38_osd_out_in_cluster.py [line:46] 
Now operate denali02
2017-05-23 13:26:20,035 INFO TC38_osd_out_in_cluster.py [line:47] 3
2017-05-23 13:26:20,035 INFO TC38_osd_out_in_cluster.py [line:48] 
Now operate osd.5
2017-05-23 13:26:20,035 INFO TC38_osd_out_in_cluster.py [line:49] out osd.5
2017-05-23 13:26:20,035 INFO osd.py [line:60] execute command is ceph osd out osd.5 & sleep 3
2017-05-23 13:26:24,138 INFO osd.py [line:65] osd.5 is already out cluster
2017-05-23 13:26:24,138 INFO TC38_osd_out_in_cluster.py [line:51] check if IO error
2017-05-23 13:26:24,496 INFO client.py [line:159] home/denali

2017-05-23 13:26:24,778 INFO TC38_osd_out_in_cluster.py [line:55] Set the osd.5 pid for kill
2017-05-23 13:26:27,227 INFO node.py [line:166] osd.3  ---> processId 
2017-05-23 13:26:27,227 INFO node.py [line:166] osd.4  ---> processId 
2017-05-23 13:26:27,227 INFO node.py [line:166] osd.5  ---> processId 
2017-05-23 13:26:27,227 INFO osd.py [line:75] execute command is ceph osd in osd.5 & sleep 3
2017-05-23 13:26:30,861 INFO osd.py [line:78] osd.5 is already in cluster
2017-05-23 13:27:01,200 INFO client.py [line:159] home/denali

2017-05-23 13:27:01,526 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:27:02,884 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_ERR
            26 pgs are stuck inactive for more than 300 seconds
            37 pgs degraded
            232 pgs peering
            17 pgs stale
            26 pgs stuck inactive
            53 pgs stuck unclean
            36 pgs undersized
            recovery 6080/174886 objects degraded (3.477%)
            recovery 1/174886 objects misplaced (0.001%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e182: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79665: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            40969 MB used, 261 GB / 301 GB avail
            6080/174886 objects degraded (3.477%)
            1/174886 objects misplaced (0.001%)
                2668 active+clean
                 229 remapped+peering
                  62 active+remapped
                  36 active+undersized+degraded
                  17 stale+active+clean
                   3 peering
                   1 active+degraded+remapped
recovery io 10615 kB/s, 5 objects/s
  client io 2332 kB/s wr, 0 op/s rd, 583 op/s wr

2017-05-23 13:27:02,900 INFO cluster.py [line:207] Now status is HEALTH_ERR, sleep 60s and try again 
2017-05-23 13:28:02,903 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:28:05,134 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e182: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79692: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47200 MB used, 292 GB / 338 GB avail
                3016 active+clean
  client io 852 kB/s wr, 0 op/s rd, 213 op/s wr

2017-05-23 13:28:05,134 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:28:05,134 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:28:05,134 INFO TC38_osd_out_in_cluster.py [line:81] stop osd.5 in cluster successfully
2017-05-23 13:28:05,134 INFO TC38_osd_out_in_cluster.py [line:46] 
Now operate denali03
2017-05-23 13:28:05,150 INFO TC38_osd_out_in_cluster.py [line:47] 3
2017-05-23 13:28:05,150 INFO TC38_osd_out_in_cluster.py [line:48] 
Now operate osd.6
2017-05-23 13:28:05,150 INFO TC38_osd_out_in_cluster.py [line:49] out osd.6
2017-05-23 13:28:05,150 INFO osd.py [line:60] execute command is ceph osd out osd.6 & sleep 3
2017-05-23 13:28:14,569 ERROR osd.py [line:68] Error when mark osd.6 out
2017-05-23 13:28:14,569 ERROR osd.py [line:69] ceph osd out osd.6 & sleep 3
2017-05-23 13:28:14,569 ERROR osd.py [line:70] 
2017-05-23 13:28:14,569 INFO TC38_osd_out_in_cluster.py [line:51] check if IO error
2017-05-23 13:28:14,930 INFO client.py [line:159] home/denali

2017-05-23 13:28:15,460 INFO TC38_osd_out_in_cluster.py [line:55] Set the osd.6 pid for kill
2017-05-23 13:28:18,039 INFO node.py [line:166] osd.6  ---> processId 
2017-05-23 13:28:18,039 INFO node.py [line:166] osd.7  ---> processId 
2017-05-23 13:28:18,039 INFO node.py [line:166] osd.8  ---> processId 
2017-05-23 13:28:18,039 INFO osd.py [line:75] execute command is ceph osd in osd.6 & sleep 3
2017-05-23 13:28:25,760 ERROR osd.py [line:81] Error when mark osd.6 in
2017-05-23 13:28:25,760 ERROR osd.py [line:82] ceph osd in osd.6 & sleep 3
2017-05-23 13:28:25,760 ERROR osd.py [line:83] 
2017-05-23 13:28:56,147 INFO client.py [line:159] home/denali

2017-05-23 13:28:56,506 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:29:02,293 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e193: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79736: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47203 MB used, 292 GB / 338 GB avail
                3016 active+clean
  client io 1784 kB/s wr, 0 op/s rd, 446 op/s wr

2017-05-23 13:29:02,293 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:29:02,293 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:29:02,293 INFO TC38_osd_out_in_cluster.py [line:81] stop osd.6 in cluster successfully
2017-05-23 13:29:02,309 INFO TC38_osd_out_in_cluster.py [line:46] 
Now operate denali03
2017-05-23 13:29:02,309 INFO TC38_osd_out_in_cluster.py [line:47] 3
2017-05-23 13:29:02,309 INFO TC38_osd_out_in_cluster.py [line:48] 
Now operate osd.7
2017-05-23 13:29:02,309 INFO TC38_osd_out_in_cluster.py [line:49] out osd.7
2017-05-23 13:29:02,309 INFO osd.py [line:60] execute command is ceph osd out osd.7 & sleep 3
2017-05-23 13:29:06,805 INFO osd.py [line:65] osd.7 is already out cluster
2017-05-23 13:29:06,805 INFO TC38_osd_out_in_cluster.py [line:51] check if IO error
2017-05-23 13:29:07,148 INFO client.py [line:159] home/denali

2017-05-23 13:29:07,444 INFO TC38_osd_out_in_cluster.py [line:55] Set the osd.7 pid for kill
2017-05-23 13:29:09,009 INFO node.py [line:166] osd.6  ---> processId 
2017-05-23 13:29:09,009 INFO node.py [line:166] osd.7  ---> processId 
2017-05-23 13:29:09,009 INFO node.py [line:166] osd.8  ---> processId 
2017-05-23 13:29:09,009 INFO osd.py [line:75] execute command is ceph osd in osd.7 & sleep 3
2017-05-23 13:29:18,157 ERROR osd.py [line:81] Error when mark osd.7 in
2017-05-23 13:29:18,157 ERROR osd.py [line:82] ceph osd in osd.7 & sleep 3
2017-05-23 13:29:18,157 ERROR osd.py [line:83] 
2017-05-23 13:29:53,493 INFO client.py [line:159] home/denali

2017-05-23 13:29:53,898 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:29:54,927 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            172 pgs degraded
            164 pgs stuck unclean
            172 pgs undersized
            recovery 9192/174886 objects degraded (5.256%)
            recovery 1/174886 objects misplaced (0.001%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e210: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79775: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47207 MB used, 292 GB / 338 GB avail
            9192/174886 objects degraded (5.256%)
            1/174886 objects misplaced (0.001%)
                2678 active+clean
                 172 active+undersized+degraded
                 166 active+remapped
  client io 2967 kB/s wr, 0 op/s rd, 741 op/s wr

2017-05-23 13:29:54,927 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:29:54,927 INFO cluster.py [line:231] usefull PG number is 2678
2017-05-23 13:29:54,943 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:29:55,755 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            172 pgs degraded
            164 pgs stuck unclean
            172 pgs undersized
            recovery 9192/174886 objects degraded (5.256%)
            recovery 1/174886 objects misplaced (0.001%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e210: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79776: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47207 MB used, 292 GB / 338 GB avail
            9192/174886 objects degraded (5.256%)
            1/174886 objects misplaced (0.001%)
                2678 active+clean
                 172 active+undersized+degraded
                 166 active+remapped
  client io 2560 kB/s wr, 0 op/s rd, 640 op/s wr

2017-05-23 13:29:55,755 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:29:55,755 INFO cluster.py [line:231] usefull PG number is 2678
2017-05-23 13:29:55,755 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:29:56,706 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            172 pgs degraded
            164 pgs stuck unclean
            172 pgs undersized
            recovery 9192/174886 objects degraded (5.256%)
            recovery 1/174886 objects misplaced (0.001%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e210: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79777: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47207 MB used, 292 GB / 338 GB avail
            9192/174886 objects degraded (5.256%)
            1/174886 objects misplaced (0.001%)
                2678 active+clean
                 172 active+undersized+degraded
                 166 active+remapped
  client io 1064 kB/s wr, 0 op/s rd, 266 op/s wr

2017-05-23 13:29:56,720 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:29:56,720 INFO cluster.py [line:231] usefull PG number is 2678
2017-05-23 13:29:56,720 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:29:57,780 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            172 pgs degraded
            167 pgs stuck unclean
            172 pgs undersized
            recovery 9192/174886 objects degraded (5.256%)
            recovery 1/174886 objects misplaced (0.001%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e210: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79778: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47207 MB used, 292 GB / 338 GB avail
            9192/174886 objects degraded (5.256%)
            1/174886 objects misplaced (0.001%)
                2678 active+clean
                 172 active+undersized+degraded
                 166 active+remapped
  client io 1442 kB/s wr, 0 op/s rd, 360 op/s wr

2017-05-23 13:29:57,780 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:29:57,780 INFO cluster.py [line:231] usefull PG number is 2678
2017-05-23 13:29:57,780 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:29:58,717 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            172 pgs degraded
            167 pgs stuck unclean
            172 pgs undersized
            recovery 9192/174886 objects degraded (5.256%)
            recovery 1/174886 objects misplaced (0.001%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e210: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79779: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47207 MB used, 292 GB / 338 GB avail
            9192/174886 objects degraded (5.256%)
            1/174886 objects misplaced (0.001%)
                2678 active+clean
                 172 active+undersized+degraded
                 166 active+remapped
  client io 2199 kB/s wr, 0 op/s rd, 549 op/s wr

2017-05-23 13:29:58,717 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:29:58,717 INFO cluster.py [line:231] usefull PG number is 2678
2017-05-23 13:29:58,717 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:29:59,825 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            172 pgs degraded
            167 pgs stuck unclean
            172 pgs undersized
            recovery 9192/174886 objects degraded (5.256%)
            recovery 1/174886 objects misplaced (0.001%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e210: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79780: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47207 MB used, 292 GB / 338 GB avail
            9192/174886 objects degraded (5.256%)
            1/174886 objects misplaced (0.001%)
                2678 active+clean
                 172 active+undersized+degraded
                 166 active+remapped
  client io 2922 kB/s wr, 0 op/s rd, 730 op/s wr

2017-05-23 13:29:59,825 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:29:59,825 INFO cluster.py [line:231] usefull PG number is 2678
2017-05-23 13:29:59,825 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:30:00,762 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            172 pgs degraded
            167 pgs stuck unclean
            172 pgs undersized
            recovery 9192/174886 objects degraded (5.256%)
            recovery 1/174886 objects misplaced (0.001%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e210: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79781: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47207 MB used, 292 GB / 338 GB avail
            9192/174886 objects degraded (5.256%)
            1/174886 objects misplaced (0.001%)
                2678 active+clean
                 172 active+undersized+degraded
                 166 active+remapped
  client io 2374 kB/s wr, 0 op/s rd, 593 op/s wr

2017-05-23 13:30:00,762 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:30:00,762 INFO cluster.py [line:231] usefull PG number is 2678
2017-05-23 13:30:00,762 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:30:01,588 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            172 pgs degraded
            167 pgs stuck unclean
            172 pgs undersized
            recovery 9192/174886 objects degraded (5.256%)
            recovery 1/174886 objects misplaced (0.001%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e210: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79781: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47207 MB used, 292 GB / 338 GB avail
            9192/174886 objects degraded (5.256%)
            1/174886 objects misplaced (0.001%)
                2678 active+clean
                 172 active+undersized+degraded
                 166 active+remapped
  client io 2374 kB/s wr, 0 op/s rd, 593 op/s wr

2017-05-23 13:30:01,588 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:30:01,588 INFO cluster.py [line:231] usefull PG number is 2678
2017-05-23 13:30:01,604 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:30:02,352 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_WARN
            172 pgs degraded
            167 pgs stuck unclean
            172 pgs undersized
            recovery 9192/174886 objects degraded (5.256%)
            recovery 1/174886 objects misplaced (0.001%)
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e210: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79781: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47207 MB used, 292 GB / 338 GB avail
            9192/174886 objects degraded (5.256%)
            1/174886 objects misplaced (0.001%)
                2678 active+clean
                 172 active+undersized+degraded
                 166 active+remapped
  client io 2374 kB/s wr, 0 op/s rd, 593 op/s wr

2017-05-23 13:30:02,352 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:30:02,352 INFO cluster.py [line:231] usefull PG number is 2678
2017-05-23 13:30:02,368 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:30:03,226 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e210: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79782: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47207 MB used, 292 GB / 338 GB avail
                3016 active+clean
recovery io 4432 kB/s, 53 objects/s
  client io 4397 kB/s wr, 0 op/s rd, 1099 op/s wr

2017-05-23 13:30:03,226 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:30:03,226 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:30:03,226 INFO TC38_osd_out_in_cluster.py [line:81] stop osd.7 in cluster successfully
2017-05-23 13:30:03,242 INFO TC38_osd_out_in_cluster.py [line:46] 
Now operate denali03
2017-05-23 13:30:03,242 INFO TC38_osd_out_in_cluster.py [line:47] 3
2017-05-23 13:30:03,242 INFO TC38_osd_out_in_cluster.py [line:48] 
Now operate osd.8
2017-05-23 13:30:03,242 INFO TC38_osd_out_in_cluster.py [line:49] out osd.8
2017-05-23 13:30:03,242 INFO osd.py [line:60] execute command is ceph osd out osd.8 & sleep 3
2017-05-23 13:30:06,799 INFO osd.py [line:65] osd.8 is already out cluster
2017-05-23 13:30:06,799 INFO TC38_osd_out_in_cluster.py [line:51] check if IO error
2017-05-23 13:30:07,141 INFO client.py [line:159] home/denali

2017-05-23 13:30:07,423 INFO TC38_osd_out_in_cluster.py [line:55] Set the osd.8 pid for kill
2017-05-23 13:30:08,098 INFO node.py [line:166] osd.6  ---> processId 
2017-05-23 13:30:08,098 INFO node.py [line:166] osd.7  ---> processId 
2017-05-23 13:30:08,098 INFO node.py [line:166] osd.8  ---> processId 
2017-05-23 13:30:08,098 INFO osd.py [line:75] execute command is ceph osd in osd.8 & sleep 3
2017-05-23 13:30:12,849 INFO osd.py [line:78] osd.8 is already in cluster
2017-05-23 13:30:43,201 INFO client.py [line:159] home/denali

2017-05-23 13:30:43,513 INFO cluster.py [line:203] execute command is ceph -s
2017-05-23 13:30:44,724 INFO cluster.py [line:205]    cluster 012dd832-3b17-4cb0-b763-c71908a82dda
     health HEALTH_OK
     monmap e3: 3 mons at {denali01=192.168.28.26:6789/0,denali02=192.168.28.33:6789/0,denali03=192.168.28.34:6789/0}
            election epoch 14, quorum 0,1,2 denali01,denali02,denali03
     osdmap e221: 9 osds: 9 up, 9 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v79815: 3016 pgs, 13 pools, 311 GB data, 87443 objects
            47209 MB used, 292 GB / 338 GB avail
                3016 active+clean
  client io 1337 kB/s wr, 0 op/s rd, 334 op/s wr

2017-05-23 13:30:44,724 INFO cluster.py [line:230] PG number is 3016
2017-05-23 13:30:44,724 INFO cluster.py [line:231] usefull PG number is 3016
2017-05-23 13:30:44,724 INFO TC38_osd_out_in_cluster.py [line:81] stop osd.8 in cluster successfully
2017-05-23 13:30:44,724 INFO TC38_osd_out_in_cluster.py [line:93] 
Step 3:stop IO from clients
2017-05-23 13:30:45,161 INFO TC38_osd_out_in_cluster.py [line:96] TC38_osd_out_in_cluster runs complete
