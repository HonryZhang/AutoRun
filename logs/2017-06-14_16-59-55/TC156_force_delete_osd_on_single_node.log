2017-06-14 16:59:55,836 INFO TC156_force_delete_osd_on_single_node.py [line:22] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node, force kill delete osd one by one
3. check IO status

2017-06-14 16:59:56,957 INFO monitors.py [line:126]    "quorum_leader_name": "server113",
stdin: is not a tty

2017-06-14 16:59:56,957 INFO monitors.py [line:129]    "quorum_leader_name": "server113",
2017-06-14 16:59:56,957 INFO TC156_force_delete_osd_on_single_node.py [line:24] init case
2017-06-14 16:59:56,958 INFO TC156_force_delete_osd_on_single_node.py [line:29] 
Step1: Check IO from clients
2017-06-14 16:59:57,412 INFO client.py [line:172] ['enali    80278  80277  0 09:00 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali    80280  80278  0 09:00 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-14 16:59:57,413 INFO client.py [line:177] IO stopped
2017-06-14 16:59:57,413 INFO client.py [line:178] start IO again
2017-06-14 16:59:57,413 INFO base.py [line:37] 
Now start IO on  server103rbdImg0
2017-06-14 16:59:57,642 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=server103 -pool=reliablityTestPool -rbdname=server103rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=server103rbdImg0
2017-06-14 16:59:57,862 INFO base.py [line:37] 
Now start IO on  server103rbdImg1
2017-06-14 16:59:58,053 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=server103 -pool=reliablityTestPool -rbdname=server103rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=server103rbdImg1
2017-06-14 16:59:58,318 INFO base.py [line:37] 
Now start IO on  server103rbdImg2
2017-06-14 16:59:58,530 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=server103 -pool=reliablityTestPool -rbdname=server103rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=server103rbdImg2
2017-06-14 16:59:58,773 INFO base.py [line:37] 
Now start IO on  server103rbdImg3
2017-06-14 16:59:58,956 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=server103 -pool=reliablityTestPool -rbdname=server103rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=server103rbdImg3
2017-06-14 16:59:59,334 INFO base.py [line:37] 
Now start IO on  server103rbdImg4
2017-06-14 16:59:59,560 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=server103 -pool=reliablityTestPool -rbdname=server103rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=server103rbdImg4
2017-06-14 17:01:01,839 INFO node.py [line:114] init osd on node server113
2017-06-14 17:01:02,484 INFO node.py [line:129] osd.0  ---> processId 
2017-06-14 17:01:02,485 INFO node.py [line:129] osd.1  ---> processId 
2017-06-14 17:01:02,485 INFO node.py [line:129] osd.2  ---> processId 
2017-06-14 17:01:02,485 INFO node.py [line:129] osd.3  ---> processId 
2017-06-14 17:01:02,485 INFO node.py [line:129] osd.4  ---> processId 
2017-06-14 17:01:02,710 INFO osd.py [line:28] node is  server113
2017-06-14 17:01:02,710 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=0 & sleep 3
2017-06-14 17:01:06,260 INFO osd.py [line:32] osd osd.0 is shutdown successfully
2017-06-14 17:01:11,265 INFO osd.py [line:102] node is  server113
2017-06-14 17:01:11,265 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-06-14 17:01:41,490 INFO osd.py [line:107] osd osd.0 is start successfully
2017-06-14 17:01:41,490 INFO osd.py [line:28] node is  server113
2017-06-14 17:01:41,490 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=1 & sleep 3
2017-06-14 17:01:44,996 INFO osd.py [line:32] osd osd.1 is shutdown successfully
2017-06-14 17:01:50,001 INFO osd.py [line:102] node is  server113
2017-06-14 17:01:50,001 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-06-14 17:02:20,195 INFO osd.py [line:107] osd osd.1 is start successfully
2017-06-14 17:02:20,195 INFO osd.py [line:28] node is  server113
2017-06-14 17:02:20,195 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=2 & sleep 3
2017-06-14 17:02:24,049 INFO osd.py [line:32] osd osd.2 is shutdown successfully
2017-06-14 17:02:29,054 INFO osd.py [line:102] node is  server113
2017-06-14 17:02:29,054 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-06-14 17:02:59,297 INFO osd.py [line:107] osd osd.2 is start successfully
2017-06-14 17:02:59,298 INFO osd.py [line:28] node is  server113
2017-06-14 17:02:59,298 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=3 & sleep 3
2017-06-14 17:03:03,295 INFO osd.py [line:32] osd osd.3 is shutdown successfully
2017-06-14 17:03:08,300 INFO osd.py [line:102] node is  server113
2017-06-14 17:03:08,301 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-06-14 17:03:38,495 INFO osd.py [line:107] osd osd.3 is start successfully
2017-06-14 17:03:38,495 INFO osd.py [line:28] node is  server113
2017-06-14 17:03:38,496 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=4 & sleep 3
2017-06-14 17:03:42,225 INFO osd.py [line:32] osd osd.4 is shutdown successfully
2017-06-14 17:03:47,231 INFO osd.py [line:102] node is  server113
2017-06-14 17:03:47,231 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-06-14 17:04:17,425 INFO osd.py [line:107] osd osd.4 is start successfully
2017-06-14 17:04:18,060 INFO node.py [line:150] osd.0  ---> processId 
2017-06-14 17:04:18,060 INFO node.py [line:150] osd.1  ---> processId 
2017-06-14 17:04:18,060 INFO node.py [line:150] osd.2  ---> processId 
2017-06-14 17:04:18,060 INFO node.py [line:150] osd.3  ---> processId 
2017-06-14 17:04:18,060 INFO node.py [line:150] osd.4  ---> processId 
2017-06-14 17:04:18,253 INFO node.py [line:200] otal 0
2017-06-14 17:04:18,253 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-0-block -> ../../nvme2n1p4
2017-06-14 17:04:18,253 INFO node.py [line:203] 0
2017-06-14 17:04:18,253 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-0-block -> ../../nvme2n1p4
2017-06-14 17:04:18,253 INFO node.py [line:212] nvme2n1
2017-06-14 17:04:18,254 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-0-db -> ../../nvme2n1p3
2017-06-14 17:04:18,254 INFO node.py [line:203] 0
2017-06-14 17:04:18,254 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-0-db -> ../../nvme2n1p3
2017-06-14 17:04:18,254 INFO node.py [line:212] nvme2n1
2017-06-14 17:04:18,254 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-0-wal -> ../../nvme2n1p2
2017-06-14 17:04:18,254 INFO node.py [line:203] 0
2017-06-14 17:04:18,254 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-0-wal -> ../../nvme2n1p2
2017-06-14 17:04:18,254 INFO node.py [line:212] nvme2n1
2017-06-14 17:04:18,254 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-1-block -> ../../nvme1n1p4
2017-06-14 17:04:18,255 INFO node.py [line:203] 1
2017-06-14 17:04:18,255 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-1-block -> ../../nvme1n1p4
2017-06-14 17:04:18,255 INFO node.py [line:212] nvme1n1
2017-06-14 17:04:18,255 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-1-db -> ../../nvme1n1p3
2017-06-14 17:04:18,255 INFO node.py [line:203] 1
2017-06-14 17:04:18,255 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-1-db -> ../../nvme1n1p3
2017-06-14 17:04:18,255 INFO node.py [line:212] nvme1n1
2017-06-14 17:04:18,255 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-1-wal -> ../../nvme1n1p2
2017-06-14 17:04:18,255 INFO node.py [line:203] 1
2017-06-14 17:04:18,255 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:01 ceph-1-wal -> ../../nvme1n1p2
2017-06-14 17:04:18,256 INFO node.py [line:212] nvme1n1
2017-06-14 17:04:18,256 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:02 ceph-2-block -> ../../nvme4n1p4
2017-06-14 17:04:18,256 INFO node.py [line:203] 2
2017-06-14 17:04:18,256 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:02 ceph-2-block -> ../../nvme4n1p4
2017-06-14 17:04:18,256 INFO node.py [line:212] nvme4n1
2017-06-14 17:04:18,256 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:02 ceph-2-db -> ../../nvme4n1p3
2017-06-14 17:04:18,256 INFO node.py [line:203] 2
2017-06-14 17:04:18,256 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:02 ceph-2-db -> ../../nvme4n1p3
2017-06-14 17:04:18,256 INFO node.py [line:212] nvme4n1
2017-06-14 17:04:18,256 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:02 ceph-2-wal -> ../../nvme4n1p2
2017-06-14 17:04:18,257 INFO node.py [line:203] 2
2017-06-14 17:04:18,257 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:02 ceph-2-wal -> ../../nvme4n1p2
2017-06-14 17:04:18,257 INFO node.py [line:212] nvme4n1
2017-06-14 17:04:18,257 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-3-block -> ../../nvme0n1p4
2017-06-14 17:04:18,257 INFO node.py [line:203] 3
2017-06-14 17:04:18,257 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-3-block -> ../../nvme0n1p4
2017-06-14 17:04:18,257 INFO node.py [line:212] nvme0n1
2017-06-14 17:04:18,257 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-3-db -> ../../nvme0n1p3
2017-06-14 17:04:18,257 INFO node.py [line:203] 3
2017-06-14 17:04:18,257 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-3-db -> ../../nvme0n1p3
2017-06-14 17:04:18,258 INFO node.py [line:212] nvme0n1
2017-06-14 17:04:18,258 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-3-wal -> ../../nvme0n1p2
2017-06-14 17:04:18,258 INFO node.py [line:203] 3
2017-06-14 17:04:18,258 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-3-wal -> ../../nvme0n1p2
2017-06-14 17:04:18,258 INFO node.py [line:212] nvme0n1
2017-06-14 17:04:18,258 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-4-block -> ../../nvme3n1p4
2017-06-14 17:04:18,258 INFO node.py [line:203] 4
2017-06-14 17:04:18,258 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-4-block -> ../../nvme3n1p4
2017-06-14 17:04:18,258 INFO node.py [line:212] nvme3n1
2017-06-14 17:04:18,258 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-4-db -> ../../nvme3n1p3
2017-06-14 17:04:18,259 INFO node.py [line:203] 4
2017-06-14 17:04:18,259 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-4-db -> ../../nvme3n1p3
2017-06-14 17:04:18,259 INFO node.py [line:212] nvme3n1
2017-06-14 17:04:18,259 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-4-wal -> ../../nvme3n1p2
2017-06-14 17:04:18,259 INFO node.py [line:203] 4
2017-06-14 17:04:18,259 INFO node.py [line:204] lrwxrwxrwx 1 root root 15 Jun 14 17:03 ceph-4-wal -> ../../nvme3n1p2
2017-06-14 17:04:18,259 INFO node.py [line:212] nvme3n1
2017-06-14 17:04:18,259 INFO node.py [line:200] lrwxrwxrwx 1 root root 15 Jun 14 10:35 head-reverse-part -> ../../nvme3n1p1
2017-06-14 17:04:18,259 INFO node.py [line:200] 
2017-06-14 17:04:18,678 INFO node.py [line:220] osd.0  ---> disk 
2017-06-14 17:04:18,678 INFO node.py [line:220] osd.1  ---> disk 
2017-06-14 17:04:18,678 INFO node.py [line:220] osd.2  ---> disk 
2017-06-14 17:04:18,678 INFO node.py [line:220] osd.3  ---> disk 
2017-06-14 17:04:18,678 INFO node.py [line:220] osd.4  ---> disk 
2017-06-14 17:04:18,883 INFO node.py [line:176] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-14 17:04:18,883 INFO node.py [line:178] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-14 17:04:19,807 INFO TC156_force_delete_osd_on_single_node.py [line:41] start to check cluster status before case running
2017-06-14 17:04:19,807 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-14 17:04:20,198 INFO cluster.py [line:213]    cluster e95425c5-dda1-4a27-be7b-9baccd00f2ec
     health HEALTH_OK
     monmap e3: 3 mons at {server113=192.168.1.113:6789/0,server114=192.168.1.114:6789/0,server115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 server113,server114,server115
     osdmap e123: 15 osds: 15 up, 15 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v7255: 1680 pgs, 12 pools, 2032 bytes data, 206 objects
            129 GB used, 10358 GB / 10488 GB avail
                1680 active+clean
stdin: is not a tty

2017-06-14 17:04:20,199 INFO cluster.py [line:238] PG number is 1680
2017-06-14 17:04:20,199 INFO cluster.py [line:239] usefull PG number is 1680
2017-06-14 17:04:20,199 INFO TC156_force_delete_osd_on_single_node.py [line:44] health status is OK
2017-06-14 17:04:20,199 INFO TC156_force_delete_osd_on_single_node.py [line:51] set pid process
2017-06-14 17:04:20,841 INFO node.py [line:167] osd.0  ---> processId 
2017-06-14 17:04:20,841 INFO node.py [line:167] osd.1  ---> processId 
2017-06-14 17:04:20,841 INFO node.py [line:167] osd.2  ---> processId 
2017-06-14 17:04:20,841 INFO node.py [line:167] osd.3  ---> processId 
2017-06-14 17:04:20,841 INFO node.py [line:167] osd.4  ---> processId 
2017-06-14 17:04:21,028 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-14 17:04:24,212 INFO osd.py [line:132] execute command is sudo -i ceph osd crush rm osd.0 & sleep 10
2017-06-14 17:04:34,405 INFO osd.py [line:134] tdin: is not a tty
removed item id 0 name 'osd.0' from crush map

2017-06-14 17:04:34,406 INFO osd.py [line:142] execute command is sudo -i ceph auth del osd.0 & sleep 10
2017-06-14 17:04:44,627 INFO osd.py [line:144] tdin: is not a tty
updated

2017-06-14 17:04:44,627 INFO osd.py [line:177] execute command is sudo -i ceph osd rm osd.0 & sleep 10
2017-06-14 17:04:54,812 ERROR osd.py [line:180] Error when delete osd from osd.0
2017-06-14 17:04:54,812 ERROR osd.py [line:181] sudo -i ceph osd rm osd.0 & sleep 10
2017-06-14 17:04:54,812 ERROR osd.py [line:182] tdin: is not a tty
Error EBUSY: osd.0 is still up; must be down before removal. 

2017-06-14 17:05:24,832 INFO osd.py [line:168] execute command is sudo -i parted -s -a optimal /dev/ mklabel gpt & sleep 30
2017-06-14 17:05:55,059 INFO osd.py [line:170] arning: Unable to open /dev read-write (Is a directory).  /dev has been opened read-only.
Warning: Unable to open /dev read-write (Is a directory).  /dev has been opened read-only.
Error: Can't have the end before the start! (start sector=34 length=-48)
Error: Can't have a partition outside the disk!
Error: Is a directory during read on /dev
Error: Is a directory during read on /dev
Error: Can't write to /dev, because it is opened read-only.
Error: Can't have a partition outside the disk!
stdin: is not a tty

2017-06-14 17:05:55,059 INFO osd.py [line:189] sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/updateCephConfig.sh [osd.0]
2017-06-14 17:05:55,267 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-14 17:05:55,714 INFO cluster.py [line:213]    cluster e95425c5-dda1-4a27-be7b-9baccd00f2ec
     health HEALTH_OK
     monmap e3: 3 mons at {server113=192.168.1.113:6789/0,server114=192.168.1.114:6789/0,server115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 server113,server114,server115
     osdmap e126: 15 osds: 15 up, 15 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v7266: 1680 pgs, 12 pools, 2032 bytes data, 206 objects
            129 GB used, 10358 GB / 10488 GB avail
                1680 active+clean
stdin: is not a tty

2017-06-14 17:05:55,714 INFO cluster.py [line:238] PG number is 1680
2017-06-14 17:05:55,715 INFO cluster.py [line:239] usefull PG number is 1680
2017-06-14 17:05:55,715 INFO TC156_force_delete_osd_on_single_node.py [line:66] osd.0 delete succesfully
2017-06-14 17:05:55,715 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-14 17:05:58,936 INFO osd.py [line:132] execute command is sudo -i ceph osd crush rm osd.1 & sleep 10
2017-06-14 17:06:09,161 INFO osd.py [line:134] tdin: is not a tty
removed item id 1 name 'osd.1' from crush map

2017-06-14 17:06:09,161 INFO osd.py [line:142] execute command is sudo -i ceph auth del osd.1 & sleep 10
2017-06-14 17:06:19,387 INFO osd.py [line:144] tdin: is not a tty
updated

2017-06-14 17:06:19,387 INFO osd.py [line:177] execute command is sudo -i ceph osd rm osd.1 & sleep 10
2017-06-14 17:06:29,581 ERROR osd.py [line:180] Error when delete osd from osd.1
2017-06-14 17:06:29,581 ERROR osd.py [line:181] sudo -i ceph osd rm osd.1 & sleep 10
2017-06-14 17:06:29,581 ERROR osd.py [line:182] tdin: is not a tty
Error EBUSY: osd.1 is still up; must be down before removal. 

2017-06-14 17:06:59,611 INFO osd.py [line:168] execute command is sudo -i parted -s -a optimal /dev/ mklabel gpt & sleep 30
2017-06-14 17:07:29,805 INFO osd.py [line:170] arning: Unable to open /dev read-write (Is a directory).  /dev has been opened read-only.
Warning: Unable to open /dev read-write (Is a directory).  /dev has been opened read-only.
Error: Can't have the end before the start! (start sector=34 length=-48)
Error: Can't have a partition outside the disk!
Error: Is a directory during read on /dev
Error: Is a directory during read on /dev
Error: Can't write to /dev, because it is opened read-only.
Error: Can't have a partition outside the disk!
stdin: is not a tty

2017-06-14 17:07:29,805 INFO osd.py [line:189] sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/updateCephConfig.sh [osd.1]
2017-06-14 17:07:30,021 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-14 17:07:30,426 INFO cluster.py [line:213]    cluster e95425c5-dda1-4a27-be7b-9baccd00f2ec
     health HEALTH_OK
     monmap e3: 3 mons at {server113=192.168.1.113:6789/0,server114=192.168.1.114:6789/0,server115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 server113,server114,server115
     osdmap e129: 15 osds: 15 up, 15 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v7277: 1680 pgs, 12 pools, 2032 bytes data, 206 objects
            129 GB used, 10358 GB / 10488 GB avail
                1680 active+clean
stdin: is not a tty

2017-06-14 17:07:30,427 INFO cluster.py [line:238] PG number is 1680
2017-06-14 17:07:30,427 INFO cluster.py [line:239] usefull PG number is 1680
2017-06-14 17:07:30,427 INFO TC156_force_delete_osd_on_single_node.py [line:66] osd.1 delete succesfully
2017-06-14 17:07:30,427 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-14 17:07:33,724 INFO osd.py [line:132] execute command is sudo -i ceph osd crush rm osd.2 & sleep 10
2017-06-14 17:07:43,950 INFO osd.py [line:134] tdin: is not a tty
removed item id 2 name 'osd.2' from crush map

2017-06-14 17:07:43,951 INFO osd.py [line:142] execute command is sudo -i ceph auth del osd.2 & sleep 10
2017-06-14 17:07:54,135 INFO osd.py [line:144] tdin: is not a tty
updated

2017-06-14 17:07:54,135 INFO osd.py [line:177] execute command is sudo -i ceph osd rm osd.2 & sleep 10
2017-06-14 17:08:04,320 ERROR osd.py [line:180] Error when delete osd from osd.2
2017-06-14 17:08:04,320 ERROR osd.py [line:181] sudo -i ceph osd rm osd.2 & sleep 10
2017-06-14 17:08:04,320 ERROR osd.py [line:182] tdin: is not a tty
Error EBUSY: osd.2 is still up; must be down before removal. 

2017-06-14 17:08:34,349 INFO osd.py [line:168] execute command is sudo -i parted -s -a optimal /dev/ mklabel gpt & sleep 30
2017-06-14 17:09:04,549 INFO osd.py [line:170] arning: Unable to open /dev read-write (Is a directory).  /dev has been opened read-only.
Warning: Unable to open /dev read-write (Is a directory).  /dev has been opened read-only.
Error: Can't have the end before the start! (start sector=34 length=-48)
Error: Can't have a partition outside the disk!
Error: Is a directory during read on /dev
Error: Is a directory during read on /dev
Error: Can't write to /dev, because it is opened read-only.
Error: Can't have a partition outside the disk!
stdin: is not a tty

2017-06-14 17:09:04,549 INFO osd.py [line:189] sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/updateCephConfig.sh [osd.2]
2017-06-14 17:09:04,754 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-14 17:09:05,163 INFO cluster.py [line:213]    cluster e95425c5-dda1-4a27-be7b-9baccd00f2ec
     health HEALTH_OK
     monmap e3: 3 mons at {server113=192.168.1.113:6789/0,server114=192.168.1.114:6789/0,server115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 server113,server114,server115
     osdmap e132: 15 osds: 15 up, 15 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v7287: 1680 pgs, 12 pools, 2032 bytes data, 206 objects
            129 GB used, 10358 GB / 10488 GB avail
                1680 active+clean
stdin: is not a tty

2017-06-14 17:09:05,164 INFO cluster.py [line:238] PG number is 1680
2017-06-14 17:09:05,164 INFO cluster.py [line:239] usefull PG number is 1680
2017-06-14 17:09:05,164 INFO TC156_force_delete_osd_on_single_node.py [line:66] osd.2 delete succesfully
2017-06-14 17:09:05,164 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-14 17:09:08,359 INFO osd.py [line:132] execute command is sudo -i ceph osd crush rm osd.3 & sleep 10
2017-06-14 17:09:18,560 INFO osd.py [line:134] tdin: is not a tty
removed item id 3 name 'osd.3' from crush map

2017-06-14 17:09:18,561 INFO osd.py [line:142] execute command is sudo -i ceph auth del osd.3 & sleep 10
2017-06-14 17:09:28,778 INFO osd.py [line:144] tdin: is not a tty
updated

2017-06-14 17:09:28,778 INFO osd.py [line:177] execute command is sudo -i ceph osd rm osd.3 & sleep 10
2017-06-14 17:09:38,962 ERROR osd.py [line:180] Error when delete osd from osd.3
2017-06-14 17:09:38,962 ERROR osd.py [line:181] sudo -i ceph osd rm osd.3 & sleep 10
2017-06-14 17:09:38,962 ERROR osd.py [line:182] tdin: is not a tty
Error EBUSY: osd.3 is still up; must be down before removal. 

2017-06-14 17:10:08,983 INFO osd.py [line:168] execute command is sudo -i parted -s -a optimal /dev/ mklabel gpt & sleep 30
2017-06-14 17:10:39,176 INFO osd.py [line:170] arning: Unable to open /dev read-write (Is a directory).  /dev has been opened read-only.
Warning: Unable to open /dev read-write (Is a directory).  /dev has been opened read-only.
Error: Can't have the end before the start! (start sector=34 length=-48)
Error: Can't have a partition outside the disk!
Error: Is a directory during read on /dev
Error: Is a directory during read on /dev
Error: Can't write to /dev, because it is opened read-only.
Error: Can't have a partition outside the disk!
stdin: is not a tty

2017-06-14 17:10:39,176 INFO osd.py [line:189] sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/updateCephConfig.sh [osd.3]
2017-06-14 17:10:39,384 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-14 17:10:39,762 INFO cluster.py [line:213]    cluster e95425c5-dda1-4a27-be7b-9baccd00f2ec
     health HEALTH_OK
     monmap e3: 3 mons at {server113=192.168.1.113:6789/0,server114=192.168.1.114:6789/0,server115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 server113,server114,server115
     osdmap e136: 15 osds: 15 up, 15 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v7308: 1680 pgs, 12 pools, 2032 bytes data, 206 objects
            129 GB used, 10358 GB / 10488 GB avail
                1680 active+clean
  client io 121 kB/s rd, 0 B/s wr, 121 op/s rd, 80 op/s wr
stdin: is not a tty

2017-06-14 17:10:39,763 INFO cluster.py [line:238] PG number is 1680
2017-06-14 17:10:39,763 INFO cluster.py [line:239] usefull PG number is 1680
2017-06-14 17:10:39,763 INFO TC156_force_delete_osd_on_single_node.py [line:66] osd.3 delete succesfully
2017-06-14 17:10:39,763 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-14 17:10:42,975 INFO osd.py [line:132] execute command is sudo -i ceph osd crush rm osd.4 & sleep 10
2017-06-14 17:10:53,185 INFO osd.py [line:134] tdin: is not a tty
removed item id 4 name 'osd.4' from crush map

2017-06-14 17:10:53,185 INFO osd.py [line:142] execute command is sudo -i ceph auth del osd.4 & sleep 10
2017-06-14 17:11:03,370 INFO osd.py [line:144] tdin: is not a tty
updated

2017-06-14 17:11:03,371 INFO osd.py [line:177] execute command is sudo -i ceph osd rm osd.4 & sleep 10
2017-06-14 17:11:13,566 ERROR osd.py [line:180] Error when delete osd from osd.4
2017-06-14 17:11:13,566 ERROR osd.py [line:181] sudo -i ceph osd rm osd.4 & sleep 10
2017-06-14 17:11:13,566 ERROR osd.py [line:182] tdin: is not a tty
Error EBUSY: osd.4 is still up; must be down before removal. 

2017-06-14 17:11:43,595 INFO osd.py [line:168] execute command is sudo -i parted -s -a optimal /dev/ mklabel gpt & sleep 30
2017-06-14 17:12:13,788 INFO osd.py [line:170] arning: Unable to open /dev read-write (Is a directory).  /dev has been opened read-only.
Warning: Unable to open /dev read-write (Is a directory).  /dev has been opened read-only.
Error: Can't have the end before the start! (start sector=34 length=-48)
Error: Can't have a partition outside the disk!
Error: Is a directory during read on /dev
Error: Is a directory during read on /dev
Error: Can't write to /dev, because it is opened read-only.
Error: Can't have a partition outside the disk!
stdin: is not a tty

2017-06-14 17:12:13,788 INFO osd.py [line:189] sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/updateCephConfig.sh [osd.4]
2017-06-14 17:12:13,989 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-14 17:12:14,368 INFO cluster.py [line:213]    cluster e95425c5-dda1-4a27-be7b-9baccd00f2ec
     health HEALTH_OK
     monmap e3: 3 mons at {server113=192.168.1.113:6789/0,server114=192.168.1.114:6789/0,server115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 server113,server114,server115
     osdmap e141: 15 osds: 15 up, 15 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v7323: 1680 pgs, 12 pools, 2032 bytes data, 206 objects
            129 GB used, 10358 GB / 10488 GB avail
                1680 active+clean
stdin: is not a tty

2017-06-14 17:12:14,368 INFO cluster.py [line:238] PG number is 1680
2017-06-14 17:12:14,368 INFO cluster.py [line:239] usefull PG number is 1680
2017-06-14 17:12:14,368 INFO TC156_force_delete_osd_on_single_node.py [line:66] osd.4 delete succesfully
2017-06-14 17:12:15,624 INFO node.py [line:268] execute command is sudo -i ceph osd crush remove server113
2017-06-14 17:12:15,624 INFO node.py [line:269] tdin: is not a tty
removed item id -2 name 'server113' from crush map

2017-06-14 17:13:17,545 INFO TC156_force_delete_osd_on_single_node.py [line:81] start to create osd on node server113 
2017-06-14 17:13:17,546 INFO node.py [line:225] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/changeCommon.sh 
2017-06-14 17:13:18,166 INFO node.py [line:232] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/create_osds_local.sh -f 
2017-06-14 17:13:18,166 INFO node.py [line:233] NFO: osd_num_in_each_disk=1
WARNING: /dev/nvme2n1 has partitions
WARNING: /dev/nvme1n1 has partitions
WARNING: /dev/nvme4n1 has partitions
ERROR: '/dev/sda' was mounted
Reslut:Create osd on server113 failed.
Detail:'/dev/sda' was mounted.
stdin: is not a tty

2017-06-14 17:13:18,166 ERROR node.py [line:235] Error when create osd
2017-06-14 17:13:18,166 ERROR node.py [line:236] sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/create_osds_local.sh -f 
2017-06-14 17:13:18,167 ERROR node.py [line:237] NFO: osd_num_in_each_disk=1
WARNING: /dev/nvme2n1 has partitions
WARNING: /dev/nvme1n1 has partitions
WARNING: /dev/nvme4n1 has partitions
ERROR: '/dev/sda' was mounted
Reslut:Create osd on server113 failed.
Detail:'/dev/sda' was mounted.
stdin: is not a tty

