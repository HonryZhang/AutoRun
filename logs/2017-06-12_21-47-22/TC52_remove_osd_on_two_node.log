2017-06-12 21:47:22,316 INFO TC52_remove_osd_on_two_node.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. remove one osd from the first node
4. login the second node
5. remove one osd from the second node

2017-06-12 21:47:23,278 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-12 21:47:23,278 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-12 21:47:23,278 INFO TC52_remove_osd_on_two_node.py [line:29] start to check cluster status before case running
2017-06-12 21:47:25,283 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-12 21:47:25,703 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 6, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e125: 20 osds: 20 up, 20 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v11331: 3216 pgs, 13 pools, 98371 bytes data, 222 objects
            172 GB used, 6705 GB / 6877 GB avail
                3216 active+clean
  client io 367 kB/s rd, 476 op/s rd, 0 op/s wr
stdin: is not a tty

2017-06-12 21:47:25,704 INFO cluster.py [line:238] PG number is 3216
2017-06-12 21:47:25,704 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-12 21:47:25,704 INFO TC52_remove_osd_on_two_node.py [line:32] health status is OK
2017-06-12 21:47:25,704 INFO TC52_remove_osd_on_two_node.py [line:37] 
Step1: Check IO from clients
2017-06-12 21:47:26,156 INFO client.py [line:172] ['enali    24146  24145  0 13:47 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali    24148  24146  0 13:47 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-12 21:47:26,157 INFO client.py [line:177] IO stopped
2017-06-12 21:47:26,157 INFO client.py [line:178] start IO again
2017-06-12 21:47:26,157 INFO base.py [line:37] 
Now start IO on  client100rbdImg0
2017-06-12 21:47:26,373 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0
2017-06-12 21:47:26,597 INFO base.py [line:37] 
Now start IO on  client100rbdImg1
2017-06-12 21:47:26,780 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1
2017-06-12 21:47:27,040 INFO base.py [line:37] 
Now start IO on  client100rbdImg2
2017-06-12 21:47:27,245 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2
2017-06-12 21:47:27,467 INFO base.py [line:37] 
Now start IO on  client100rbdImg3
2017-06-12 21:47:27,684 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3
2017-06-12 21:47:27,917 INFO base.py [line:37] 
Now start IO on  client100rbdImg4
2017-06-12 21:47:28,134 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4
2017-06-12 21:47:28,510 INFO node.py [line:159] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-12 21:47:28,510 INFO node.py [line:161] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-12 21:47:29,626 INFO node.py [line:159] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-12 21:47:29,626 INFO node.py [line:161] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-12 21:47:30,541 INFO TC52_remove_osd_on_two_node.py [line:47] 
Step2: kill one osd from two node
2017-06-12 21:47:30,542 INFO TC52_remove_osd_on_two_node.py [line:49] start to delete osd on node taheo125 
2017-06-12 21:47:30,727 INFO node.py [line:185] 1
2017-06-12 21:47:30,728 INFO node.py [line:195] nvme27n1
2017-06-12 21:47:30,728 INFO node.py [line:185] 1
2017-06-12 21:47:30,728 INFO node.py [line:195] nvme27n1
2017-06-12 21:47:30,728 INFO node.py [line:185] 1
2017-06-12 21:47:30,728 INFO node.py [line:195] nvme27n1
2017-06-12 21:47:30,728 INFO node.py [line:185] 2
2017-06-12 21:47:30,728 INFO node.py [line:195] nvme20n1
2017-06-12 21:47:30,729 INFO node.py [line:185] 2
2017-06-12 21:47:30,729 INFO node.py [line:195] nvme20n1
2017-06-12 21:47:30,729 INFO node.py [line:185] 2
2017-06-12 21:47:30,729 INFO node.py [line:195] nvme20n1
2017-06-12 21:47:30,729 INFO node.py [line:185] 3
2017-06-12 21:47:30,729 INFO node.py [line:195] nvme2n1
2017-06-12 21:47:30,729 INFO node.py [line:185] 3
2017-06-12 21:47:30,729 INFO node.py [line:195] nvme2n1
2017-06-12 21:47:30,729 INFO node.py [line:185] 3
2017-06-12 21:47:30,730 INFO node.py [line:195] nvme2n1
2017-06-12 21:47:30,730 INFO node.py [line:185] 4
2017-06-12 21:47:30,730 INFO node.py [line:195] nvme17n1
2017-06-12 21:47:30,730 INFO node.py [line:185] 4
2017-06-12 21:47:30,730 INFO node.py [line:195] nvme17n1
2017-06-12 21:47:30,730 INFO node.py [line:185] 4
2017-06-12 21:47:30,730 INFO node.py [line:195] nvme17n1
2017-06-12 21:47:30,730 INFO node.py [line:185] 5
2017-06-12 21:47:30,730 INFO node.py [line:195] nvme10n1
2017-06-12 21:47:30,731 INFO node.py [line:185] 5
2017-06-12 21:47:30,731 INFO node.py [line:195] nvme10n1
2017-06-12 21:47:30,731 INFO node.py [line:185] 5
2017-06-12 21:47:30,731 INFO node.py [line:195] nvme10n1
2017-06-12 21:47:30,731 INFO node.py [line:185] 6
2017-06-12 21:47:30,731 INFO node.py [line:195] nvme13n1
2017-06-12 21:47:30,731 INFO node.py [line:185] 6
2017-06-12 21:47:30,731 INFO node.py [line:195] nvme13n1
2017-06-12 21:47:30,731 INFO node.py [line:185] 6
2017-06-12 21:47:30,732 INFO node.py [line:195] nvme13n1
2017-06-12 21:47:30,732 INFO node.py [line:185] 
2017-06-12 21:47:30,732 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-12 21:47:33,951 INFO osd.py [line:89] node is  taheo125
2017-06-12 21:47:33,951 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=1 & sleep 30
2017-06-12 21:48:04,141 ERROR osd.py [line:96] Error when start osdosd.1
2017-06-12 21:48:04,141 ERROR osd.py [line:97] sudo -i start ceph-osd id=1 & sleep 30
2017-06-12 21:48:04,141 ERROR osd.py [line:98] tdin: is not a tty
start: Job is already running: ceph-osd (ceph/1)

2017-06-12 21:48:04,141 INFO osd.py [line:156] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/delete_osds_local.sh -n -d /dev/
2017-06-12 21:48:04,404 ERROR osd.py [line:160] Error when delete osd.1
2017-06-12 21:48:04,404 ERROR osd.py [line:161] sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/delete_osds_local.sh -n -d /dev/
2017-06-12 21:48:04,404 ERROR osd.py [line:162] ARNING: '/dev/' is not block device
'/dev/' is not block device, 
stdin: is not a tty

