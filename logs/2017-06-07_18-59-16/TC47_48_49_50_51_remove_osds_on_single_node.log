2017-06-07 18:59:16,531 INFO TC47_48_49_50_51_remove_osds_on_single_node.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. remove the osd one by one on the first node
4. check the status
5. create osd on the node

2017-06-07 18:59:17,577 INFO monitors.py [line:126]    "quorum_leader_name": "CW113",
stdin: is not a tty

2017-06-07 18:59:17,577 INFO monitors.py [line:129]    "quorum_leader_name": "CW113",
2017-06-07 18:59:19,580 INFO node.py [line:97] init osd on node CW113
2017-06-07 18:59:19,863 INFO node.py [line:112] osd.0  ---> processId 12729
2017-06-07 18:59:19,863 INFO node.py [line:112] osd.1  ---> processId 13828
2017-06-07 18:59:19,863 INFO node.py [line:112] osd.2  ---> processId 14826
2017-06-07 18:59:19,863 INFO node.py [line:112] osd.3  ---> processId 15749
2017-06-07 18:59:19,863 INFO node.py [line:112] osd.4  ---> processId 16676
2017-06-07 18:59:19,864 INFO osd.py [line:28] node is  CW113
2017-06-07 18:59:19,864 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=0 & sleep 3
2017-06-07 18:59:24,924 INFO osd.py [line:32] osd osd.0 is shutdown successfully
2017-06-07 18:59:29,929 INFO osd.py [line:102] node is  CW113
2017-06-07 18:59:29,929 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 0 & sleep 30
2017-06-07 19:00:00,114 INFO osd.py [line:107] osd osd.0 is start successfully
2017-06-07 19:00:00,114 INFO osd.py [line:28] node is  CW113
2017-06-07 19:00:00,114 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=1 & sleep 3
2017-06-07 19:00:05,175 INFO osd.py [line:32] osd osd.1 is shutdown successfully
2017-06-07 19:00:10,254 INFO osd.py [line:102] node is  CW113
2017-06-07 19:00:10,254 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 1 & sleep 30
2017-06-07 19:00:40,478 INFO osd.py [line:107] osd osd.1 is start successfully
2017-06-07 19:00:40,478 INFO osd.py [line:28] node is  CW113
2017-06-07 19:00:40,478 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=2 & sleep 3
2017-06-07 19:00:45,559 INFO osd.py [line:32] osd osd.2 is shutdown successfully
2017-06-07 19:00:50,565 INFO osd.py [line:102] node is  CW113
2017-06-07 19:00:50,565 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 2 & sleep 30
2017-06-07 19:01:20,774 INFO osd.py [line:107] osd osd.2 is start successfully
2017-06-07 19:01:20,774 INFO osd.py [line:28] node is  CW113
2017-06-07 19:01:20,774 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=3 & sleep 3
2017-06-07 19:01:25,145 INFO osd.py [line:32] osd osd.3 is shutdown successfully
2017-06-07 19:01:30,151 INFO osd.py [line:102] node is  CW113
2017-06-07 19:01:30,151 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 3 & sleep 30
2017-06-07 19:02:00,356 INFO osd.py [line:107] osd osd.3 is start successfully
2017-06-07 19:02:00,357 INFO osd.py [line:28] node is  CW113
2017-06-07 19:02:00,357 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=4 & sleep 3
2017-06-07 19:02:05,138 INFO osd.py [line:32] osd osd.4 is shutdown successfully
2017-06-07 19:02:10,144 INFO osd.py [line:102] node is  CW113
2017-06-07 19:02:10,144 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 4 & sleep 30
2017-06-07 19:02:40,328 INFO osd.py [line:107] osd osd.4 is start successfully
2017-06-07 19:02:40,562 INFO node.py [line:133] osd.0  ---> processId 7179
2017-06-07 19:02:40,562 INFO node.py [line:133] osd.1  ---> processId 16470
2017-06-07 19:02:40,563 INFO node.py [line:133] osd.2  ---> processId 27281
2017-06-07 19:02:40,563 INFO node.py [line:133] osd.3  ---> processId 38312
2017-06-07 19:02:40,563 INFO node.py [line:133] osd.4  ---> processId 48478
2017-06-07 19:02:40,563 INFO cluster.py [line:211] execute command is ceph -s
2017-06-07 19:02:40,975 INFO cluster.py [line:213]    cluster 9afd408f-471c-4742-8db0-071924668d84
     health HEALTH_OK
     monmap e3: 3 mons at {CW113=192.168.1.113:6789/0,CW114=192.168.1.114:6789/0,CW115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 CW113,CW114,CW115
     osdmap e114: 13 osds: 13 up, 13 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v6379: 3072 pgs, 11 pools, 38584 MB data, 9687 objects
            187 GB used, 8902 GB / 9089 GB avail
                3072 active+clean
  client io 46567 B/s rd, 20875 kB/s wr, 32 op/s rd, 5228 op/s wr

2017-06-07 19:02:40,976 INFO cluster.py [line:238] PG number is 3072
2017-06-07 19:02:40,976 INFO cluster.py [line:239] usefull PG number is 3072
2017-06-07 19:02:40,976 INFO cluster.py [line:302] osd on node CW113 were init successfully
2017-06-07 19:02:40,976 INFO node.py [line:97] init osd on node CW114
2017-06-07 19:02:41,226 INFO node.py [line:112] osd.5  ---> processId 17835
2017-06-07 19:02:41,226 INFO node.py [line:112] osd.6  ---> processId 19003
2017-06-07 19:02:41,226 INFO node.py [line:112] osd.7  ---> processId 19924
2017-06-07 19:02:41,227 INFO osd.py [line:28] node is  CW114
2017-06-07 19:02:41,227 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=5 & sleep 3
2017-06-07 19:02:46,190 INFO osd.py [line:32] osd osd.5 is shutdown successfully
2017-06-07 19:02:51,195 INFO osd.py [line:102] node is  CW114
2017-06-07 19:02:51,196 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 5 & sleep 30
2017-06-07 19:03:21,382 INFO osd.py [line:107] osd osd.5 is start successfully
2017-06-07 19:03:21,382 INFO osd.py [line:28] node is  CW114
2017-06-07 19:03:21,383 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=6 & sleep 3
2017-06-07 19:03:25,862 INFO osd.py [line:32] osd osd.6 is shutdown successfully
2017-06-07 19:03:30,868 INFO osd.py [line:102] node is  CW114
2017-06-07 19:03:30,868 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 6 & sleep 30
2017-06-07 19:04:01,068 INFO osd.py [line:107] osd osd.6 is start successfully
2017-06-07 19:04:01,069 INFO osd.py [line:28] node is  CW114
2017-06-07 19:04:01,069 INFO osd.py [line:29] execute command is sudo -i stop ceph-osd id=7 & sleep 3
2017-06-07 19:04:06,030 INFO osd.py [line:32] osd osd.7 is shutdown successfully
2017-06-07 19:04:11,035 INFO osd.py [line:102] node is  CW114
2017-06-07 19:04:11,035 INFO osd.py [line:103] execute command is sudo -i ceph-osd -i 7 & sleep 30
2017-06-07 19:04:41,221 INFO osd.py [line:107] osd osd.7 is start successfully
2017-06-07 19:04:41,503 INFO node.py [line:133] osd.5  ---> processId 50564
2017-06-07 19:04:41,503 INFO node.py [line:133] osd.6  ---> processId 53354
2017-06-07 19:04:41,503 INFO node.py [line:133] osd.7  ---> processId 56245
2017-06-07 19:04:41,503 INFO cluster.py [line:211] execute command is ceph -s
2017-06-07 19:04:41,766 INFO cluster.py [line:213] raceback (most recent call last):
  File "/usr/local/bin/ceph", line 118, in <module>
    import rados
ImportError: librados.so.2: cannot open shared object file: No such file or directory

