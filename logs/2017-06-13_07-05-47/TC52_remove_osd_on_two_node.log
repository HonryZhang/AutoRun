2017-06-13 07:05:47,546 INFO TC52_remove_osd_on_two_node.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. remove one osd from the first node
4. login the second node
5. remove one osd from the second node

2017-06-13 07:05:48,498 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 07:05:48,498 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 07:05:48,499 INFO TC52_remove_osd_on_two_node.py [line:29] start to check cluster status before case running
2017-06-13 07:05:50,503 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 07:05:50,871 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 6, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e125: 20 osds: 20 up, 20 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v42068: 3216 pgs, 13 pools, 98371 bytes data, 222 objects
            172 GB used, 6705 GB / 6877 GB avail
                3216 active+clean
  client io 290 kB/s rd, 365 op/s rd, 0 op/s wr
stdin: is not a tty

2017-06-13 07:05:50,872 INFO cluster.py [line:238] PG number is 3216
2017-06-13 07:05:50,872 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 07:05:50,872 INFO TC52_remove_osd_on_two_node.py [line:32] health status is OK
2017-06-13 07:05:50,872 INFO TC52_remove_osd_on_two_node.py [line:37] 
Step1: Check IO from clients
2017-06-13 07:05:51,304 INFO client.py [line:172] ['enali    96672  96671  0 23:05 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali    96675  96672  0 23:05 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 07:05:51,304 INFO client.py [line:177] IO stopped
2017-06-13 07:05:51,304 INFO client.py [line:178] start IO again
2017-06-13 07:05:51,304 INFO base.py [line:37] 
Now start IO on  client100rbdImg0
2017-06-13 07:05:51,489 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0
2017-06-13 07:05:51,712 INFO base.py [line:37] 
Now start IO on  client100rbdImg1
2017-06-13 07:05:51,914 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1
2017-06-13 07:05:52,175 INFO base.py [line:37] 
Now start IO on  client100rbdImg2
2017-06-13 07:05:52,380 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2
2017-06-13 07:05:52,634 INFO base.py [line:37] 
Now start IO on  client100rbdImg3
2017-06-13 07:05:52,839 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3
2017-06-13 07:05:53,063 INFO base.py [line:37] 
Now start IO on  client100rbdImg4
2017-06-13 07:05:53,281 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4
2017-06-13 07:05:53,703 INFO node.py [line:159] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-13 07:05:53,703 INFO node.py [line:161] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-13 07:05:54,868 INFO node.py [line:159] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-13 07:05:54,868 INFO node.py [line:161] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-13 07:05:55,879 INFO TC52_remove_osd_on_two_node.py [line:47] 
Step2: kill one osd from two node
2017-06-13 07:05:55,879 INFO TC52_remove_osd_on_two_node.py [line:49] start to delete osd on node taheo125 
2017-06-13 07:05:56,066 INFO node.py [line:183] otal 0
