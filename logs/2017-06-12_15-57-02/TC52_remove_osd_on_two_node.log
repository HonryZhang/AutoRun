2017-06-12 15:57:12,833 INFO TC52_remove_osd_on_two_node.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. remove one osd from the first node
4. login the second node
5. remove one osd from the second node

2017-06-12 15:57:13,480 INFO monitors.py [line:126]    "quorum_leader_name": "CW113",
stdin: is not a tty

2017-06-12 15:57:13,480 INFO monitors.py [line:129]    "quorum_leader_name": "CW113",
2017-06-12 15:57:13,481 INFO TC52_remove_osd_on_two_node.py [line:29] start to check cluster status before case running
2017-06-12 15:57:15,483 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-12 15:57:15,913 INFO cluster.py [line:213]    cluster 9afd408f-471c-4742-8db0-071924668d84
     health HEALTH_OK
     monmap e3: 3 mons at {CW113=192.168.1.113:6789/0,CW114=192.168.1.114:6789/0,CW115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 CW113,CW114,CW115
     osdmap e11717: 13 osds: 13 up, 12 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v139556: 3072 pgs, 11 pools, 51700 MB data, 13302 objects
            208 GB used, 8182 GB / 8390 GB avail
                3072 active+clean
  client io 5459 B/s rd, 30482 B/s wr, 10 op/s rd, 5 op/s wr
stdin: is not a tty

2017-06-12 15:57:15,914 INFO cluster.py [line:238] PG number is 3072
2017-06-12 15:57:15,914 INFO cluster.py [line:239] usefull PG number is 3072
2017-06-12 15:57:15,914 INFO TC52_remove_osd_on_two_node.py [line:32] health status is OK
2017-06-12 15:57:15,914 INFO TC52_remove_osd_on_two_node.py [line:37] 
Step1: Check IO from clients
2017-06-12 15:57:16,367 INFO client.py [line:172] ['enali   134122 134121  0 07:57 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   134124 134122  0 07:57 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-12 15:57:16,368 INFO client.py [line:177] IO stopped
2017-06-12 15:57:16,368 INFO client.py [line:178] start IO again
2017-06-12 15:57:16,368 INFO base.py [line:37] 
Now start IO on  client103rbdImg0
2017-06-12 15:57:16,580 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client103 -pool=reliablityTestPool -rbdname=client103rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client103rbdImg0
2017-06-12 15:57:16,868 INFO base.py [line:37] 
Now start IO on  client103rbdImg1
2017-06-12 15:57:17,060 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client103 -pool=reliablityTestPool -rbdname=client103rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client103rbdImg1
2017-06-12 15:57:17,302 INFO base.py [line:37] 
Now start IO on  client103rbdImg2
2017-06-12 15:57:17,527 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client103 -pool=reliablityTestPool -rbdname=client103rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client103rbdImg2
2017-06-12 15:57:17,780 INFO base.py [line:37] 
Now start IO on  client103rbdImg3
2017-06-12 15:57:18,104 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client103 -pool=reliablityTestPool -rbdname=client103rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client103rbdImg3
2017-06-12 15:57:18,343 INFO base.py [line:37] 
Now start IO on  client103rbdImg4
2017-06-12 15:57:18,555 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client103 -pool=reliablityTestPool -rbdname=client103rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client103rbdImg4
2017-06-12 15:57:18,927 INFO node.py [line:159] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-12 15:57:18,928 INFO node.py [line:161] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-12 15:57:22,530 INFO node.py [line:159] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-12 15:57:22,531 INFO node.py [line:161] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-12 15:57:23,480 INFO TC52_remove_osd_on_two_node.py [line:47] 
Step2: kill one osd from two node
2017-06-12 15:57:23,480 INFO TC52_remove_osd_on_two_node.py [line:49] start to delete osd on node CW113 
2017-06-12 15:57:23,668 INFO node.py [line:185] 0
2017-06-12 15:57:23,668 INFO node.py [line:192] nvme2n1
2017-06-12 15:57:23,669 INFO node.py [line:185] 0
2017-06-12 15:57:23,669 INFO node.py [line:192] nvme2n1
2017-06-12 15:57:23,669 INFO node.py [line:185] 0
2017-06-12 15:57:23,669 INFO node.py [line:192] nvme2n1
2017-06-12 15:57:23,669 INFO node.py [line:185] 1
2017-06-12 15:57:23,669 INFO node.py [line:192] nvme1n1
2017-06-12 15:57:23,669 INFO node.py [line:185] 1
2017-06-12 15:57:23,669 INFO node.py [line:192] nvme1n1
2017-06-12 15:57:23,670 INFO node.py [line:185] 1
2017-06-12 15:57:23,670 INFO node.py [line:192] nvme1n1
2017-06-12 15:57:23,670 INFO node.py [line:185] 2
2017-06-12 15:57:23,670 INFO node.py [line:192] nvme3n1
2017-06-12 15:57:23,670 INFO node.py [line:185] 2
2017-06-12 15:57:23,670 INFO node.py [line:192] nvme3n1
2017-06-12 15:57:23,670 INFO node.py [line:185] 2
2017-06-12 15:57:23,670 INFO node.py [line:192] nvme3n1
2017-06-12 15:57:23,671 INFO node.py [line:185] 3
2017-06-12 15:57:23,671 INFO node.py [line:192] nvme0n1
2017-06-12 15:57:23,671 INFO node.py [line:185] 3
2017-06-12 15:57:23,671 INFO node.py [line:192] nvme0n1
2017-06-12 15:57:23,671 INFO node.py [line:185] 3
2017-06-12 15:57:23,671 INFO node.py [line:192] nvme0n1
2017-06-12 15:57:23,671 INFO node.py [line:185] 4
2017-06-12 15:57:23,671 INFO node.py [line:192] nvme4n1
2017-06-12 15:57:23,672 INFO node.py [line:185] 4
2017-06-12 15:57:23,673 INFO node.py [line:192] nvme4n1
2017-06-12 15:57:23,674 INFO node.py [line:185] 4
2017-06-12 15:57:23,674 INFO node.py [line:192] nvme4n1
2017-06-12 15:57:23,674 INFO node.py [line:185] 
2017-06-12 15:57:23,674 INFO node.py [line:192] nvme2n1
2017-06-12 15:57:23,674 INFO node.py [line:200] osd.1  ---> disk nvme1n1
2017-06-12 15:57:23,674 INFO node.py [line:200] osd.2  ---> disk nvme3n1
2017-06-12 15:57:23,674 INFO node.py [line:200] osd.3  ---> disk nvme0n1
2017-06-12 15:57:23,674 INFO node.py [line:200] osd.4  ---> disk nvme4n1
2017-06-12 15:57:23,675 INFO node.py [line:200] osd.0  ---> disk nvme2n1
2017-06-12 15:57:23,675 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-12 15:57:27,108 INFO osd.py [line:89] node is  CW113
2017-06-12 15:57:27,108 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=1 & sleep 30
2017-06-12 15:57:57,328 ERROR osd.py [line:96] Error when start osdosd.1
2017-06-12 15:57:57,328 ERROR osd.py [line:97] sudo -i start ceph-osd id=1 & sleep 30
2017-06-12 15:57:57,328 ERROR osd.py [line:98] tdin: is not a tty
start: Job is already running: ceph-osd (ceph/1)

2017-06-12 15:57:57,329 INFO osd.py [line:156] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/delete_osds_local.sh -n -d /dev/nvme1n1
2017-06-12 16:00:32,397 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-12 16:00:32,827 INFO cluster.py [line:213]    cluster 9afd408f-471c-4742-8db0-071924668d84
     health HEALTH_OK
     monmap e3: 3 mons at {CW113=192.168.1.113:6789/0,CW114=192.168.1.114:6789/0,CW115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 CW113,CW114,CW115
     osdmap e11798: 12 osds: 12 up, 11 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v139758: 3072 pgs, 11 pools, 1400 GB data, 387 kobjects
            302 GB used, 7389 GB / 7691 GB avail
                3072 active+clean
  client io 435 MB/s wr, 0 op/s rd, 55716 op/s wr
stdin: is not a tty

2017-06-12 16:00:32,828 INFO cluster.py [line:238] PG number is 3072
2017-06-12 16:00:32,828 INFO cluster.py [line:239] usefull PG number is 3072
2017-06-12 16:00:32,828 INFO TC52_remove_osd_on_two_node.py [line:63] osd.1 create succesfully
2017-06-12 16:00:32,828 INFO TC52_remove_osd_on_two_node.py [line:64] osd was delete successfully on node CW113 
2017-06-12 16:00:34,575 INFO TC52_remove_osd_on_two_node.py [line:67] start to delete osd on node CW114 
2017-06-12 16:00:34,796 INFO node.py [line:185] 5
2017-06-12 16:00:34,796 INFO node.py [line:192] nvme2n1
2017-06-12 16:00:34,796 INFO node.py [line:185] 5
2017-06-12 16:00:34,796 INFO node.py [line:192] nvme2n1
2017-06-12 16:00:34,796 INFO node.py [line:185] 5
2017-06-12 16:00:34,796 INFO node.py [line:192] nvme2n1
2017-06-12 16:00:34,796 INFO node.py [line:185] 6
2017-06-12 16:00:34,797 INFO node.py [line:192] nvme4n1
2017-06-12 16:00:34,797 INFO node.py [line:185] 6
2017-06-12 16:00:34,797 INFO node.py [line:192] nvme4n1
2017-06-12 16:00:34,797 INFO node.py [line:185] 6
2017-06-12 16:00:34,797 INFO node.py [line:192] nvme4n1
2017-06-12 16:00:34,797 INFO node.py [line:185] 7
2017-06-12 16:00:34,797 INFO node.py [line:192] nvme3n1
2017-06-12 16:00:34,797 INFO node.py [line:185] 7
2017-06-12 16:00:34,798 INFO node.py [line:192] nvme3n1
2017-06-12 16:00:34,798 INFO node.py [line:185] 7
2017-06-12 16:00:34,798 INFO node.py [line:192] nvme3n1
2017-06-12 16:00:34,798 INFO node.py [line:185] 
2017-06-12 16:00:34,798 INFO node.py [line:192] nvme3n1
2017-06-12 16:00:34,798 INFO node.py [line:200] osd.5  ---> disk nvme2n1
2017-06-12 16:00:34,798 INFO node.py [line:200] osd.6  ---> disk nvme4n1
2017-06-12 16:00:34,798 INFO node.py [line:200] osd.7  ---> disk nvme3n1
2017-06-12 16:00:34,799 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-12 16:00:38,056 INFO osd.py [line:89] node is  CW113
2017-06-12 16:00:38,056 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=5 & sleep 30
2017-06-12 16:01:08,321 ERROR osd.py [line:96] Error when start osdosd.5
2017-06-12 16:01:08,321 ERROR osd.py [line:97] sudo -i start ceph-osd id=5 & sleep 30
2017-06-12 16:01:08,321 ERROR osd.py [line:98] eph-osd (ceph/5) stop/pre-start, process 10661
stdin: is not a tty

2017-06-12 16:01:08,321 INFO osd.py [line:156] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/delete_osds_local.sh -n -d /dev/nvme2n1
2017-06-12 16:01:08,600 ERROR osd.py [line:160] Error when delete osd.5
2017-06-12 16:01:08,601 ERROR osd.py [line:161] sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/delete_osds_local.sh -n -d /dev/nvme2n1
2017-06-12 16:01:08,601 ERROR osd.py [line:162] ARNING: no found part label of /dev/nvme2n1 in /etc/ceph/ceph.conf, this means /dev/nvme2n1 is not in use by any osd
'/dev/nvme2n1' is not used by any osd, 
stdin: is not a tty

