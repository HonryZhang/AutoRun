2017-06-13 09:18:47,181 INFO TC54_kill_mon_on_single_node.py [line:23] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the mon node 
3. stop mon service
4. start mon service
5. should be run after TC53

2017-06-13 09:18:47,853 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 09:18:47,854 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 09:18:49,859 INFO TC54_kill_mon_on_single_node.py [line:28] start to check cluster status before case running
2017-06-13 09:18:51,863 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 09:18:52,208 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 10, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e429: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v49976: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2053 GB used, 4136 GB / 6189 GB avail
                3216 active+clean
  client io 64214 kB/s rd, 301 MB/s wr, 8209 op/s rd, 38655 op/s wr
stdin: is not a tty

2017-06-13 09:18:52,208 INFO cluster.py [line:238] PG number is 3216
2017-06-13 09:18:52,208 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 09:18:52,208 INFO TC54_kill_mon_on_single_node.py [line:31] health status is OK
2017-06-13 09:18:52,208 INFO TC54_kill_mon_on_single_node.py [line:37] 
Step1: Check IO from clients
2017-06-13 09:18:52,620 INFO client.py [line:172] ['oot      33296      1  0 Jun12 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      33298  33296 69 Jun12 ?        01:04:56 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1', 'root      33365      1  0 Jun12 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      33366  33365 90 Jun12 ?        01:25:26 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2', 'root      33436      1  0 Jun12 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      33439  33436 99 Jun12 ?        01:45:34 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3', 'root      33508      1  0 Jun12 ?        00:00:00 sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'root      33510  33508 99 Jun12 ?        02:08:13 fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4', 'denali   147327 147324  0 01:18 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali   147331 147327  0 01:18 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 09:18:52,621 INFO client.py [line:174] IO is running
2017-06-13 09:18:52,621 INFO monitors.py [line:55] mon is  taheo125
2017-06-13 09:18:52,621 INFO monitors.py [line:56] execute command is sudo -i stop ceph-mon id=taheo125 & sleep 5
2017-06-13 09:18:57,842 ERROR monitors.py [line:61] Error when shutdown mon taheo125
2017-06-13 09:18:57,842 ERROR monitors.py [line:62] sudo -i stop ceph-mon id=taheo125 & sleep 5
2017-06-13 09:18:57,842 ERROR monitors.py [line:63] tdin: is not a tty
stop: Unknown instance: ceph/taheo125

2017-06-13 09:18:57,842 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 09:18:57,842 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 09:19:28,052 ERROR monitors.py [line:75] Error when start mon taheo125
2017-06-13 09:19:28,052 ERROR monitors.py [line:76] sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 09:19:28,052 ERROR monitors.py [line:77] tdin: is not a tty
2017-06-13 09:19:01.655674 7f98fbfad4c0 -1 asok(0x7f98f85351c0) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-mon.taheo125.asok': (17) File exists
IO error: lock /var/lib/ceph/mon/ceph-taheo125/store.db/LOCK: Resource temporarily unavailable
2017-06-13 09:19:01.663880 7f98fbfad4c0 -1 error opening mon data directory at '/var/lib/ceph/mon/ceph-taheo125': (22) Invalid argument

2017-06-13 09:19:28,290 INFO monitors.py [line:45] ['enali   31236 31219  0 09:19 ?        00:00:00 bash -c ps -ef | grep ceph-mon', 'denali   31238 31236  0 09:19 ?        00:00:00 grep ceph-mon', 'root     67478     1  2 09:17 ?        00:00:02 ceph-mon -i taheo125', '']
2017-06-13 09:19:28,290 INFO monitors.py [line:51] mon pid is 67478
2017-06-13 09:19:28,290 INFO monitors.py [line:92] execute command is sudo -i kill -9 67478 & sleep 3
2017-06-13 09:20:01,505 INFO monitors.py [line:68] mon is  taheo125
2017-06-13 09:20:01,506 INFO monitors.py [line:69] execute command is sudo -i ceph-mon -i taheo125 & sleep 30
2017-06-13 09:20:31,690 INFO monitors.py [line:73] mon taheo125 is start successfully
2017-06-13 09:20:31,690 INFO monitors.py [line:103] node is  taheo125
2017-06-13 09:20:31,691 INFO monitors.py [line:104] execute command is ps -ef | grep 'ceph-mon' 
2017-06-13 09:20:31,975 INFO monitors.py [line:106] oot     38012     1  2 09:20 ?        00:00:00 ceph-mon -i taheo125
denali   48960 48959  0 09:20 ?        00:00:00 bash -c ps -ef | grep 'ceph-mon' 
denali   48962 48960  0 09:20 ?        00:00:00 grep ceph-mon

2017-06-13 09:20:31,975 INFO monitors.py [line:114] taheo125 is alrady started
2017-06-13 09:21:01,984 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 09:21:02,309 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 14, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e429: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v50019: 3216 pgs, 13 pools, 1498 GB data, 375 kobjects
            2055 GB used, 4134 GB / 6189 GB avail
                3216 active+clean
  client io 1160 kB/s rd, 321 op/s rd, 0 op/s wr
stdin: is not a tty

2017-06-13 09:21:02,309 INFO cluster.py [line:238] PG number is 3216
2017-06-13 09:21:02,310 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 09:21:02,310 INFO TC54_kill_mon_on_single_node.py [line:62] stop mon service on taheo125 in cluster successfully
2017-06-13 09:21:02,727 INFO TC54_kill_mon_on_single_node.py [line:83] 
case runs complete
