2017-06-13 07:44:51,895 INFO TC52_remove_osd_on_two_node.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. remove one osd from the first node
4. login the second node
5. remove one osd from the second node

2017-06-13 07:44:52,831 INFO monitors.py [line:126]    "quorum_leader_name": "taheo125",
stdin: is not a tty

2017-06-13 07:44:52,831 INFO monitors.py [line:129]    "quorum_leader_name": "taheo125",
2017-06-13 07:44:52,831 INFO TC52_remove_osd_on_two_node.py [line:29] start to check cluster status before case running
2017-06-13 07:44:54,835 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 07:44:55,260 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 6, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e156: 19 osds: 19 up, 19 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v44267: 3216 pgs, 13 pools, 98371 bytes data, 222 objects
            164 GB used, 6369 GB / 6533 GB avail
                3216 active+clean
  client io 150 kB/s rd, 225 op/s rd, 0 op/s wr
stdin: is not a tty

2017-06-13 07:44:55,261 INFO cluster.py [line:238] PG number is 3216
2017-06-13 07:44:55,261 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 07:44:55,261 INFO TC52_remove_osd_on_two_node.py [line:32] health status is OK
2017-06-13 07:44:55,261 INFO TC52_remove_osd_on_two_node.py [line:37] 
Step1: Check IO from clients
2017-06-13 07:44:55,719 INFO client.py [line:172] ['enali    33212  33211  0 23:44 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali    33214  33212  0 23:44 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-13 07:44:55,720 INFO client.py [line:177] IO stopped
2017-06-13 07:44:55,720 INFO client.py [line:178] start IO again
2017-06-13 07:44:55,720 INFO base.py [line:37] 
Now start IO on  client100rbdImg0
2017-06-13 07:44:55,943 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg0
2017-06-13 07:44:56,201 INFO base.py [line:37] 
Now start IO on  client100rbdImg1
2017-06-13 07:44:56,424 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg1
2017-06-13 07:44:56,722 INFO base.py [line:37] 
Now start IO on  client100rbdImg2
2017-06-13 07:44:57,020 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg2
2017-06-13 07:44:57,240 INFO base.py [line:37] 
Now start IO on  client100rbdImg3
2017-06-13 07:44:57,461 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg3
2017-06-13 07:44:57,754 INFO base.py [line:37] 
Now start IO on  client100rbdImg4
2017-06-13 07:44:57,973 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client100 -pool=reliablityTestPool -rbdname=client100rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client100rbdImg4
2017-06-13 07:44:58,367 INFO node.py [line:159] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-13 07:44:58,367 INFO node.py [line:161] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-13 07:44:59,480 INFO node.py [line:159] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-13 07:44:59,481 INFO node.py [line:161] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-13 07:45:00,449 INFO TC52_remove_osd_on_two_node.py [line:47] 
Step2: kill one osd from two node
2017-06-13 07:45:00,449 INFO TC52_remove_osd_on_two_node.py [line:49] start to delete osd on node taheo125 
2017-06-13 07:45:00,662 INFO node.py [line:183] otal 0
2017-06-13 07:45:00,662 INFO node.py [line:183] lrwxrwxrwx 1 root root 15 Jun 13 07:33 ceph-0-block -> ../../nvme6n1p4
2017-06-13 07:45:00,663 INFO node.py [line:186] 0
2017-06-13 07:45:00,663 INFO node.py [line:187] lrwxrwxrwx 1 root root 15 Jun 13 07:33 ceph-0-block -> ../../nvme6n1p4
2017-06-13 07:45:00,663 INFO node.py [line:195] nvme6n1
2017-06-13 07:45:00,663 INFO node.py [line:183] lrwxrwxrwx 1 root root 15 Jun 13 07:33 ceph-0-db -> ../../nvme6n1p3
2017-06-13 07:45:00,663 INFO node.py [line:186] 0
2017-06-13 07:45:00,663 INFO node.py [line:187] lrwxrwxrwx 1 root root 15 Jun 13 07:33 ceph-0-db -> ../../nvme6n1p3
2017-06-13 07:45:00,664 INFO node.py [line:195] nvme6n1
2017-06-13 07:45:00,664 INFO node.py [line:183] lrwxrwxrwx 1 root root 15 Jun 13 07:33 ceph-0-wal -> ../../nvme6n1p2
2017-06-13 07:45:00,664 INFO node.py [line:186] 0
2017-06-13 07:45:00,664 INFO node.py [line:187] lrwxrwxrwx 1 root root 15 Jun 13 07:33 ceph-0-wal -> ../../nvme6n1p2
2017-06-13 07:45:00,664 INFO node.py [line:195] nvme6n1
2017-06-13 07:45:00,664 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 13 07:34 ceph-1-block -> ../../nvme27n1p4
2017-06-13 07:45:00,664 INFO node.py [line:186] 1
2017-06-13 07:45:00,664 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 13 07:34 ceph-1-block -> ../../nvme27n1p4
2017-06-13 07:45:00,664 INFO node.py [line:195] nvme27n1
2017-06-13 07:45:00,665 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 13 07:34 ceph-1-db -> ../../nvme27n1p3
2017-06-13 07:45:00,665 INFO node.py [line:186] 1
2017-06-13 07:45:00,665 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 13 07:34 ceph-1-db -> ../../nvme27n1p3
2017-06-13 07:45:00,665 INFO node.py [line:195] nvme27n1
2017-06-13 07:45:00,665 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 13 07:34 ceph-1-wal -> ../../nvme27n1p2
2017-06-13 07:45:00,665 INFO node.py [line:186] 1
2017-06-13 07:45:00,665 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 13 07:34 ceph-1-wal -> ../../nvme27n1p2
2017-06-13 07:45:00,665 INFO node.py [line:195] nvme27n1
2017-06-13 07:45:00,665 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:16 ceph-4-block -> ../../nvme17n1p4
2017-06-13 07:45:00,666 INFO node.py [line:186] 4
2017-06-13 07:45:00,666 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:16 ceph-4-block -> ../../nvme17n1p4
2017-06-13 07:45:00,666 INFO node.py [line:195] nvme17n1
2017-06-13 07:45:00,666 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:16 ceph-4-db -> ../../nvme17n1p3
2017-06-13 07:45:00,666 INFO node.py [line:186] 4
2017-06-13 07:45:00,666 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:16 ceph-4-db -> ../../nvme17n1p3
2017-06-13 07:45:00,666 INFO node.py [line:195] nvme17n1
2017-06-13 07:45:00,666 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:16 ceph-4-wal -> ../../nvme17n1p2
2017-06-13 07:45:00,666 INFO node.py [line:186] 4
2017-06-13 07:45:00,666 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:16 ceph-4-wal -> ../../nvme17n1p2
2017-06-13 07:45:00,667 INFO node.py [line:195] nvme17n1
2017-06-13 07:45:00,667 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-5-block -> ../../nvme10n1p4
2017-06-13 07:45:00,667 INFO node.py [line:186] 5
2017-06-13 07:45:00,667 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-5-block -> ../../nvme10n1p4
2017-06-13 07:45:00,667 INFO node.py [line:195] nvme10n1
2017-06-13 07:45:00,667 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-5-db -> ../../nvme10n1p3
2017-06-13 07:45:00,667 INFO node.py [line:186] 5
2017-06-13 07:45:00,667 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-5-db -> ../../nvme10n1p3
2017-06-13 07:45:00,667 INFO node.py [line:195] nvme10n1
2017-06-13 07:45:00,668 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-5-wal -> ../../nvme10n1p2
2017-06-13 07:45:00,668 INFO node.py [line:186] 5
2017-06-13 07:45:00,668 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-5-wal -> ../../nvme10n1p2
2017-06-13 07:45:00,668 INFO node.py [line:195] nvme10n1
2017-06-13 07:45:00,668 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-6-block -> ../../nvme13n1p4
2017-06-13 07:45:00,668 INFO node.py [line:186] 6
2017-06-13 07:45:00,668 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-6-block -> ../../nvme13n1p4
2017-06-13 07:45:00,668 INFO node.py [line:195] nvme13n1
2017-06-13 07:45:00,668 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-6-db -> ../../nvme13n1p3
2017-06-13 07:45:00,668 INFO node.py [line:186] 6
2017-06-13 07:45:00,669 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-6-db -> ../../nvme13n1p3
2017-06-13 07:45:00,669 INFO node.py [line:195] nvme13n1
2017-06-13 07:45:00,669 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-6-wal -> ../../nvme13n1p2
2017-06-13 07:45:00,669 INFO node.py [line:186] 6
2017-06-13 07:45:00,669 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:17 ceph-6-wal -> ../../nvme13n1p2
2017-06-13 07:45:00,669 INFO node.py [line:195] nvme13n1
2017-06-13 07:45:00,669 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 13 07:43 head-reverse-part -> ../../nvme27n1p1
2017-06-13 07:45:00,669 INFO node.py [line:183] 
2017-06-13 07:45:00,669 INFO node.py [line:203] osd.4  ---> disk nvme17n1
2017-06-13 07:45:00,670 INFO node.py [line:203] osd.5  ---> disk nvme10n1
2017-06-13 07:45:00,670 INFO node.py [line:203] osd.6  ---> disk nvme13n1
2017-06-13 07:45:00,670 INFO node.py [line:203] osd.0  ---> disk nvme6n1
2017-06-13 07:45:00,670 INFO node.py [line:203] osd.1  ---> disk nvme27n1
2017-06-13 07:45:00,670 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-13 07:45:03,890 INFO osd.py [line:89] node is  taheo125
2017-06-13 07:45:03,890 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=4 & sleep 30
2017-06-13 07:45:34,111 ERROR osd.py [line:96] Error when start osdosd.4
2017-06-13 07:45:34,111 ERROR osd.py [line:97] sudo -i start ceph-osd id=4 & sleep 30
2017-06-13 07:45:34,111 ERROR osd.py [line:98] tdin: is not a tty
start: Job is already running: ceph-osd (ceph/4)

2017-06-13 07:45:34,111 INFO osd.py [line:156] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/delete_osds_local.sh -n -d /dev/nvme17n1
2017-06-13 07:46:03,515 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-13 07:46:03,917 INFO cluster.py [line:213]    cluster c1b31c6e-ebc7-4c97-98ec-d59964f99e42
     health HEALTH_OK
     monmap e3: 3 mons at {taheo125=192.168.40.125:6789/0,tahoe126=192.168.40.126:6789/0,tahoe127=192.168.40.127:6789/0}
            election epoch 6, quorum 0,1,2 taheo125,tahoe126,tahoe127
     osdmap e169: 18 osds: 18 up, 18 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v44343: 3216 pgs, 13 pools, 789 GB data, 279 kobjects
            196 GB used, 5993 GB / 6189 GB avail
                3216 active+clean
  client io 26141 kB/s rd, 818 MB/s wr, 19179 op/s rd, 110 kop/s wr
stdin: is not a tty

2017-06-13 07:46:03,917 INFO cluster.py [line:238] PG number is 3216
2017-06-13 07:46:03,917 INFO cluster.py [line:239] usefull PG number is 3216
2017-06-13 07:46:03,918 INFO TC52_remove_osd_on_two_node.py [line:63] osd.4 create succesfully
2017-06-13 07:46:03,918 INFO TC52_remove_osd_on_two_node.py [line:64] osd was delete successfully on node taheo125 
2017-06-13 07:46:05,645 INFO TC52_remove_osd_on_two_node.py [line:67] start to delete osd on node taheo126 
2017-06-13 07:46:05,866 INFO node.py [line:183] otal 0
2017-06-13 07:46:05,866 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-10-block -> ../../nvme28n1p4
2017-06-13 07:46:05,866 INFO node.py [line:186] 10
2017-06-13 07:46:05,866 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-10-block -> ../../nvme28n1p4
2017-06-13 07:46:05,866 INFO node.py [line:195] nvme28n1
2017-06-13 07:46:05,866 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-10-db -> ../../nvme28n1p3
2017-06-13 07:46:05,866 INFO node.py [line:186] 10
2017-06-13 07:46:05,867 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-10-db -> ../../nvme28n1p3
2017-06-13 07:46:05,867 INFO node.py [line:195] nvme28n1
2017-06-13 07:46:05,867 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-10-wal -> ../../nvme28n1p2
2017-06-13 07:46:05,867 INFO node.py [line:186] 10
2017-06-13 07:46:05,867 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-10-wal -> ../../nvme28n1p2
2017-06-13 07:46:05,867 INFO node.py [line:195] nvme28n1
2017-06-13 07:46:05,867 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-11-block -> ../../nvme19n1p4
2017-06-13 07:46:05,867 INFO node.py [line:186] 11
2017-06-13 07:46:05,867 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-11-block -> ../../nvme19n1p4
2017-06-13 07:46:05,868 INFO node.py [line:195] nvme19n1
2017-06-13 07:46:05,868 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-11-db -> ../../nvme19n1p3
2017-06-13 07:46:05,868 INFO node.py [line:186] 11
2017-06-13 07:46:05,868 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-11-db -> ../../nvme19n1p3
2017-06-13 07:46:05,868 INFO node.py [line:195] nvme19n1
2017-06-13 07:46:05,868 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-11-wal -> ../../nvme19n1p2
2017-06-13 07:46:05,868 INFO node.py [line:186] 11
2017-06-13 07:46:05,868 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-11-wal -> ../../nvme19n1p2
2017-06-13 07:46:05,868 INFO node.py [line:195] nvme19n1
2017-06-13 07:46:05,869 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-12-block -> ../../nvme21n1p4
2017-06-13 07:46:05,869 INFO node.py [line:186] 12
2017-06-13 07:46:05,869 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-12-block -> ../../nvme21n1p4
2017-06-13 07:46:05,869 INFO node.py [line:195] nvme21n1
2017-06-13 07:46:05,869 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-12-db -> ../../nvme21n1p3
2017-06-13 07:46:05,869 INFO node.py [line:186] 12
2017-06-13 07:46:05,869 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-12-db -> ../../nvme21n1p3
2017-06-13 07:46:05,869 INFO node.py [line:195] nvme21n1
2017-06-13 07:46:05,869 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-12-wal -> ../../nvme21n1p2
2017-06-13 07:46:05,870 INFO node.py [line:186] 12
2017-06-13 07:46:05,870 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-12-wal -> ../../nvme21n1p2
2017-06-13 07:46:05,870 INFO node.py [line:195] nvme21n1
2017-06-13 07:46:05,870 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-13-block -> ../../nvme12n1p4
2017-06-13 07:46:05,870 INFO node.py [line:186] 13
2017-06-13 07:46:05,870 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-13-block -> ../../nvme12n1p4
2017-06-13 07:46:05,870 INFO node.py [line:195] nvme12n1
2017-06-13 07:46:05,870 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-13-db -> ../../nvme12n1p3
2017-06-13 07:46:05,870 INFO node.py [line:186] 13
2017-06-13 07:46:05,871 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-13-db -> ../../nvme12n1p3
2017-06-13 07:46:05,871 INFO node.py [line:195] nvme12n1
2017-06-13 07:46:05,871 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-13-wal -> ../../nvme12n1p2
2017-06-13 07:46:05,871 INFO node.py [line:186] 13
2017-06-13 07:46:05,871 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:19 ceph-13-wal -> ../../nvme12n1p2
2017-06-13 07:46:05,871 INFO node.py [line:195] nvme12n1
2017-06-13 07:46:05,871 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-7-block -> ../../nvme15n1p4
2017-06-13 07:46:05,871 INFO node.py [line:186] 7
2017-06-13 07:46:05,871 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-7-block -> ../../nvme15n1p4
2017-06-13 07:46:05,872 INFO node.py [line:195] nvme15n1
2017-06-13 07:46:05,872 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-7-db -> ../../nvme15n1p3
2017-06-13 07:46:05,872 INFO node.py [line:186] 7
2017-06-13 07:46:05,872 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-7-db -> ../../nvme15n1p3
2017-06-13 07:46:05,872 INFO node.py [line:195] nvme15n1
2017-06-13 07:46:05,872 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-7-wal -> ../../nvme15n1p2
2017-06-13 07:46:05,872 INFO node.py [line:186] 7
2017-06-13 07:46:05,872 INFO node.py [line:187] lrwxrwxrwx 1 root root 16 Jun 12 18:18 ceph-7-wal -> ../../nvme15n1p2
2017-06-13 07:46:05,872 INFO node.py [line:195] nvme15n1
2017-06-13 07:46:05,873 INFO node.py [line:183] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-8-block -> ../../nvme5n1p4
2017-06-13 07:46:05,873 INFO node.py [line:186] 8
2017-06-13 07:46:05,873 INFO node.py [line:187] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-8-block -> ../../nvme5n1p4
2017-06-13 07:46:05,873 INFO node.py [line:195] nvme5n1
2017-06-13 07:46:05,873 INFO node.py [line:183] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-8-db -> ../../nvme5n1p3
2017-06-13 07:46:05,873 INFO node.py [line:186] 8
2017-06-13 07:46:05,873 INFO node.py [line:187] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-8-db -> ../../nvme5n1p3
2017-06-13 07:46:05,873 INFO node.py [line:195] nvme5n1
2017-06-13 07:46:05,873 INFO node.py [line:183] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-8-wal -> ../../nvme5n1p2
2017-06-13 07:46:05,873 INFO node.py [line:186] 8
2017-06-13 07:46:05,874 INFO node.py [line:187] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-8-wal -> ../../nvme5n1p2
2017-06-13 07:46:05,874 INFO node.py [line:195] nvme5n1
2017-06-13 07:46:05,874 INFO node.py [line:183] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-9-block -> ../../nvme8n1p4
2017-06-13 07:46:05,874 INFO node.py [line:186] 9
2017-06-13 07:46:05,874 INFO node.py [line:187] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-9-block -> ../../nvme8n1p4
2017-06-13 07:46:05,874 INFO node.py [line:195] nvme8n1
2017-06-13 07:46:05,874 INFO node.py [line:183] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-9-db -> ../../nvme8n1p3
2017-06-13 07:46:05,874 INFO node.py [line:186] 9
2017-06-13 07:46:05,874 INFO node.py [line:187] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-9-db -> ../../nvme8n1p3
2017-06-13 07:46:05,875 INFO node.py [line:195] nvme8n1
2017-06-13 07:46:05,875 INFO node.py [line:183] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-9-wal -> ../../nvme8n1p2
2017-06-13 07:46:05,875 INFO node.py [line:186] 9
2017-06-13 07:46:05,875 INFO node.py [line:187] lrwxrwxrwx 1 root root 15 Jun 12 18:18 ceph-9-wal -> ../../nvme8n1p2
2017-06-13 07:46:05,875 INFO node.py [line:195] nvme8n1
2017-06-13 07:46:05,875 INFO node.py [line:183] lrwxrwxrwx 1 root root 16 Jun 12 18:19 head-reverse-part -> ../../nvme12n1p1
2017-06-13 07:46:05,875 INFO node.py [line:183] 
2017-06-13 07:46:05,875 INFO TC52_remove_osd_on_two_node.py [line:70] []
