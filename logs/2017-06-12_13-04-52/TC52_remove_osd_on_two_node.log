2017-06-12 13:04:52,534 INFO TC52_remove_osd_on_two_node.py [line:24] 
This test case will do the following steps:
1. start IO on 10 images with randwrite and verify
2. login the first node 
3. remove one osd from the first node
4. login the second node
5. remove one osd from the second node

2017-06-12 13:04:53,490 INFO monitors.py [line:126]    "quorum_leader_name": "CW113",
stdin: is not a tty

2017-06-12 13:04:53,491 INFO monitors.py [line:129]    "quorum_leader_name": "CW113",
2017-06-12 13:04:53,491 INFO TC52_remove_osd_on_two_node.py [line:29] start to check cluster status before case running
2017-06-12 13:04:55,495 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-12 13:04:55,900 INFO cluster.py [line:213]    cluster 9afd408f-471c-4742-8db0-071924668d84
     health HEALTH_OK
     monmap e3: 3 mons at {CW113=192.168.1.113:6789/0,CW114=192.168.1.114:6789/0,CW115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 CW113,CW114,CW115
     osdmap e11378: 13 osds: 13 up, 13 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v134899: 3072 pgs, 11 pools, 93002 MB data, 23686 objects
            297 GB used, 8791 GB / 9089 GB avail
                3072 active+clean
stdin: is not a tty

2017-06-12 13:04:55,901 INFO cluster.py [line:238] PG number is 3072
2017-06-12 13:04:55,901 INFO cluster.py [line:239] usefull PG number is 3072
2017-06-12 13:04:55,901 INFO TC52_remove_osd_on_two_node.py [line:32] health status is OK
2017-06-12 13:04:55,901 INFO TC52_remove_osd_on_two_node.py [line:37] 
Step1: Check IO from clients
2017-06-12 13:04:56,417 INFO client.py [line:172] ['enali    83836  83835  0 05:04 ?        00:00:00 bash -c sudo -i ps -ef | grep fio ', 'denali    83838  83836  0 05:04 ?        00:00:00 grep fio', 'stdin: is not a tty', '']
2017-06-12 13:04:56,418 INFO client.py [line:177] IO stopped
2017-06-12 13:04:56,418 INFO client.py [line:178] start IO again
2017-06-12 13:04:56,418 INFO base.py [line:37] 
Now start IO on  client103rbdImg0
2017-06-12 13:04:56,775 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client103 -pool=reliablityTestPool -rbdname=client103rbdImg0 -rw=randwrite -bs=8K -size=100G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client103rbdImg0
2017-06-12 13:04:57,017 INFO base.py [line:37] 
Now start IO on  client103rbdImg1
2017-06-12 13:04:57,229 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client103 -pool=reliablityTestPool -rbdname=client103rbdImg1 -rw=randwrite -bs=8K -size=200G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client103rbdImg1
2017-06-12 13:04:57,498 INFO base.py [line:37] 
Now start IO on  client103rbdImg2
2017-06-12 13:04:57,719 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client103 -pool=reliablityTestPool -rbdname=client103rbdImg2 -rw=randwrite -bs=8K -size=300G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client103rbdImg2
2017-06-12 13:04:57,949 INFO base.py [line:37] 
Now start IO on  client103rbdImg3
2017-06-12 13:04:58,282 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client103 -pool=reliablityTestPool -rbdname=client103rbdImg3 -rw=randwrite -bs=8K -size=400G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client103rbdImg3
2017-06-12 13:04:58,528 INFO base.py [line:37] 
Now start IO on  client103rbdImg4
2017-06-12 13:04:58,748 INFO client.py [line:141] nohup sudo -i fio -direct=1 -iodepth_batch_complete=1 -ioengine=rbd -clientname=client103 -pool=reliablityTestPool -rbdname=client103rbdImg4 -rw=randwrite -bs=8K -size=500G -norandommap=1 -randrepeat=0 -verify=md5 -verify_fatal=1 -verify_dump=1 -do_verify=1 -overwrite=1 -iodepth=256 -name=client103rbdImg4
2017-06-12 13:04:59,168 INFO node.py [line:159] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-12 13:04:59,168 INFO node.py [line:161] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-12 13:05:00,245 INFO node.py [line:159] /var/lib/jenkins/workspace/AutoRun/config/changeCommon.sh
2017-06-12 13:05:00,245 INFO node.py [line:161] /var/lib/jenkins/workspace/AutoRun/config/updateCephConfig.sh
2017-06-12 13:05:01,084 INFO TC52_remove_osd_on_two_node.py [line:47] 
Step2: kill one osd from two node
2017-06-12 13:05:01,084 INFO TC52_remove_osd_on_two_node.py [line:49] start to delete osd on node CW113 
2017-06-12 13:05:01,301 INFO node.py [line:185] 0
2017-06-12 13:05:01,301 INFO node.py [line:192] nvme2n1
2017-06-12 13:05:01,302 INFO node.py [line:185] 0
2017-06-12 13:05:01,302 INFO node.py [line:192] nvme2n1
2017-06-12 13:05:01,302 INFO node.py [line:185] 0
2017-06-12 13:05:01,302 INFO node.py [line:192] nvme2n1
2017-06-12 13:05:01,302 INFO node.py [line:185] 1
2017-06-12 13:05:01,302 INFO node.py [line:192] nvme1n1
2017-06-12 13:05:01,302 INFO node.py [line:185] 1
2017-06-12 13:05:01,302 INFO node.py [line:192] nvme1n1
2017-06-12 13:05:01,303 INFO node.py [line:185] 1
2017-06-12 13:05:01,303 INFO node.py [line:192] nvme1n1
2017-06-12 13:05:01,303 INFO node.py [line:185] 2
2017-06-12 13:05:01,303 INFO node.py [line:192] nvme3n1
2017-06-12 13:05:01,303 INFO node.py [line:185] 2
2017-06-12 13:05:01,303 INFO node.py [line:192] nvme3n1
2017-06-12 13:05:01,303 INFO node.py [line:185] 2
2017-06-12 13:05:01,303 INFO node.py [line:192] nvme3n1
2017-06-12 13:05:01,304 INFO node.py [line:185] 3
2017-06-12 13:05:01,304 INFO node.py [line:192] nvme0n1
2017-06-12 13:05:01,304 INFO node.py [line:185] 3
2017-06-12 13:05:01,304 INFO node.py [line:192] nvme0n1
2017-06-12 13:05:01,304 INFO node.py [line:185] 3
2017-06-12 13:05:01,304 INFO node.py [line:192] nvme0n1
2017-06-12 13:05:01,304 INFO node.py [line:185] 4
2017-06-12 13:05:01,304 INFO node.py [line:192] nvme4n1
2017-06-12 13:05:01,304 INFO node.py [line:185] 4
2017-06-12 13:05:01,305 INFO node.py [line:192] nvme4n1
2017-06-12 13:05:01,305 INFO node.py [line:185] 4
2017-06-12 13:05:01,305 INFO node.py [line:192] nvme4n1
2017-06-12 13:05:01,305 INFO node.py [line:185] 
2017-06-12 13:05:01,305 INFO node.py [line:192] nvme4n1
2017-06-12 13:05:01,305 INFO node.py [line:200] osd.0  ---> disk nvme2n1
2017-06-12 13:05:01,305 INFO node.py [line:200] osd.1  ---> disk nvme1n1
2017-06-12 13:05:01,305 INFO node.py [line:200] osd.2  ---> disk nvme3n1
2017-06-12 13:05:01,305 INFO node.py [line:200] osd.3  ---> disk nvme0n1
2017-06-12 13:05:01,306 INFO node.py [line:200] osd.4  ---> disk nvme4n1
2017-06-12 13:05:01,306 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-12 13:05:04,492 INFO osd.py [line:89] node is  CW113
2017-06-12 13:05:04,492 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=0 & sleep 30
2017-06-12 13:05:34,646 ERROR osd.py [line:96] Error when start osdosd.0
2017-06-12 13:05:34,646 ERROR osd.py [line:97] sudo -i start ceph-osd id=0 & sleep 30
2017-06-12 13:05:34,646 ERROR osd.py [line:98] tdin: is not a tty
start: Job is already running: ceph-osd (ceph/0)

2017-06-12 13:05:34,646 INFO osd.py [line:156] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/delete_osds_local.sh -n -d /dev/nvme2n1
2017-06-12 13:07:12,619 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-12 13:07:13,032 INFO cluster.py [line:213]    cluster 9afd408f-471c-4742-8db0-071924668d84
     health HEALTH_OK
     monmap e3: 3 mons at {CW113=192.168.1.113:6789/0,CW114=192.168.1.114:6789/0,CW115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 CW113,CW114,CW115
     osdmap e11456: 12 osds: 12 up, 12 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v135007: 3072 pgs, 11 pools, 93002 MB data, 23686 objects
            289 GB used, 8101 GB / 8390 GB avail
                3072 active+clean
stdin: is not a tty

2017-06-12 13:07:13,032 INFO cluster.py [line:238] PG number is 3072
2017-06-12 13:07:13,032 INFO cluster.py [line:239] usefull PG number is 3072
2017-06-12 13:07:13,032 INFO TC52_remove_osd_on_two_node.py [line:63] osd.0 create succesfully
2017-06-12 13:07:13,033 INFO TC52_remove_osd_on_two_node.py [line:64] osd was delete successfully on node CW113 
2017-06-12 13:07:14,514 INFO TC52_remove_osd_on_two_node.py [line:67] start to delete osd on node CW114 
2017-06-12 13:07:14,699 INFO node.py [line:185] 5
2017-06-12 13:07:14,699 INFO node.py [line:192] nvme2n1
2017-06-12 13:07:14,699 INFO node.py [line:185] 5
2017-06-12 13:07:14,699 INFO node.py [line:192] nvme2n1
2017-06-12 13:07:14,699 INFO node.py [line:185] 5
2017-06-12 13:07:14,699 INFO node.py [line:192] nvme2n1
2017-06-12 13:07:14,699 INFO node.py [line:185] 6
2017-06-12 13:07:14,699 INFO node.py [line:192] nvme4n1
2017-06-12 13:07:14,700 INFO node.py [line:185] 6
2017-06-12 13:07:14,700 INFO node.py [line:192] nvme4n1
2017-06-12 13:07:14,700 INFO node.py [line:185] 6
2017-06-12 13:07:14,700 INFO node.py [line:192] nvme4n1
2017-06-12 13:07:14,700 INFO node.py [line:185] 7
2017-06-12 13:07:14,700 INFO node.py [line:192] nvme3n1
2017-06-12 13:07:14,700 INFO node.py [line:185] 7
2017-06-12 13:07:14,700 INFO node.py [line:192] nvme3n1
2017-06-12 13:07:14,701 INFO node.py [line:185] 7
2017-06-12 13:07:14,701 INFO node.py [line:192] nvme3n1
2017-06-12 13:07:14,701 INFO node.py [line:185] 
2017-06-12 13:07:14,701 INFO node.py [line:192] nvme3n1
2017-06-12 13:07:14,701 INFO node.py [line:200] osd.5  ---> disk nvme2n1
2017-06-12 13:07:14,701 INFO node.py [line:200] osd.6  ---> disk nvme4n1
2017-06-12 13:07:14,701 INFO node.py [line:200] osd.7  ---> disk nvme3n1
2017-06-12 13:07:14,701 INFO osd.py [line:40] execute command is sudo -i kill -9  & sleep 3
2017-06-12 13:07:17,924 INFO osd.py [line:89] node is  CW113
2017-06-12 13:07:17,925 INFO osd.py [line:90] execute command is sudo -i start ceph-osd id=5 & sleep 30
2017-06-12 13:07:48,141 ERROR osd.py [line:96] Error when start osdosd.5
2017-06-12 13:07:48,141 ERROR osd.py [line:97] sudo -i start ceph-osd id=5 & sleep 30
2017-06-12 13:07:48,141 ERROR osd.py [line:98] eph-osd (ceph/5) stop/pre-start, process 20049
stdin: is not a tty

2017-06-12 13:07:48,142 INFO osd.py [line:156] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/delete_osds_local.sh -n -d /dev/nvme2n1
2017-06-12 13:10:42,631 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-12 13:10:43,013 INFO cluster.py [line:213]    cluster 9afd408f-471c-4742-8db0-071924668d84
     health HEALTH_OK
     monmap e3: 3 mons at {CW113=192.168.1.113:6789/0,CW114=192.168.1.114:6789/0,CW115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 CW113,CW114,CW115
     osdmap e11582: 12 osds: 12 up, 11 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v135216: 3072 pgs, 11 pools, 93002 MB data, 23686 objects
            280 GB used, 7411 GB / 7691 GB avail
                3072 active+clean
  client io 172 kB/s rd, 197 op/s rd, 0 op/s wr
stdin: is not a tty

2017-06-12 13:10:43,014 INFO cluster.py [line:238] PG number is 3072
2017-06-12 13:10:43,014 INFO cluster.py [line:239] usefull PG number is 3072
2017-06-12 13:10:43,014 INFO TC52_remove_osd_on_two_node.py [line:79] osd.5 delete succesfully
2017-06-12 13:10:43,014 INFO TC52_remove_osd_on_two_node.py [line:89] osd was delete successfully on node CW114 
2017-06-12 13:10:44,558 INFO TC52_remove_osd_on_two_node.py [line:95] start to create osd on CW113
2017-06-12 13:10:44,558 INFO node.py [line:205] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/changeCommon.sh nvme2n1
2017-06-12 13:10:56,794 INFO node.py [line:212] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/create_osds_local.sh -f 
2017-06-12 13:10:56,794 INFO node.py [line:213] NFO: osd_num_in_each_disk=1
INFO: ============osd.0==============
INFO: --- Create osd.0 OK ---
Reslut:Create osd on CW113 successfully.
Detail:
stdin: is not a tty

2017-06-12 13:10:56,994 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-12 13:10:57,515 INFO cluster.py [line:213]    cluster 9afd408f-471c-4742-8db0-071924668d84
     health HEALTH_ERR
            73 pgs are stuck inactive for more than 300 seconds
            141 pgs peering
            73 pgs stuck inactive
            1/12 in osds are down
     monmap e3: 3 mons at {CW113=192.168.1.113:6789/0,CW114=192.168.1.114:6789/0,CW115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 CW113,CW114,CW115
     osdmap e11585: 13 osds: 12 up, 12 in; 550 remapped pgs
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v135230: 3072 pgs, 11 pools, 93002 MB data, 23686 objects
            280 GB used, 7411 GB / 7691 GB avail
                2931 active+clean
                 141 remapped+peering
stdin: is not a tty

2017-06-12 13:10:57,515 INFO cluster.py [line:215] Now status is HEALTH_ERR, sleep 60s and try again 
2017-06-12 13:11:57,562 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-12 13:11:57,971 INFO cluster.py [line:213]    cluster 9afd408f-471c-4742-8db0-071924668d84
     health HEALTH_WARN
            79 pgs backfill_wait
            1 pgs peering
            67 pgs stuck unclean
            recovery 4612/49678 objects misplaced (9.284%)
     monmap e3: 3 mons at {CW113=192.168.1.113:6789/0,CW114=192.168.1.114:6789/0,CW115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 CW113,CW114,CW115
     osdmap e11637: 13 osds: 13 up, 12 in; 75 remapped pgs
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v135300: 3072 pgs, 11 pools, 93002 MB data, 23686 objects
            288 GB used, 8101 GB / 8390 GB avail
            4612/49678 objects misplaced (9.284%)
                2992 active+clean
                  79 active+remapped+backfill_wait
                   1 peering
stdin: is not a tty

2017-06-12 13:11:57,971 INFO cluster.py [line:238] PG number is 3072
2017-06-12 13:11:57,971 INFO cluster.py [line:239] usefull PG number is 2992
2017-06-12 13:12:58,027 INFO cluster.py [line:247] cost 61 seconds, left 5939 seconds when check the ceph status
2017-06-12 13:12:58,027 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-12 13:12:58,420 INFO cluster.py [line:213]    cluster 9afd408f-471c-4742-8db0-071924668d84
     health HEALTH_WARN
            29 pgs backfill_wait
            3 pgs backfilling
            1 pgs peering
            29 pgs stuck unclean
            recovery 1797/48287 objects misplaced (3.721%)
     monmap e3: 3 mons at {CW113=192.168.1.113:6789/0,CW114=192.168.1.114:6789/0,CW115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 CW113,CW114,CW115
     osdmap e11681: 13 osds: 13 up, 12 in; 30 remapped pgs
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v135367: 3072 pgs, 11 pools, 93002 MB data, 23686 objects
            289 GB used, 8101 GB / 8390 GB avail
            1797/48287 objects misplaced (3.721%)
                3039 active+clean
                  29 active+remapped+backfill_wait
                   3 active+remapped+backfilling
                   1 peering
recovery io 20784 kB/s, 5 objects/s
  client io 59237 B/s rd, 86 op/s rd, 0 op/s wr
stdin: is not a tty

2017-06-12 13:12:58,420 INFO cluster.py [line:238] PG number is 3072
2017-06-12 13:12:58,421 INFO cluster.py [line:239] usefull PG number is 3039
2017-06-12 13:13:58,481 INFO cluster.py [line:247] cost 60 seconds, left 5879 seconds when check the ceph status
2017-06-12 13:13:58,481 INFO cluster.py [line:211] execute command is sudo -i ceph -s
2017-06-12 13:13:58,890 INFO cluster.py [line:213]    cluster 9afd408f-471c-4742-8db0-071924668d84
     health HEALTH_OK
     monmap e3: 3 mons at {CW113=192.168.1.113:6789/0,CW114=192.168.1.114:6789/0,CW115=192.168.1.115:6789/0}
            election epoch 6, quorum 0,1,2 CW113,CW114,CW115
     osdmap e11709: 13 osds: 13 up, 12 in
            flags sortbitwise,require_jewel_osds,require_kraken_osds
      pgmap v135424: 3072 pgs, 11 pools, 93002 MB data, 23686 objects
            288 GB used, 8101 GB / 8390 GB avail
                3072 active+clean
  client io 33241 B/s rd, 48 op/s rd, 0 op/s wr
stdin: is not a tty

2017-06-12 13:13:58,890 INFO cluster.py [line:238] PG number is 3072
2017-06-12 13:13:58,890 INFO cluster.py [line:239] usefull PG number is 3072
2017-06-12 13:13:58,890 INFO TC52_remove_osd_on_two_node.py [line:101] osd on CW113 create succesfully
2017-06-12 13:14:00,355 INFO TC52_remove_osd_on_two_node.py [line:104] start to create osd on CW114
2017-06-12 13:14:00,355 INFO node.py [line:205] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/changeCommon.sh nvme2n1
2017-06-12 13:14:03,980 INFO node.py [line:212] execute command is sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/create_osds_local.sh -f 
2017-06-12 13:14:03,980 INFO node.py [line:213] NFO: osd_num_in_each_disk=1
WARNING: /dev/nvme2n1 has partitions
ERROR: make gpt label in '/dev/nvme2n1' failed.Error: Partition(s) 2, 3, 4 on /dev/nvme2n1 have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
Reslut:Create osd on CW114 failed.
Detail:make gpt label in '/dev/nvme2n1' failed.Error: Partition(s) 2, 3, 4 on /dev/nvme2n1 have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
stdin: is not a tty

2017-06-12 13:14:03,981 ERROR node.py [line:215] Error when create osd
2017-06-12 13:14:03,981 ERROR node.py [line:216] sudo -i /usr/local/bin/scripts/create_cluster_scripts/bluestore/create_osds_local.sh -f 
2017-06-12 13:14:03,981 ERROR node.py [line:217] NFO: osd_num_in_each_disk=1
WARNING: /dev/nvme2n1 has partitions
ERROR: make gpt label in '/dev/nvme2n1' failed.Error: Partition(s) 2, 3, 4 on /dev/nvme2n1 have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
Reslut:Create osd on CW114 failed.
Detail:make gpt label in '/dev/nvme2n1' failed.Error: Partition(s) 2, 3, 4 on /dev/nvme2n1 have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
stdin: is not a tty

